{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish Image Captioning System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:52:38.584599Z",
     "iopub.status.busy": "2025-09-13T11:52:38.584025Z",
     "iopub.status.idle": "2025-09-13T11:54:47.920499Z",
     "shell.execute_reply": "2025-09-13T11:54:47.919734Z",
     "shell.execute_reply.started": "2025-09-13T11:52:38.584576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Rerun this cell at each session start\n",
    "\n",
    "# Uninstall conflicting packages (Kaggle specific)\n",
    "!pip uninstall -y bigframes cesium gcsfs\n",
    "\n",
    "# Performance metrics\n",
    "!pip install -r /kaggle/input/requirements/requirements.txt\n",
    "\n",
    "# To use nltk\n",
    "import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\n",
    "\n",
    "# Download mT5-small and load CLIP ViT-B/32\n",
    "import clip, torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "mt5 = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"Setup Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:54:47.922272Z",
     "iopub.status.busy": "2025-09-13T11:54:47.921623Z",
     "iopub.status.idle": "2025-09-13T11:54:48.047741Z",
     "shell.execute_reply": "2025-09-13T11:54:48.046978Z",
     "shell.execute_reply.started": "2025-09-13T11:54:47.922241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Unified configuration and environment setup\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"XXX\"  # e.g., \"clip_prefix_captioning\"\n",
    "RUN_NAME = \"XXX\"      # e.g., \"exp1\"\n",
    "\n",
    "ENABLE_WANDB = True  # Set False to skip W&B entirely\n",
    "\n",
    "WANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/32\",  # backbone\n",
    "    \"prefix_tokens\": 32,          # stronger conditioning\n",
    "    \"batch_size\": 16,              # reduced to mitigate OOM\n",
    "    \"grad_accum_steps\": 2,        # accumulate to simulate larger effective batch\n",
    "    \"enable_t5_gradient_checkpointing\": True,  # reduce memory\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 50,                  # allow a bit longer now that we fine-tune CLIP\n",
    "    \"dataset_limit\": None,\n",
    "    # --- CLIP fine-tuning controls ---\n",
    "    \"freeze_clip\": False,          # set False to allow full CLIP fine-tuning\n",
    "    \"unfreeze_clip_last_n\": 0,     # if >0 unfreezes only last N vision blocks\n",
    "    \"clip_lr_scale\": 0.05,         # scaled LR for ALL CLIP params (lower than main)\n",
    "    \"use_clip_patch_tokens\": True, # richer conditioning (patch tokens path) (set False to save memory)\n",
    "    # --- T5 freezing ---\n",
    "    \"freeze_t5_encoder\": False,    # unfreeze encoder so it can adapt\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    # --- Optimization ---\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 500,           # linear warmup steps before cosine decay\n",
    "    # --- Inference defaults ---\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    \"use_amp\": True,\n",
    "    # Early stop\n",
    "    \"early_stop_patience\": 5,\n",
    "    \"early_stop_min_delta\": 0.001,\n",
    "    # Optional extras:\n",
    "    \"use_bf16\": True,\n",
    "    \"enable_tf32\": True,\n",
    "    \"finite_loss_skip\": True,\n",
    "    \"save_every\": 0\n",
    "}\n",
    "\n",
    "# Global flags/handles populated after (optional) wandb init cell\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "\n",
    "# Always create a local cfg object here; W&B init (next cell) can sync/override\n",
    "class _Cfg: ...\n",
    "cfg = _Cfg()\n",
    "for k, v in base_config.items():\n",
    "    setattr(cfg, k, v)\n",
    "print(\"[INFO] Local config object created.\")\n",
    "if ENABLE_WANDB:\n",
    "    print(\"[INFO] Run the next 'W&B Init' cell to enable Weights & Biases tracking.\")\n",
    "else:\n",
    "    print(\"[INFO] W&B disabled (ENABLE_WANDB=False). Using local config only.\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# --- Enforce CUDA-only environment ---\n",
    "assert torch.cuda.is_available(), \"CUDA GPU is required but not detected. Please run in a CUDA-enabled environment.\"\n",
    "device = torch.device('cuda')\n",
    "print(f\"[DEVICE] Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Performance toggles\n",
    "if getattr(cfg, 'enable_tf32', False):\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"[DEVICE] TF32 enabled.\")\n",
    "    except Exception as _e:\n",
    "        print(\"[WARN] Could not enable TF32:\", _e)\n",
    "\n",
    "if getattr(cfg, 'use_bf16', False):\n",
    "    bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    print(f\"[DEVICE] bfloat16 support: {bf16_ok}\")\n",
    "\n",
    "print(\"Active config (local):\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:54:48.416041Z",
     "iopub.status.busy": "2025-09-13T11:54:48.415668Z",
     "iopub.status.idle": "2025-09-13T11:54:51.856129Z",
     "shell.execute_reply": "2025-09-13T11:54:51.855310Z",
     "shell.execute_reply.started": "2025-09-13T11:54:48.416018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# (CUDA enforcement handled in config cell; assume cuda device later)\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model, use_fast=False)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        # CLIP freezing / unfreezing strategy\n",
    "        if cfg.unfreeze_clip_last_n and cfg.unfreeze_clip_last_n > 0:\n",
    "            # Freeze everything first\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "            blocks = list(self.clip.visual.transformer.resblocks)\n",
    "            for block in blocks[-cfg.unfreeze_clip_last_n:]:\n",
    "                for p in block.parameters():\n",
    "                    p.requires_grad = True\n",
    "        else:\n",
    "            # Respect freeze_clip flag (False means fully trainable)\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = not cfg.freeze_clip\n",
    "        # T5 freeze toggles\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine embedding dim dynamically (CLIP projection output)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    def _encode_image_single(self, images: torch.Tensor):\n",
    "        # (B, D) pooled embedding (already passed through ln_post + proj inside encode_image)\n",
    "        pooled = self.clip.encode_image(images)\n",
    "        return pooled\n",
    "\n",
    "    def _encode_image_patch_tokens(self, images: torch.Tensor):\n",
    "        \"\"\"Return patch-averaged embedding projected into CLIP joint space.\n",
    "        We manually replicate encode_image path but average patch tokens (excluding CLS),\n",
    "        then apply ln_post and proj so final dim == visual.output_dim (e.g. 512) to match ProjectionMLP in_dim.\"\"\"\n",
    "        visual = self.clip.visual\n",
    "        x = visual.conv1(images)                      # (B, width, grid, grid)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)     # (B, width, patches)\n",
    "        x = x.permute(0, 2, 1)                        # (B, patches, width)\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)         # prepend CLS\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        x = visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)                        # (sequence, B, width)\n",
    "        for block in visual.transformer.resblocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(1, 0, 2)                        # (B, sequence, width)\n",
    "        patches = x[:, 1:, :]                         # drop CLS\n",
    "        pooled = patches.mean(dim=1)                  # (B, width)\n",
    "        if hasattr(visual, 'ln_post'):\n",
    "            pooled = visual.ln_post(pooled)\n",
    "        if hasattr(visual, 'proj') and visual.proj is not None:\n",
    "            pooled = pooled @ visual.proj             # (B, output_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        images = images.to(device)\n",
    "        clip_emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        images = images.to(next(self.parameters()).device)\n",
    "        emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        return self.proj(emb)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu görüntüyü açıkla: \", ban_sentinels=True, **gen_kwargs):\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T11:10:04.044472Z",
     "iopub.status.busy": "2025-09-12T11:10:04.043853Z",
     "iopub.status.idle": "2025-09-12T11:10:22.353930Z",
     "shell.execute_reply": "2025-09-12T11:10:22.353055Z",
     "shell.execute_reply.started": "2025-09-12T11:10:04.044446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# (Optional) Weights & Biases initialization\n",
    "if 'ENABLE_WANDB' in globals() and ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config, reinit=True)\n",
    "        cfg = wandb.config  # sync cfg to wandb\n",
    "        # Explicitly log config dict to the run (config, summary, and a one-time log)\n",
    "        try:\n",
    "            cfg_dict = dict(base_config)\n",
    "        except Exception:\n",
    "            cfg_dict = {k: getattr(cfg, k) for k in base_config.keys() if hasattr(cfg, k)}\n",
    "        wandb.config.update(cfg_dict, allow_val_change=True)\n",
    "        # Store a namespaced copy in summary for quick viewing\n",
    "        wandb.summary.update({f\"cfg/{k}\": v for k, v in cfg_dict.items()})\n",
    "        # Also log once at step 0 for time-series traceability\n",
    "        wandb.log({\"cfg\": cfg_dict}, step=0)\n",
    "        use_wandb = True\n",
    "        print('[wandb] run initialized and config logged.')\n",
    "    except Exception as e:\n",
    "        use_wandb = False\n",
    "        print('[wandb] disabled (init failed):', e)\n",
    "else:\n",
    "    print('[wandb] Skipped (ENABLE_WANDB is False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:33:12.704959Z",
     "iopub.status.busy": "2025-09-13T07:33:12.704573Z",
     "iopub.status.idle": "2025-09-13T07:33:35.069473Z",
     "shell.execute_reply": "2025-09-13T07:33:35.068667Z",
     "shell.execute_reply.started": "2025-09-13T07:33:12.704922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "json_path = '/kaggle/input/tasviret/flickr8k/tasviret8k_captions.json'\n",
    "images_root = '/kaggle/input/tasviret/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=None)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Clip ViT-B/32 params:\", sum(p.numel() for p in model_mm.clip.parameters() if p.requires_grad))\n",
    "print(\"Projection params:\", sum(p.numel() for p in model_mm.proj.parameters() if p.requires_grad))\n",
    "print(\"mt5-small params:\", sum(p.numel() for p in model_mm.model.parameters() if p.requires_grad))\n",
    "print(\"Total trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T11:11:42.416565Z",
     "iopub.status.busy": "2025-09-12T11:11:42.415804Z",
     "iopub.status.idle": "2025-09-12T13:59:44.149541Z",
     "shell.execute_reply": "2025-09-12T13:59:44.148749Z",
     "shell.execute_reply.started": "2025-09-12T11:11:42.416535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training cell: loss + BLEU1 early stopping + grad accum + checkpoints + history lists (fixed for this pipeline)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required.\"\n",
    "device = torch.device(\"cuda\")\n",
    "model_mm.to(device)\n",
    "\n",
    "# Helper: compute BLEU on validation set using existing generation API\n",
    "from collections import defaultdict\n",
    "\n",
    "def _compute_val_bleu(model, val_dataset, images_root, batch_size=16, amp_dtype=None):\n",
    "    if val_dataset is None or len(val_dataset) == 0:\n",
    "        return None, None\n",
    "    refs_map = defaultdict(list)\n",
    "    for fname, cap in val_dataset.samples:\n",
    "        c = str(cap).strip()\n",
    "        if c:\n",
    "            refs_map[fname].append(c)\n",
    "    if not refs_map:\n",
    "        return None, None\n",
    "    image_files = list(refs_map.keys())\n",
    "    image_paths = [os.path.join(images_root, f) for f in image_files]\n",
    "\n",
    "    def _generate_batch(paths):\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    hyps, refs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            outs = _generate_batch(batch_paths)\n",
    "            for pth, pred in zip(batch_paths, outs):\n",
    "                fname = os.path.basename(pth)\n",
    "                hyps.append(pred if pred else \"<EMPTY>\")\n",
    "                refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "    try:\n",
    "        from pycocoevalcap.bleu.bleu import Bleu\n",
    "        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "        return float(bleu_scores[0]), float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_model_with_bleu(\n",
    "    model_mm,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    tokenizer,  # kept for signature compatibility; not used directly here\n",
    "    cfg,\n",
    "    num_epochs=None,\n",
    "    grad_accum_steps=None,\n",
    "    min_epochs_before_bleu=10,\n",
    "    bleu_patience=5,\n",
    "):\n",
    "    # Resolve config values from cfg object\n",
    "    lr = getattr(cfg, 'lr')\n",
    "    weight_decay = getattr(cfg, 'weight_decay', 0.01)\n",
    "    warmup_steps = getattr(cfg, 'warmup_steps', 0)\n",
    "    epochs = num_epochs if num_epochs is not None else getattr(cfg, 'epochs', 30)\n",
    "    grad_accum = grad_accum_steps if grad_accum_steps is not None else getattr(cfg, 'grad_accum_steps', 1)\n",
    "\n",
    "    # Optional AMP dtype per earlier cells\n",
    "    amp_dtype = None\n",
    "    if getattr(cfg, 'use_amp', True):\n",
    "        if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "            amp_dtype = torch.bfloat16\n",
    "        else:\n",
    "            amp_dtype = torch.float16\n",
    "\n",
    "    model_mm.train()\n",
    "\n",
    "    # Optimizer: keep CLIP LR scale logic similar to earlier cells\n",
    "    main_params, clip_params = [], []\n",
    "    for name, p in model_mm.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (clip_params if name.startswith('clip.') else main_params).append(p)\n",
    "    param_groups = []\n",
    "    if main_params:\n",
    "        param_groups.append({\"params\": main_params, \"lr\": lr})\n",
    "    if clip_params:\n",
    "        scaled_lr = lr * getattr(cfg, 'clip_lr_scale', 0.05)\n",
    "        param_groups.append({\"params\": clip_params, \"lr\": scaled_lr})\n",
    "        print(f\"[INFO] CLIP fine-tune params: {len(clip_params)} with lr={scaled_lr:.2e}\")\n",
    "\n",
    "    optimizer = AdamW(param_groups, weight_decay=weight_decay)\n",
    "\n",
    "    # Simple warmup to 1.0; matches your snippet intent\n",
    "    scheduler = LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) / float(max(1, warmup_steps)), 1.0) if warmup_steps > 0 else 1.0,\n",
    "    )\n",
    "\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    best_path = os.path.join('checkpoints', 'best.pt')\n",
    "    last_path = os.path.join('checkpoints', 'last.pt')\n",
    "\n",
    "    best_bleu = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    global_step = 0\n",
    "\n",
    "    train_losses, val_losses, epochs_list = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # === Training loop ===\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            imgs, srcs, tgts = batch\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            if amp_dtype is not None:\n",
    "                with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                    outputs = model_mm(imgs, srcs, tgts)\n",
    "                    loss = outputs.loss / grad_accum\n",
    "                loss.backward()\n",
    "            else:\n",
    "                outputs = model_mm(imgs, srcs, tgts)\n",
    "                loss = outputs.loss / grad_accum\n",
    "                loss.backward()\n",
    "\n",
    "            if (step + 1) % grad_accum == 0:\n",
    "                if getattr(cfg, 'grad_clip', 0.0) > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model_mm.parameters(), getattr(cfg, 'grad_clip', 1.0))\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "\n",
    "        # === Validation loss ===\n",
    "        model_mm.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                if amp_dtype is not None:\n",
    "                    with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                        outputs = model_mm(imgs, srcs, tgts)\n",
    "                        total_val_loss += outputs.loss.item()\n",
    "                else:\n",
    "                    outputs = model_mm(imgs, srcs, tgts)\n",
    "                    total_val_loss += outputs.loss.item()\n",
    "        avg_val_loss = total_val_loss / max(1, len(val_loader))\n",
    "\n",
    "        # Save losses to lists\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        epochs_list.append(epoch + 1)\n",
    "\n",
    "        # === BLEU evaluation ===\n",
    "        bleu1 = bleu4 = None\n",
    "        if epoch >= min_epochs_before_bleu:\n",
    "            try:\n",
    "                bleu1, bleu4 = _compute_val_bleu(model_mm, val_dataset, images_root, batch_size=16, amp_dtype=amp_dtype)\n",
    "            except Exception as e:\n",
    "                print('[WARN] BLEU eval failed:', e)\n",
    "\n",
    "            # Check improvement\n",
    "            if bleu1 is not None and bleu1 > best_bleu:\n",
    "                best_bleu = bleu1\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'model': model_mm.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'best_bleu': best_bleu,\n",
    "                        'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "                    },\n",
    "                    best_path,\n",
    "                )\n",
    "                print(f\"[Epoch {epoch}] New best BLEU-1={bleu1:.4f} saved at {best_path}\")\n",
    "            elif bleu1 is not None:\n",
    "                no_improve += 1\n",
    "\n",
    "        # Save rolling checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': model_mm.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "            },\n",
    "            last_path,\n",
    "        )\n",
    "\n",
    "        # Log (guard W&B)\n",
    "        time_elapsed = time.time() - t0\n",
    "        try:\n",
    "            if 'use_wandb' in globals() and use_wandb:\n",
    "                log_dict = {\n",
    "                    'train/epoch_loss': avg_train_loss,\n",
    "                    'val/epoch_loss': avg_val_loss,\n",
    "                    'lr': scheduler.get_last_lr()[0],\n",
    "                }\n",
    "                if epoch >= min_epochs_before_bleu and bleu1 is not None:\n",
    "                    log_dict['val/bleu1'] = bleu1\n",
    "                    if bleu4 is not None:\n",
    "                        log_dict['val/bleu4'] = bleu4\n",
    "                wandb.log(log_dict, step=epoch)\n",
    "        except Exception as e:\n",
    "            print('[wandb] log skipped:', e)\n",
    "\n",
    "        # Print summary (include BLEU after activation)\n",
    "        if epoch >= min_epochs_before_bleu and bleu1 is not None:\n",
    "            print(f\"[Epoch {epoch}] Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | BLEU-1={bleu1:.4f} | Time={time_elapsed:.1f}s\")\n",
    "        else:\n",
    "            print(f\"[Epoch {epoch}] Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Time={time_elapsed:.1f}s\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch >= min_epochs_before_bleu and no_improve >= bleu_patience:\n",
    "            print(f\"⏹ Early stopping at epoch {epoch}. Best BLEU-1={best_bleu:.4f} at epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        model_mm.train()\n",
    "\n",
    "    return best_bleu, best_epoch, train_losses, val_losses, epochs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_bleu, best_epoch, train_losses, val_losses, epochs_list = train_model_with_bleu(\n",
    "    model_mm,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_mm.tokenizer,\n",
    "    cfg,\n",
    "    num_epochs=cfg.epochs,\n",
    "    grad_accum_steps=cfg.grad_accum_steps,\n",
    "    min_epochs_before_bleu=8,\n",
    "    bleu_patience=getattr(cfg, 'bleu_patience', 5),\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T13:59:50.269363Z",
     "iopub.status.busy": "2025-09-12T13:59:50.269067Z",
     "iopub.status.idle": "2025-09-12T13:59:50.865811Z",
     "shell.execute_reply": "2025-09-12T13:59:50.865126Z",
     "shell.execute_reply.started": "2025-09-12T13:59:50.269338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss curves in the same plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'epochs_list' in globals() and len(epochs_list) == len(train_losses) == len(val_losses) and len(epochs_list) > 0:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(epochs_list, train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs_list, val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No loss history found. Run the training cell first to populate train_losses/val_losses/epochs_list.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Loader Function (from checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:34:29.755646Z",
     "iopub.status.busy": "2025-09-13T07:34:29.754861Z",
     "iopub.status.idle": "2025-09-13T07:34:29.765396Z",
     "shell.execute_reply": "2025-09-13T07:34:29.764651Z",
     "shell.execute_reply.started": "2025-09-13T07:34:29.755618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Direct .pt Checkpoint Loader ===\n",
    "import torch, os\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "def load_model_from_checkpoint_file(\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    resume_optimizer: bool = False,\n",
    "    build_optimizer_fn=None,\n",
    "    build_scheduler_fn=None,\n",
    "    override_cfg: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"Load model (and optional optimizer/scheduler) directly from a single .pt file.\n",
    "    \n",
    "    Works with both:\n",
    "    - { 'model_state', 'optimizer_state', 'scheduler_state', 'cfg', ... }\n",
    "    - { 'model', 'optimizer', 'scheduler', 'cfg', ... }\n",
    "\n",
    "    Returns: (model, cfg_obj, optimizer, scheduler, epoch, global_step)\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "    bundle = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # --- Model state ---\n",
    "    if 'model_state' in bundle:\n",
    "        model_state = bundle['model_state']\n",
    "    elif 'model' in bundle:\n",
    "        model_state = bundle['model']\n",
    "    else:\n",
    "        raise ValueError(\"Checkpoint missing model weights (expected 'model_state' or 'model')\")\n",
    "\n",
    "    # --- Config ---\n",
    "    if 'cfg' not in bundle:\n",
    "        raise ValueError(\"Checkpoint missing 'cfg'\")\n",
    "    cfg_json = dict(bundle['cfg'])\n",
    "    if override_cfg:\n",
    "        cfg_json.update(override_cfg)\n",
    "\n",
    "    class _Cfg: ...\n",
    "    cfg_obj = _Cfg()\n",
    "    for k, v in cfg_json.items():\n",
    "        setattr(cfg_obj, k, v)\n",
    "\n",
    "    # --- Build model and load weights ---\n",
    "    model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "\n",
    "    # --- Metadata ---\n",
    "    epoch = bundle.get('epoch')\n",
    "    global_step = bundle.get('global_step')\n",
    "\n",
    "    # --- Optimizer and scheduler (optional) ---\n",
    "    optimizer, scheduler = None, None\n",
    "    if resume_optimizer:\n",
    "        # Optimizer\n",
    "        opt_state = bundle.get('optimizer_state') or bundle.get('optimizer')\n",
    "        if opt_state is not None and build_optimizer_fn is not None:\n",
    "            optimizer = build_optimizer_fn(cfg_obj, model)\n",
    "            try:\n",
    "                optimizer.load_state_dict(opt_state)\n",
    "                print('[IMPORT] Optimizer state restored.')\n",
    "            except Exception as e:\n",
    "                print(f'[WARN] Failed to load optimizer state: {e}')\n",
    "        elif opt_state is not None:\n",
    "            print('[WARN] Optimizer state present but build_optimizer_fn not provided; skipping restore.')\n",
    "\n",
    "        # Scheduler\n",
    "        sch_state = bundle.get('scheduler_state') or bundle.get('scheduler')\n",
    "        if sch_state is not None and optimizer and build_scheduler_fn is not None:\n",
    "            scheduler = build_scheduler_fn(cfg_obj, optimizer)\n",
    "            try:\n",
    "                scheduler.load_state_dict(sch_state)\n",
    "                print('[IMPORT] Scheduler state restored.')\n",
    "            except Exception as e:\n",
    "                print(f'[WARN] Failed to load scheduler state: {e}')\n",
    "\n",
    "    print(f\"[IMPORT] Loaded model from file: {checkpoint_path}\")\n",
    "    return model, cfg_obj, optimizer, scheduler, epoch, global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:34:37.324408Z",
     "iopub.status.busy": "2025-09-13T07:34:37.324091Z",
     "iopub.status.idle": "2025-09-13T07:35:08.849105Z",
     "shell.execute_reply": "2025-09-13T07:35:08.848337Z",
     "shell.execute_reply.started": "2025-09-13T07:34:37.324378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/working/checkpoints/best.pt', device=torch.device('cuda'), resume_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:35:21.660859Z",
     "iopub.status.busy": "2025-09-13T07:35:21.660203Z",
     "iopub.status.idle": "2025-09-13T07:35:22.980384Z",
     "shell.execute_reply": "2025-09-13T07:35:22.979805Z",
     "shell.execute_reply.started": "2025-09-13T07:35:21.660834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test/Inference on the test_dataset\n",
    "import os, random, torch\n",
    "from collections import defaultdict\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_test_set(model, test_dataset, images_root, batch_size=16, top_k_examples=5):\n",
    "    if test_dataset is None or len(test_dataset) == 0:\n",
    "        print(\"No test dataset available.\")\n",
    "        return\n",
    "\n",
    "    # Build references map\n",
    "    refs_map = defaultdict(list)\n",
    "    for fname, cap in test_dataset.samples:\n",
    "        c = str(cap).strip()\n",
    "        if c:\n",
    "            refs_map[fname].append(c)\n",
    "\n",
    "    image_files = list(refs_map.keys())\n",
    "    image_paths = [os.path.join(images_root, f) for f in image_files]\n",
    "\n",
    "    print(f\"Evaluating on {len(image_paths)} test images…\")\n",
    "    model.eval()\n",
    "    hyps, refs = [], []\n",
    "\n",
    "    def _generate_batch(paths):\n",
    "        local_amp = globals().get('amp_dtype', None)\n",
    "        if local_amp is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=local_amp):\n",
    "                return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            outs = _generate_batch(batch_paths)\n",
    "            for pth, pred in zip(batch_paths, outs):\n",
    "                fname = os.path.basename(pth)\n",
    "                hyps.append(pred if pred else \"<EMPTY>\")\n",
    "                refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "    # Prepare for COCO eval\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "    # Compute BLEU, METEOR, ROUGE-L, CIDEr\n",
    "    try:\n",
    "        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] BLEU failed:\", e)\n",
    "        bleu_scores = [0,0,0,0]\n",
    "    try:\n",
    "        meteor_score, _ = Meteor().compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] METEOR failed:\", e)\n",
    "        meteor_score = 0\n",
    "    try:\n",
    "        rouge_score, _ = Rouge().compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] ROUGE-L failed:\", e)\n",
    "        rouge_score = 0\n",
    "    try:\n",
    "        cider_score, _ = Cider().compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] CIDEr failed:\", e)\n",
    "        cider_score = 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nTest Set Metrics:\\nBLEU-1: {bleu_scores[0]:.4f}  BLEU-2: {bleu_scores[1]:.4f}  BLEU-3: {bleu_scores[2]:.4f}  BLEU-4: {bleu_scores[3]:.4f}\")\n",
    "    print(f\"METEOR: {meteor_score:.4f}  ROUGE-L: {rouge_score:.4f}  CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "    # Show qualitative examples\n",
    "    k = min(top_k_examples, len(image_files))\n",
    "    if k > 0:\n",
    "        print(f\"\\nQualitative examples (random {k}):\")\n",
    "        picked = random.sample(image_files, k)\n",
    "        sample_paths = [os.path.join(images_root, f) for f in picked]\n",
    "        preds = _generate_batch(sample_paths)\n",
    "        for pth, pred in zip(sample_paths, preds):\n",
    "            fname = os.path.basename(pth)\n",
    "            gt_refs = refs_map.get(fname, [])\n",
    "            print(\"\\nImage:\", fname)\n",
    "            if gt_refs:\n",
    "                print(\"Ground truths (up to 3):\")\n",
    "                for r in gt_refs[:3]:\n",
    "                    print(\" -\", r)\n",
    "            else:\n",
    "                print(\"(No references found)\")\n",
    "            print(\"Prediction:\")\n",
    "            print(\" -\", pred if pred else \"<EMPTY>\")\n",
    "            try:\n",
    "                img = Image.open(pth).convert(\"RGB\")\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(img); plt.axis(\"off\")\n",
    "                plt.title(fname)\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not display image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:35:27.562399Z",
     "iopub.status.busy": "2025-09-13T07:35:27.561531Z",
     "iopub.status.idle": "2025-09-13T07:40:47.803512Z",
     "shell.execute_reply": "2025-09-13T07:40:47.802852Z",
     "shell.execute_reply.started": "2025-09-13T07:35:27.562358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_test_set(model_mm, test_dataset, images_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:17:49.699083Z",
     "iopub.status.busy": "2025-09-12T14:17:49.698799Z",
     "iopub.status.idle": "2025-09-12T14:17:49.847297Z",
     "shell.execute_reply": "2025-09-12T14:17:49.846601Z",
     "shell.execute_reply.started": "2025-09-12T14:17:49.699062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Single Image Inference with Metrics (BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE)\n",
    "from typing import Optional, Dict, Any, List\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, torch\n",
    "from PIL import Image\n",
    "\n",
    "_PYCOCO_SCORERS = {}\n",
    "\n",
    "def _lazy_load_scorers():\n",
    "    global _PYCOCO_SCORERS\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.meteor.meteor import Meteor\n",
    "    from pycocoevalcap.rouge.rouge import Rouge\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    try:\n",
    "        from pycocoevalcap.spice.spice import Spice\n",
    "    except Exception:\n",
    "        Spice = None\n",
    "    if 'bleu' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['bleu'] = Bleu(4)\n",
    "    if 'meteor' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['meteor'] = Meteor()\n",
    "    if 'rouge' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['rouge'] = Rouge()\n",
    "    if 'cider' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['cider'] = Cider()\n",
    "    if Spice and 'spice' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['spice'] = Spice()\n",
    "    return _PYCOCO_SCORERS\n",
    "\n",
    "def _compute_single_caption_metrics(pred: str, refs: List[str]) -> Dict[str, float]:\n",
    "    refs_clean = [r.strip() for r in refs if r and r.strip()]\n",
    "    if not refs_clean:\n",
    "        return {}\n",
    "    scorers = _lazy_load_scorers()\n",
    "    gts = {0: refs_clean}\n",
    "    res = {0: [pred]}\n",
    "    out = {}\n",
    "    try:\n",
    "        bleu_scores, _ = scorers['bleu'].compute_score(gts, res)\n",
    "        out['bleu1'] = float(bleu_scores[0])\n",
    "        out['bleu2'] = float(bleu_scores[1])\n",
    "        out['bleu3'] = float(bleu_scores[2])\n",
    "        out['bleu4'] = float(bleu_scores[3])\n",
    "        out['bleu']  = float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "    try:\n",
    "        meteor_score, _ = scorers['meteor'].compute_score(gts, res)\n",
    "        out['meteor'] = float(meteor_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] METEOR failed:', e)\n",
    "    try:\n",
    "        rouge_score, _ = scorers['rouge'].compute_score(gts, res)\n",
    "        out['rougeL'] = float(rouge_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] ROUGE-L failed:', e)\n",
    "    try:\n",
    "        cider_score, _ = scorers['cider'].compute_score(gts, res)\n",
    "        out['cider'] = float(cider_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] CIDEr failed:', e)\n",
    "    if 'spice' in scorers:\n",
    "        try:\n",
    "            spice_score, spice_scores = scorers['spice'].compute_score(gts, res)\n",
    "            out['spice'] = float(spice_score)\n",
    "        except Exception as e:\n",
    "            print('[WARN] SPICE failed:', e)\n",
    "    return out\n",
    "\n",
    "def predict(\n",
    "    image_path: str,\n",
    "    prompt: str = \"Bu görüntüyü açıkla: \",\n",
    "    mode: str = \"beam\",              # 'beam' or 'sample'\n",
    "    max_refs: Optional[int] = 5,\n",
    "    show_image: bool = True,\n",
    "    show_refs: bool = True,\n",
    "    gen_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    json_file: Optional[str] = None,\n",
    "    ban_sentinels: bool = True,\n",
    "    compute_metrics: bool = True,\n",
    "    print_metrics: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n",
    "    jf = json_file or json_path\n",
    "    refs: List[str] = []\n",
    "    try:\n",
    "        with open(jf) as f:\n",
    "            data = json.load(f)\n",
    "        entries = data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "        target_name = os.path.basename(image_path)\n",
    "        for e in entries:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            if e.get('filename') == target_name:\n",
    "                for s in e.get('sentences', []):\n",
    "                    if isinstance(s, dict) and 'raw' in s:\n",
    "                        cap = s['raw'].strip()\n",
    "                        if cap:\n",
    "                            refs.append(cap)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse references ({e})\")\n",
    "    if max_refs is not None:\n",
    "        refs = refs[:max_refs]\n",
    "    gen_kwargs = (gen_kwargs or {}).copy()\n",
    "    if mode == \"sample\":\n",
    "        defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "    else:\n",
    "        defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3, num_beams=getattr(cfg, \"num_beams_infer\", 4))\n",
    "    for k, v in defaults.items():\n",
    "        gen_kwargs.setdefault(k, v)\n",
    "    local_amp_dtype = globals().get(\"amp_dtype\", None)\n",
    "    model_mm.eval()\n",
    "    if local_amp_dtype is not None:\n",
    "        with torch.amp.autocast('cuda', dtype=local_amp_dtype):\n",
    "            pred = model_mm.generate(\n",
    "                image_paths=[image_path],\n",
    "                prompt=prompt,\n",
    "                ban_sentinels=ban_sentinels,\n",
    "                **gen_kwargs\n",
    "            )[0]\n",
    "    else:\n",
    "        pred = model_mm.generate(\n",
    "            image_paths=[image_path],\n",
    "            prompt=prompt,\n",
    "            ban_sentinels=ban_sentinels,\n",
    "            **gen_kwargs\n",
    "        )[0]\n",
    "    metrics: Dict[str, float] = {}\n",
    "    if compute_metrics and refs:\n",
    "        try:\n",
    "            metrics = _compute_single_caption_metrics(pred if pred else \"<EMPTY>\", refs)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Metric computation failed: {e}\")\n",
    "    if show_image:\n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img); plt.axis(\"off\")\n",
    "            plt.title(\"Image\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not display image ({e})\")\n",
    "    if show_refs:\n",
    "        if refs:\n",
    "            print(\"References:\")\n",
    "            for i, r in enumerate(refs, 1):\n",
    "                print(f\"  {i}. {r}\")\n",
    "        else:\n",
    "            print(\"(No references found)\")\n",
    "    print(\"Prediction:\")\n",
    "    print(\" \", pred)\n",
    "    if print_metrics and metrics:\n",
    "        print(\"\\nMetrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"  {k}: {v}\")\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"prediction\": pred,\n",
    "        \"references\": refs,\n",
    "        \"metrics\": metrics,\n",
    "        \"mode\": mode,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:17:56.424026Z",
     "iopub.status.busy": "2025-09-12T14:17:56.423733Z",
     "iopub.status.idle": "2025-09-12T14:18:17.982071Z",
     "shell.execute_reply": "2025-09-12T14:18:17.981246Z",
     "shell.execute_reply.started": "2025-09-12T14:17:56.424006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8247064,
     "sourceId": 13025325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8247076,
     "sourceId": 13025342,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
