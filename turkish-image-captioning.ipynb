{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP (ViT-B/32) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze, edit: unfreeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-time Metrics Added\n",
    "We now compute the following on the full test set:\n",
    "- BLEU-1, BLEU-2, BLEU-3, BLEU-4\n",
    "- METEOR\n",
    "- ROUGE-L\n",
    "- CIDEr\n",
    "\n",
    "BERTScore F1 has been removed from evaluation and is no longer required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:52:38.584599Z",
     "iopub.status.busy": "2025-09-13T11:52:38.584025Z",
     "iopub.status.idle": "2025-09-13T11:54:47.920499Z",
     "shell.execute_reply": "2025-09-13T11:54:47.919734Z",
     "shell.execute_reply.started": "2025-09-13T11:52:38.584576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Rerun this cell at each session start\n",
    "\n",
    "# Uninstall conflicting packages (Kaggle specific)\n",
    "!pip uninstall -y bigframes cesium gcsfs\n",
    "\n",
    "# Performance metrics\n",
    "!pip install -r /kaggle/input/requirements/requirements.txt\n",
    "\n",
    "# To use nltk\n",
    "import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\n",
    "\n",
    "# Download mT5-small ViT-B/32\n",
    "import clip, torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "mt5 = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"Setup Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:54:47.922272Z",
     "iopub.status.busy": "2025-09-13T11:54:47.921623Z",
     "iopub.status.idle": "2025-09-13T11:54:48.047741Z",
     "shell.execute_reply": "2025-09-13T11:54:48.046978Z",
     "shell.execute_reply.started": "2025-09-13T11:54:47.922241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Unified configuration (wandb init moved to next cell)\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"bites-tr-image-captioning\"\n",
    "RUN_NAME = \"clip_mt5_prefix_run\"\n",
    "\n",
    "ENABLE_WANDB = True  # Set False to skip W&B entirely\n",
    "\n",
    "WANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/32\",  # backbone\n",
    "    \"prefix_tokens\": 32,          # stronger conditioning\n",
    "    \"batch_size\": 16,              # reduced to mitigate OOM\n",
    "    \"grad_accum_steps\": 2,        # accumulate to simulate larger effective batch\n",
    "    \"enable_t5_gradient_checkpointing\": True,  # reduce memory\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 50,                  # allow a bit longer now that we fine-tune CLIP\n",
    "    \"dataset_limit\": None,\n",
    "    # --- CLIP fine-tuning controls ---\n",
    "    \"freeze_clip\": False,          # set False to allow full CLIP fine-tuning\n",
    "    \"unfreeze_clip_last_n\": 0,     # if >0 unfreezes only last N vision blocks\n",
    "    \"clip_lr_scale\": 0.05,         # scaled LR for ALL CLIP params (lower than main)\n",
    "    \"use_clip_patch_tokens\": True, # richer conditioning (patch tokens path) (set False to save memory)\n",
    "    # --- T5 freezing ---\n",
    "    \"freeze_t5_encoder\": False,    # unfreeze encoder so it can adapt\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    # --- Optimization ---\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 500,           # linear warmup steps before cosine decay\n",
    "    # --- Inference defaults ---\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    \"use_amp\": True,\n",
    "    # Early stop\n",
    "    \"early_stop_patience\": 5,\n",
    "    \"early_stop_min_delta\": 0.001,\n",
    "    # Optional extras:\n",
    "    \"use_bf16\": True,\n",
    "    \"enable_tf32\": True,\n",
    "    \"finite_loss_skip\": True,\n",
    "    \"save_every\": 0\n",
    "}\n",
    "\n",
    "# Global flags/handles populated after (optional) wandb init cell\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "\n",
    "# Always create a local cfg object here; W&B init (next cell) can sync/override\n",
    "class _Cfg: ...\n",
    "cfg = _Cfg()\n",
    "for k, v in base_config.items():\n",
    "    setattr(cfg, k, v)\n",
    "print(\"[INFO] Local config object created.\")\n",
    "if ENABLE_WANDB:\n",
    "    print(\"[INFO] Run the next 'W&B Init' cell to enable Weights & Biases tracking.\")\n",
    "else:\n",
    "    print(\"[INFO] W&B disabled (ENABLE_WANDB=False). Using local config only.\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# --- Enforce CUDA-only environment ---\n",
    "assert torch.cuda.is_available(), \"CUDA GPU is required but not detected. Please run in a CUDA-enabled environment.\"\n",
    "device = torch.device('cuda')\n",
    "print(f\"[DEVICE] Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Performance toggles\n",
    "if getattr(cfg, 'enable_tf32', False):\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"[DEVICE] TF32 enabled.\")\n",
    "    except Exception as _e:\n",
    "        print(\"[WARN] Could not enable TF32:\", _e)\n",
    "\n",
    "if getattr(cfg, 'use_bf16', False):\n",
    "    bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    print(f\"[DEVICE] bfloat16 support: {bf16_ok}\")\n",
    "\n",
    "print(\"Active config (local):\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:54:48.416041Z",
     "iopub.status.busy": "2025-09-13T11:54:48.415668Z",
     "iopub.status.idle": "2025-09-13T11:54:51.856129Z",
     "shell.execute_reply": "2025-09-13T11:54:51.855310Z",
     "shell.execute_reply.started": "2025-09-13T11:54:48.416018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# (CUDA enforcement handled in config cell; assume cuda device later)\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model, use_fast=False)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        # CLIP freezing / unfreezing strategy\n",
    "        if cfg.unfreeze_clip_last_n and cfg.unfreeze_clip_last_n > 0:\n",
    "            # Freeze everything first\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "            blocks = list(self.clip.visual.transformer.resblocks)\n",
    "            for block in blocks[-cfg.unfreeze_clip_last_n:]:\n",
    "                for p in block.parameters():\n",
    "                    p.requires_grad = True\n",
    "        else:\n",
    "            # Respect freeze_clip flag (False means fully trainable)\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = not cfg.freeze_clip\n",
    "        # T5 freeze toggles\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine embedding dim dynamically (CLIP projection output)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    def _encode_image_single(self, images: torch.Tensor):\n",
    "        # (B, D) pooled embedding (already passed through ln_post + proj inside encode_image)\n",
    "        pooled = self.clip.encode_image(images)\n",
    "        return pooled\n",
    "\n",
    "    def _encode_image_patch_tokens(self, images: torch.Tensor):\n",
    "        \"\"\"Return patch-averaged embedding projected into CLIP joint space.\n",
    "        We manually replicate encode_image path but average patch tokens (excluding CLS),\n",
    "        then apply ln_post and proj so final dim == visual.output_dim (e.g. 512) to match ProjectionMLP in_dim.\"\"\"\n",
    "        visual = self.clip.visual\n",
    "        x = visual.conv1(images)                      # (B, width, grid, grid)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)     # (B, width, patches)\n",
    "        x = x.permute(0, 2, 1)                        # (B, patches, width)\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)         # prepend CLS\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        x = visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)                        # (sequence, B, width)\n",
    "        for block in visual.transformer.resblocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(1, 0, 2)                        # (B, sequence, width)\n",
    "        patches = x[:, 1:, :]                         # drop CLS\n",
    "        pooled = patches.mean(dim=1)                  # (B, width)\n",
    "        if hasattr(visual, 'ln_post'):\n",
    "            pooled = visual.ln_post(pooled)\n",
    "        if hasattr(visual, 'proj') and visual.proj is not None:\n",
    "            pooled = pooled @ visual.proj             # (B, output_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        images = images.to(device)\n",
    "        clip_emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        images = images.to(next(self.parameters()).device)\n",
    "        emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        return self.proj(emb)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu görüntüyü açıkla: \", ban_sentinels=True, **gen_kwargs):\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T11:10:04.044472Z",
     "iopub.status.busy": "2025-09-12T11:10:04.043853Z",
     "iopub.status.idle": "2025-09-12T11:10:22.353930Z",
     "shell.execute_reply": "2025-09-12T11:10:22.353055Z",
     "shell.execute_reply.started": "2025-09-12T11:10:04.044446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# (Optional) Weights & Biases initialization\n",
    "# Run this AFTER the config cell if you want online logging,\n",
    "# otherwise skip to keep everything offline.\n",
    "if 'ENABLE_WANDB' in globals() and ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config, reinit=True)\n",
    "        cfg = wandb.config  # sync cfg to wandb\n",
    "        # Explicitly log config dict to the run (config, summary, and a one-time log)\n",
    "        try:\n",
    "            cfg_dict = dict(base_config)\n",
    "        except Exception:\n",
    "            cfg_dict = {k: getattr(cfg, k) for k in base_config.keys() if hasattr(cfg, k)}\n",
    "        wandb.config.update(cfg_dict, allow_val_change=True)\n",
    "        # Store a namespaced copy in summary for quick viewing\n",
    "        wandb.summary.update({f\"cfg/{k}\": v for k, v in cfg_dict.items()})\n",
    "        # Also log once at step 0 for time-series traceability\n",
    "        wandb.log({\"cfg\": cfg_dict}, step=0)\n",
    "        use_wandb = True\n",
    "        print('[wandb] run initialized and config logged.')\n",
    "    except Exception as e:\n",
    "        use_wandb = False\n",
    "        print('[wandb] disabled (init failed):', e)\n",
    "else:\n",
    "    print('[wandb] Skipped (ENABLE_WANDB is False).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:33:12.704959Z",
     "iopub.status.busy": "2025-09-13T07:33:12.704573Z",
     "iopub.status.idle": "2025-09-13T07:33:35.069473Z",
     "shell.execute_reply": "2025-09-13T07:33:35.068667Z",
     "shell.execute_reply.started": "2025-09-13T07:33:12.704922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "json_path = '/kaggle/input/tasviret/flickr8k/tasviret8k_captions.json'\n",
    "images_root = '/kaggle/input/tasviret/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=None)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Clip ViT-B/32 params:\", sum(p.numel() for p in model_mm.clip.parameters() if p.requires_grad))\n",
    "print(\"Projection params:\", sum(p.numel() for p in model_mm.proj.parameters() if p.requires_grad))\n",
    "print(\"mt5-small params:\", sum(p.numel() for p in model_mm.model.parameters() if p.requires_grad))\n",
    "print(\"Total trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T11:11:42.416565Z",
     "iopub.status.busy": "2025-09-12T11:11:42.415804Z",
     "iopub.status.idle": "2025-09-12T13:59:44.149541Z",
     "shell.execute_reply": "2025-09-12T13:59:44.148749Z",
     "shell.execute_reply.started": "2025-09-12T11:11:42.416535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training cell: loss + BLEU1 early stopping + grad accum + checkpoints + history lists (fixed for this pipeline)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required.\"\n",
    "device = torch.device(\"cuda\")\n",
    "model_mm.to(device)\n",
    "\n",
    "# Helper: compute BLEU on validation set using existing generation API\n",
    "from collections import defaultdict\n",
    "\n",
    "def _compute_val_bleu(model, val_dataset, images_root, batch_size=16, amp_dtype=None):\n",
    "    if val_dataset is None or len(val_dataset) == 0:\n",
    "        return None, None\n",
    "    refs_map = defaultdict(list)\n",
    "    for fname, cap in val_dataset.samples:\n",
    "        c = str(cap).strip()\n",
    "        if c:\n",
    "            refs_map[fname].append(c)\n",
    "    if not refs_map:\n",
    "        return None, None\n",
    "    image_files = list(refs_map.keys())\n",
    "    image_paths = [os.path.join(images_root, f) for f in image_files]\n",
    "\n",
    "    def _generate_batch(paths):\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    hyps, refs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            outs = _generate_batch(batch_paths)\n",
    "            for pth, pred in zip(batch_paths, outs):\n",
    "                fname = os.path.basename(pth)\n",
    "                hyps.append(pred if pred else \"<EMPTY>\")\n",
    "                refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "    try:\n",
    "        from pycocoevalcap.bleu.bleu import Bleu\n",
    "        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "        return float(bleu_scores[0]), float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_model_with_bleu(\n",
    "    model_mm,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    tokenizer,  # kept for signature compatibility; not used directly here\n",
    "    cfg,\n",
    "    num_epochs=None,\n",
    "    grad_accum_steps=None,\n",
    "    min_epochs_before_bleu=10,\n",
    "    bleu_patience=5,\n",
    "):\n",
    "    # Resolve config values from cfg object\n",
    "    lr = getattr(cfg, 'lr')\n",
    "    weight_decay = getattr(cfg, 'weight_decay', 0.01)\n",
    "    warmup_steps = getattr(cfg, 'warmup_steps', 0)\n",
    "    epochs = num_epochs if num_epochs is not None else getattr(cfg, 'epochs', 30)\n",
    "    grad_accum = grad_accum_steps if grad_accum_steps is not None else getattr(cfg, 'grad_accum_steps', 1)\n",
    "\n",
    "    # Optional AMP dtype per earlier cells\n",
    "    amp_dtype = None\n",
    "    if getattr(cfg, 'use_amp', True):\n",
    "        if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "            amp_dtype = torch.bfloat16\n",
    "        else:\n",
    "            amp_dtype = torch.float16\n",
    "\n",
    "    model_mm.train()\n",
    "\n",
    "    # Optimizer: keep CLIP LR scale logic similar to earlier cells\n",
    "    main_params, clip_params = [], []\n",
    "    for name, p in model_mm.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (clip_params if name.startswith('clip.') else main_params).append(p)\n",
    "    param_groups = []\n",
    "    if main_params:\n",
    "        param_groups.append({\"params\": main_params, \"lr\": lr})\n",
    "    if clip_params:\n",
    "        scaled_lr = lr * getattr(cfg, 'clip_lr_scale', 0.05)\n",
    "        param_groups.append({\"params\": clip_params, \"lr\": scaled_lr})\n",
    "        print(f\"[INFO] CLIP fine-tune params: {len(clip_params)} with lr={scaled_lr:.2e}\")\n",
    "\n",
    "    optimizer = AdamW(param_groups, weight_decay=weight_decay)\n",
    "\n",
    "    # Simple warmup to 1.0; matches your snippet intent\n",
    "    scheduler = LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) / float(max(1, warmup_steps)), 1.0) if warmup_steps > 0 else 1.0,\n",
    "    )\n",
    "\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    best_path = os.path.join('checkpoints', 'best.pt')\n",
    "    last_path = os.path.join('checkpoints', 'last.pt')\n",
    "\n",
    "    best_bleu = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    global_step = 0\n",
    "\n",
    "    train_losses, val_losses, epochs_list = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # === Training loop ===\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            imgs, srcs, tgts = batch\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            if amp_dtype is not None:\n",
    "                with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                    outputs = model_mm(imgs, srcs, tgts)\n",
    "                    loss = outputs.loss / grad_accum\n",
    "                loss.backward()\n",
    "            else:\n",
    "                outputs = model_mm(imgs, srcs, tgts)\n",
    "                loss = outputs.loss / grad_accum\n",
    "                loss.backward()\n",
    "\n",
    "            if (step + 1) % grad_accum == 0:\n",
    "                if getattr(cfg, 'grad_clip', 0.0) > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model_mm.parameters(), getattr(cfg, 'grad_clip', 1.0))\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "\n",
    "        # === Validation loss ===\n",
    "        model_mm.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                if amp_dtype is not None:\n",
    "                    with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                        outputs = model_mm(imgs, srcs, tgts)\n",
    "                        total_val_loss += outputs.loss.item()\n",
    "                else:\n",
    "                    outputs = model_mm(imgs, srcs, tgts)\n",
    "                    total_val_loss += outputs.loss.item()\n",
    "        avg_val_loss = total_val_loss / max(1, len(val_loader))\n",
    "\n",
    "        # Save losses to lists\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        epochs_list.append(epoch + 1)\n",
    "\n",
    "        # === BLEU evaluation ===\n",
    "        bleu1 = bleu4 = None\n",
    "        if epoch >= min_epochs_before_bleu:\n",
    "            try:\n",
    "                bleu1, bleu4 = _compute_val_bleu(model_mm, val_dataset, images_root, batch_size=16, amp_dtype=amp_dtype)\n",
    "            except Exception as e:\n",
    "                print('[WARN] BLEU eval failed:', e)\n",
    "\n",
    "            # Check improvement\n",
    "            if bleu1 is not None and bleu1 > best_bleu:\n",
    "                best_bleu = bleu1\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'model': model_mm.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'best_bleu': best_bleu,\n",
    "                        'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "                    },\n",
    "                    best_path,\n",
    "                )\n",
    "                print(f\"[Epoch {epoch}] New best BLEU-1={bleu1:.4f} saved at {best_path}\")\n",
    "            elif bleu1 is not None:\n",
    "                no_improve += 1\n",
    "\n",
    "        # Save rolling checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': model_mm.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "            },\n",
    "            last_path,\n",
    "        )\n",
    "\n",
    "        # Log (guard W&B)\n",
    "        time_elapsed = time.time() - t0\n",
    "        try:\n",
    "            if 'use_wandb' in globals() and use_wandb:\n",
    "                log_dict = {\n",
    "                    'train/epoch_loss': avg_train_loss,\n",
    "                    'val/epoch_loss': avg_val_loss,\n",
    "                    'lr': scheduler.get_last_lr()[0],\n",
    "                }\n",
    "                if epoch >= min_epochs_before_bleu and bleu1 is not None:\n",
    "                    log_dict['val/bleu1'] = bleu1\n",
    "                    if bleu4 is not None:\n",
    "                        log_dict['val/bleu4'] = bleu4\n",
    "                wandb.log(log_dict, step=epoch)\n",
    "        except Exception as e:\n",
    "            print('[wandb] log skipped:', e)\n",
    "\n",
    "        # Print summary (include BLEU after activation)\n",
    "        if epoch >= min_epochs_before_bleu and bleu1 is not None:\n",
    "            print(f\"[Epoch {epoch}] Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | BLEU-1={bleu1:.4f} | Time={time_elapsed:.1f}s\")\n",
    "        else:\n",
    "            print(f\"[Epoch {epoch}] Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Time={time_elapsed:.1f}s\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch >= min_epochs_before_bleu and no_improve >= bleu_patience:\n",
    "            print(f\"⏹ Early stopping at epoch {epoch}. Best BLEU-1={best_bleu:.4f} at epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        model_mm.train()\n",
    "\n",
    "    return best_bleu, best_epoch, train_losses, val_losses, epochs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_bleu, best_epoch, train_losses, val_losses, epochs_list = train_model_with_bleu(\n",
    "    model_mm,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_mm.tokenizer,\n",
    "    cfg,\n",
    "    num_epochs=cfg.epochs,\n",
    "    grad_accum_steps=cfg.grad_accum_steps,\n",
    "    min_epochs_before_bleu=8,\n",
    "    bleu_patience=getattr(cfg, 'bleu_patience', 5),\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T13:59:50.269363Z",
     "iopub.status.busy": "2025-09-12T13:59:50.269067Z",
     "iopub.status.idle": "2025-09-12T13:59:50.865811Z",
     "shell.execute_reply": "2025-09-12T13:59:50.865126Z",
     "shell.execute_reply.started": "2025-09-12T13:59:50.269338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss curves in the same plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'epochs_list' in globals() and len(epochs_list) == len(train_losses) == len(val_losses) and len(epochs_list) > 0:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(epochs_list, train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs_list, val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No loss history found. Run the training cell first to populate train_losses/val_losses/epochs_list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T13:59:58.418248Z",
     "iopub.status.busy": "2025-09-12T13:59:58.417413Z",
     "iopub.status.idle": "2025-09-12T13:59:58.458044Z",
     "shell.execute_reply": "2025-09-12T13:59:58.457309Z",
     "shell.execute_reply.started": "2025-09-12T13:59:58.418223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # === Model Export / Import Utilities ===\n",
    "# import os, json, torch, math\n",
    "# from datetime import datetime\n",
    "# from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# # ------------------------------\n",
    "# # Export\n",
    "# # ------------------------------\n",
    "\n",
    "# def export_model(\n",
    "#     save_dir: str,\n",
    "#     model: torch.nn.Module,\n",
    "#     cfg_obj,\n",
    "#     optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "#     scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "#     epoch: Optional[int] = None,\n",
    "#     global_step: Optional[int] = None,\n",
    "#     best_val: Optional[float] = None,\n",
    "#     tag: str = \"latest\",\n",
    "#     use_safetensors: bool = False,\n",
    "#     # --- BLEU-aware exporting additions ---\n",
    "#     best_bleu: Optional[float] = None,\n",
    "#     bleu_scores: Optional[Dict[str, float]] = None,  # e.g., {'bleu1': ..., 'bleu4': ...}\n",
    "#     save_best_on_bleu: bool = False,\n",
    "#     bleu_min_delta: float = 0.0,\n",
    "#     best_registry_filename: str = \"best_bleu.json\",\n",
    "# ) -> str:\n",
    "#     \"\"\"Export model checkpoint + (optional) optimizer/scheduler.\n",
    "\n",
    "#     Saves:\n",
    "#       - config.json (raw cfg values)\n",
    "#       - tokenizer/ (HF tokenizer)\n",
    "#       - clip_mt5_prefix_<tag>.pt (bundle) OR .safetensors (+ meta files)\n",
    "\n",
    "#     Bundle (.pt) contains:\n",
    "#       model_state, cfg, epoch, global_step, tag, export_time,\n",
    "#       optimizer_state?, scheduler_state?, best_val?, best_bleu?, bleu_scores?, hyperparams.\n",
    "\n",
    "#     If save_best_on_bleu=True, the function will only save when `best_bleu`\n",
    "#     strictly improves over the previous best by > bleu_min_delta. Best state is\n",
    "#     tracked in <save_dir>/<best_registry_filename>.\n",
    "\n",
    "#     hyperparams dict added so retraining can auto‑rebuild optimizer/scheduler:\n",
    "#       { lr, clip_lr_scale, weight_decay, warmup_steps, grad_clip, use_amp, use_bf16 }\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     # Collect base config keys if available\n",
    "#     if 'base_config' in globals():\n",
    "#         cfg_dict = {k: getattr(cfg_obj, k) for k in base_config.keys() if hasattr(cfg_obj, k)}\n",
    "#     else:\n",
    "#         cfg_dict = {k: v for k, v in vars(cfg_obj).items() if not k.startswith('_')}\n",
    "\n",
    "#     # Persist config separately (human readable)\n",
    "#     with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "#         json.dump(cfg_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     # Save tokenizer (best effort)\n",
    "#     try:\n",
    "#         model.tokenizer.save_pretrained(os.path.join(save_dir, 'tokenizer'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] Tokenizer save failed: {e}\")\n",
    "\n",
    "#     # Load current best BLEU from registry (if gating by BLEU)\n",
    "#     registry_path = os.path.join(save_dir, best_registry_filename)\n",
    "#     prev_best_bleu = None\n",
    "#     prev_best_path = None\n",
    "#     if os.path.isfile(registry_path):\n",
    "#         try:\n",
    "#             with open(registry_path, 'r') as rf:\n",
    "#                 reg = json.load(rf)\n",
    "#             prev_best_bleu = reg.get('best_bleu')\n",
    "#             prev_best_path = reg.get('path')\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     if save_best_on_bleu:\n",
    "#         if best_bleu is None:\n",
    "#             print(\"[EXPORT] save_best_on_bleu=True but best_bleu=None; skip saving.\")\n",
    "#             return prev_best_path or \"\"\n",
    "#         improved = (prev_best_bleu is None) or ((best_bleu - float(prev_best_bleu)) > float(bleu_min_delta))\n",
    "#         if not improved:\n",
    "#             print(f\"[EXPORT] BLEU not improved (cur={best_bleu}, best={prev_best_bleu}); skip.\")\n",
    "#             return prev_best_path or \"\"\n",
    "#         # If we're saving by BLEU and tag isn't explicit, use 'best'\n",
    "#         if tag in (None, \"\", \"latest\"):\n",
    "#             tag = \"best\"\n",
    "\n",
    "#     checkpoint_name = f\"clip_mt5_prefix_{tag}\"\n",
    "\n",
    "#     hyperparams = {\n",
    "#         'lr': cfg_dict.get('lr'),\n",
    "#         'clip_lr_scale': cfg_dict.get('clip_lr_scale'),\n",
    "#         'weight_decay': cfg_dict.get('weight_decay'),\n",
    "#         'warmup_steps': cfg_dict.get('warmup_steps'),\n",
    "#         'grad_clip': cfg_dict.get('grad_clip'),\n",
    "#         'use_amp': cfg_dict.get('use_amp'),\n",
    "#         'use_bf16': cfg_dict.get('use_bf16'),\n",
    "#         'batch_size': cfg_dict.get('batch_size'),\n",
    "#     }\n",
    "\n",
    "#     # Common metadata for both formats\n",
    "#     meta = {\n",
    "#         'cfg': cfg_dict,\n",
    "#         'epoch': epoch,\n",
    "#         'global_step': global_step,\n",
    "#         'export_time': datetime.utcnow().isoformat() + 'Z',\n",
    "#         'tag': tag,\n",
    "#         'best_val': best_val,\n",
    "#         'best_bleu': best_bleu,\n",
    "#         'bleu_scores': bleu_scores,\n",
    "#         'has_optimizer': optimizer is not None,\n",
    "#         'has_scheduler': scheduler is not None,\n",
    "#         'hyperparams': hyperparams,\n",
    "#     }\n",
    "\n",
    "#     saved_path = \"\"\n",
    "\n",
    "#     if use_safetensors:\n",
    "#         try:\n",
    "#             from safetensors.torch import save_file\n",
    "#             weights = model.state_dict()\n",
    "#             save_path = os.path.join(save_dir, checkpoint_name + '.safetensors')\n",
    "#             save_file(weights, save_path)\n",
    "#             if optimizer:\n",
    "#                 torch.save(optimizer.state_dict(), os.path.join(save_dir, checkpoint_name + '.optimizer.pt'))\n",
    "#             if scheduler:\n",
    "#                 torch.save(scheduler.state_dict(), os.path.join(save_dir, checkpoint_name + '.scheduler.pt'))\n",
    "#             # Write meta.json (always reflects last export)\n",
    "#             with open(os.path.join(save_dir, 'meta.json'), 'w') as f:\n",
    "#                 json.dump(meta, f, indent=2)\n",
    "#             print(f\"[EXPORT] Weights -> {checkpoint_name}.safetensors (meta.json written)\")\n",
    "#             saved_path = save_path\n",
    "#         except ImportError:\n",
    "#             print('[WARN] safetensors not installed; falling back to .pt')\n",
    "#             use_safetensors = False\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] safetensors save failed: {e}\")\n",
    "#             return prev_best_path or \"\"\n",
    "\n",
    "#     if not use_safetensors:\n",
    "#         # Standard .pt route\n",
    "#         bundle = {\n",
    "#             'model_state': model.state_dict(),\n",
    "#             'cfg': cfg_dict,\n",
    "#             'epoch': epoch,\n",
    "#             'global_step': global_step,\n",
    "#             'export_time': datetime.utcnow().isoformat() + 'Z',\n",
    "#             'tag': tag,\n",
    "#             'best_val': best_val,\n",
    "#             'best_bleu': best_bleu,\n",
    "#             'bleu_scores': bleu_scores,\n",
    "#             'hyperparams': hyperparams,\n",
    "#         }\n",
    "#         if optimizer:\n",
    "#             bundle['optimizer_state'] = optimizer.state_dict()\n",
    "#         if scheduler:\n",
    "#             bundle['scheduler_state'] = scheduler.state_dict()\n",
    "#         out_path = os.path.join(save_dir, checkpoint_name + '.pt')\n",
    "#         torch.save(bundle, out_path)\n",
    "#         # Also emit meta.json for parity with safetensors\n",
    "#         with open(os.path.join(save_dir, 'meta.json'), 'w') as f:\n",
    "#             json.dump(meta, f, indent=2)\n",
    "#         print(f\"[EXPORT] Saved checkpoint: {out_path}\")\n",
    "#         saved_path = out_path\n",
    "\n",
    "#     # Update best BLEU registry if we saved under BLEU gating\n",
    "#     if save_best_on_bleu and best_bleu is not None and saved_path:\n",
    "#         try:\n",
    "#             reg = {\n",
    "#                 'best_bleu': float(best_bleu),\n",
    "#                 'bleu_scores': bleu_scores,\n",
    "#                 'path': saved_path,\n",
    "#                 'epoch': epoch,\n",
    "#                 'global_step': global_step,\n",
    "#                 'updated_at': datetime.utcnow().isoformat() + 'Z',\n",
    "#             }\n",
    "#             with open(registry_path, 'w') as wf:\n",
    "#                 json.dump(reg, wf, indent=2)\n",
    "#             print(f\"[EXPORT] Updated registry {best_registry_filename}: best_bleu={best_bleu:.4f}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] Could not update best BLEU registry: {e}\")\n",
    "\n",
    "#     return saved_path\n",
    "\n",
    "# # Convenience wrapper: export only if BLEU improved, saved as 'best'\n",
    "# def export_best_bleu(\n",
    "#     save_dir: str,\n",
    "#     model: torch.nn.Module,\n",
    "#     cfg_obj,\n",
    "#     *,\n",
    "#     best_bleu: float,\n",
    "#     bleu_scores: Optional[Dict[str, float]] = None,\n",
    "#     optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "#     scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "#     epoch: Optional[int] = None,\n",
    "#     global_step: Optional[int] = None,\n",
    "#     use_safetensors: bool = False,\n",
    "#     bleu_min_delta: float = 0.0,\n",
    "#     best_registry_filename: str = \"best_bleu.json\",\n",
    "# ) -> str:\n",
    "#     return export_model(\n",
    "#         save_dir=save_dir,\n",
    "#         model=model,\n",
    "#         cfg_obj=cfg_obj,\n",
    "#         optimizer=optimizer,\n",
    "#         scheduler=scheduler,\n",
    "#         epoch=epoch,\n",
    "#         global_step=global_step,\n",
    "#         best_val=None,\n",
    "#         tag=\"best\",\n",
    "#         use_safetensors=use_safetensors,\n",
    "#         best_bleu=best_bleu,\n",
    "#         bleu_scores=bleu_scores,\n",
    "#         save_best_on_bleu=True,\n",
    "#         bleu_min_delta=bleu_min_delta,\n",
    "#         best_registry_filename=best_registry_filename,\n",
    "#     )\n",
    "\n",
    "# # ------------------------------\n",
    "# # Helper: build optimizer (main + clip groups)\n",
    "# # ------------------------------\n",
    "\n",
    "# def build_optimizer_from_hparams(model: torch.nn.Module, h: Dict[str, Any]):\n",
    "#     lr = h.get('lr', 1e-4)\n",
    "#     clip_lr_scale = h.get('clip_lr_scale', 0.05) or 0.05\n",
    "#     weight_decay = h.get('weight_decay', 0.0)\n",
    "#     main_params, clip_params = [], []\n",
    "#     for n, p in model.named_parameters():\n",
    "#         if not p.requires_grad:\n",
    "#             continue\n",
    "#         (clip_params if n.startswith('clip.') else main_params).append(p)\n",
    "#     param_groups = []\n",
    "#     if main_params:\n",
    "#         param_groups.append({'params': main_params, 'lr': lr})\n",
    "#     if clip_params:\n",
    "#         param_groups.append({'params': clip_params, 'lr': lr * clip_lr_scale})\n",
    "#     if clip_params:\n",
    "#         print(f\"[OPT] CLIP params: {len(clip_params)} lr={lr * clip_lr_scale:.2e}\")\n",
    "#     return torch.optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "\n",
    "# # ------------------------------\n",
    "# # Helper: build cosine scheduler with warmup (same as training)\n",
    "# # ------------------------------\n",
    "\n",
    "# def build_scheduler_from_hparams(optimizer, h: Dict[str, Any], steps_per_epoch: int, total_epochs: int):\n",
    "#     warmup_steps = h.get('warmup_steps', 0) or 0\n",
    "#     total_steps = steps_per_epoch * total_epochs\n",
    "#     def lr_lambda(step):\n",
    "#         if warmup_steps > 0 and step < warmup_steps:\n",
    "#             return float(step) / float(max(1, warmup_steps))\n",
    "#         progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps)) if total_steps > warmup_steps else 1.0\n",
    "#         progress = min(max(progress, 0.0), 1.0)\n",
    "#         return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "#     from torch.optim.lr_scheduler import LambdaLR\n",
    "#     return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# # ------------------------------\n",
    "# # Import for finetune (unchanged except now returns hyperparams)\n",
    "# # ------------------------------\n",
    "\n",
    "# def load_model_for_finetune(\n",
    "#     load_dir: str,\n",
    "#     device: torch.device,\n",
    "#     checkpoint_tag: str = 'latest',\n",
    "#     resume_optimizer: bool = True,\n",
    "#     build_optimizer_fn=None,\n",
    "#     build_scheduler_fn=None,\n",
    "#     override_cfg: Optional[Dict[str, Any]] = None,\n",
    "#     prefer_safetensors: bool = True\n",
    "# ):\n",
    "#     # Load config\n",
    "#     with open(os.path.join(load_dir, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "#         cfg_json = json.load(f)\n",
    "#     if override_cfg:\n",
    "#         cfg_json.update(override_cfg)\n",
    "\n",
    "#     class _Cfg: ...\n",
    "#     cfg_obj = _Cfg()\n",
    "#     for k, v in cfg_json.items():\n",
    "#         setattr(cfg_obj, k, v)\n",
    "\n",
    "#     model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "\n",
    "#     ckpt_base = f\"clip_mt5_prefix_{checkpoint_tag}\"\n",
    "#     safepath = os.path.join(load_dir, ckpt_base + '.safetensors')\n",
    "#     ptpath = os.path.join(load_dir, ckpt_base + '.pt')\n",
    "\n",
    "#     optimizer_state = scheduler_state = None\n",
    "#     epoch = global_step = None\n",
    "#     hyperparams = None\n",
    "#     used_safetensors = False\n",
    "\n",
    "#     if prefer_safetensors and os.path.isfile(safepath):\n",
    "#         try:\n",
    "#             from safetensors.torch import load_file\n",
    "#             weights = load_file(safepath, device=device)\n",
    "#             model.load_state_dict(weights, strict=True)\n",
    "#             used_safetensors = True\n",
    "#             meta_path = os.path.join(load_dir, 'meta.json')\n",
    "#             meta = {}\n",
    "#             if os.path.isfile(meta_path):\n",
    "#                 with open(meta_path, 'r') as f:\n",
    "#                     meta = json.load(f)\n",
    "#             epoch = meta.get('epoch'); global_step = meta.get('global_step')\n",
    "#             hyperparams = meta.get('hyperparams')\n",
    "#             # Optional: read BLEU metadata (not returned)\n",
    "#             _best_bleu = meta.get('best_bleu'); _bleu_scores = meta.get('bleu_scores')\n",
    "#             if resume_optimizer and meta.get('has_optimizer'):\n",
    "#                 opt_file = os.path.join(load_dir, ckpt_base + '.optimizer.pt')\n",
    "#                 if os.path.isfile(opt_file):\n",
    "#                     optimizer_state = torch.load(opt_file, map_location='cpu')\n",
    "#             if resume_optimizer and meta.get('has_scheduler'):\n",
    "#                 sch_file = os.path.join(load_dir, ckpt_base + '.scheduler.pt')\n",
    "#                 if os.path.isfile(sch_file):\n",
    "#                     scheduler_state = torch.load(sch_file, map_location='cpu')\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] safetensors load failed ({e}); falling back to .pt\")\n",
    "#             used_safetensors = False\n",
    "\n",
    "#     if not used_safetensors:\n",
    "#         if not os.path.isfile(ptpath):\n",
    "#             raise FileNotFoundError(f\"No checkpoint found at {ptpath}\")\n",
    "#         bundle = torch.load(ptpath, map_location=device)\n",
    "#         model.load_state_dict(bundle['model_state'], strict=True)\n",
    "#         epoch = bundle.get('epoch'); global_step = bundle.get('global_step')\n",
    "#         hyperparams = bundle.get('hyperparams')\n",
    "#         # Optional BLEU metadata\n",
    "#         _best_bleu = bundle.get('best_bleu'); _bleu_scores = bundle.get('bleu_scores')\n",
    "#         if resume_optimizer:\n",
    "#             optimizer_state = bundle.get('optimizer_state')\n",
    "#             scheduler_state = bundle.get('scheduler_state')\n",
    "\n",
    "#     print(f\"[IMPORT] Model loaded (epoch={epoch}, global_step={global_step})\")\n",
    "#     return model, cfg_obj, hyperparams, optimizer_state, scheduler_state, epoch, global_step\n",
    "\n",
    "# # ------------------------------\n",
    "# # Simple one-shot retrain loader\n",
    "# # ------------------------------\n",
    "\n",
    "# def load_model_for_retrain(\n",
    "#     checkpoint_path: str,\n",
    "#     device: torch.device,\n",
    "#     new_lr: Optional[float] = None,\n",
    "#     new_clip_lr_scale: Optional[float] = None,\n",
    "#     reset_optimizer: bool = True,\n",
    "#     freeze_clip: Optional[bool] = None,\n",
    "#     override_cfg: Optional[Dict[str, Any]] = None,\n",
    "# ):\n",
    "#     \"\"\"Load a .pt bundle and prepare model + (fresh) optimizer hyperparams for retraining.\n",
    "\n",
    "#     Returns (model, cfg_obj, optimizer_hparams, epoch_loaded, global_step_loaded)\n",
    "#     Caller should then: optimizer = build_optimizer_from_hparams(model, optimizer_hparams)\n",
    "#     and build a new scheduler with build_scheduler_from_hparams once steps_per_epoch known.\n",
    "#     \"\"\"\n",
    "#     assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "#     bundle = torch.load(checkpoint_path, map_location=device)\n",
    "#     if 'model_state' not in bundle:\n",
    "#         raise ValueError('Not an export_model bundle (.pt)')\n",
    "#     cfg_json = dict(bundle['cfg'])\n",
    "#     if override_cfg:\n",
    "#         cfg_json.update(override_cfg)\n",
    "\n",
    "#     class _Cfg: ...\n",
    "#     cfg_obj = _Cfg()\n",
    "#     for k, v in cfg_json.items():\n",
    "#         setattr(cfg_obj, k, v)\n",
    "#     if freeze_clip is not None:\n",
    "#         cfg_obj.freeze_clip = freeze_clip\n",
    "\n",
    "#     model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "#     model.load_state_dict(bundle['model_state'], strict=True)\n",
    "\n",
    "#     base_h = bundle.get('hyperparams', {})\n",
    "#     # Override learning rates if requested\n",
    "#     if new_lr is not None:\n",
    "#         base_h['lr'] = new_lr\n",
    "#     if new_clip_lr_scale is not None:\n",
    "#         base_h['clip_lr_scale'] = new_clip_lr_scale\n",
    "\n",
    "#     # If freezing clip newly, we still keep clip_lr_scale but it won't matter\n",
    "#     epoch_loaded = bundle.get('epoch')\n",
    "#     global_step_loaded = bundle.get('global_step')\n",
    "#     # Optional BLEU metadata\n",
    "#     _best_bleu = bundle.get('best_bleu'); _bleu_scores = bundle.get('bleu_scores')\n",
    "#     print(f\"[RETRAIN] Loaded weights (epoch={epoch_loaded}). Preparing fresh optimizer hyperparams.\")\n",
    "#     return model, cfg_obj, base_h, epoch_loaded, global_step_loaded\n",
    "\n",
    "# print('[READY] Enhanced export/import + retrain helpers available (BLEU-aware best checkpointing).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:00:07.673585Z",
     "iopub.status.busy": "2025-09-12T14:00:07.673278Z",
     "iopub.status.idle": "2025-09-12T14:00:17.607270Z",
     "shell.execute_reply": "2025-09-12T14:00:17.605489Z",
     "shell.execute_reply.started": "2025-09-12T14:00:07.673563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Export the model\n",
    "# export_best_bleu(\n",
    "#     save_dir='/kaggle/working/best_model/',\n",
    "#     model=model_mm,\n",
    "#     cfg_obj=cfg,\n",
    "#     best_bleu=best_bleu,\n",
    "#     bleu_scores={'bleu1': best_bleu},\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     epoch=epoch,\n",
    "#     global_step=global_step,\n",
    "#     use_safetensors=False,\n",
    "#     bleu_min_delta=bleu_min_delta,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:34:29.755646Z",
     "iopub.status.busy": "2025-09-13T07:34:29.754861Z",
     "iopub.status.idle": "2025-09-13T07:34:29.765396Z",
     "shell.execute_reply": "2025-09-13T07:34:29.764651Z",
     "shell.execute_reply.started": "2025-09-13T07:34:29.755618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Direct .pt Checkpoint Loader (robust) ===\n",
    "import torch, os\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "def load_model_from_checkpoint_file(\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    resume_optimizer: bool = False,\n",
    "    build_optimizer_fn=None,\n",
    "    build_scheduler_fn=None,\n",
    "    override_cfg: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"Load model (and optional optimizer/scheduler) directly from a single .pt file.\n",
    "    \n",
    "    Works with both:\n",
    "    - { 'model_state', 'optimizer_state', 'scheduler_state', 'cfg', ... }\n",
    "    - { 'model', 'optimizer', 'scheduler', 'cfg', ... }\n",
    "\n",
    "    Returns: (model, cfg_obj, optimizer, scheduler, epoch, global_step)\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "    bundle = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # --- Model state ---\n",
    "    if 'model_state' in bundle:\n",
    "        model_state = bundle['model_state']\n",
    "    elif 'model' in bundle:\n",
    "        model_state = bundle['model']\n",
    "    else:\n",
    "        raise ValueError(\"Checkpoint missing model weights (expected 'model_state' or 'model')\")\n",
    "\n",
    "    # --- Config ---\n",
    "    if 'cfg' not in bundle:\n",
    "        raise ValueError(\"Checkpoint missing 'cfg'\")\n",
    "    cfg_json = dict(bundle['cfg'])\n",
    "    if override_cfg:\n",
    "        cfg_json.update(override_cfg)\n",
    "\n",
    "    class _Cfg: ...\n",
    "    cfg_obj = _Cfg()\n",
    "    for k, v in cfg_json.items():\n",
    "        setattr(cfg_obj, k, v)\n",
    "\n",
    "    # --- Build model and load weights ---\n",
    "    model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "\n",
    "    # --- Metadata ---\n",
    "    epoch = bundle.get('epoch')\n",
    "    global_step = bundle.get('global_step')\n",
    "\n",
    "    # --- Optimizer and scheduler (optional) ---\n",
    "    optimizer, scheduler = None, None\n",
    "    if resume_optimizer:\n",
    "        # Optimizer\n",
    "        opt_state = bundle.get('optimizer_state') or bundle.get('optimizer')\n",
    "        if opt_state is not None and build_optimizer_fn is not None:\n",
    "            optimizer = build_optimizer_fn(cfg_obj, model)\n",
    "            try:\n",
    "                optimizer.load_state_dict(opt_state)\n",
    "                print('[IMPORT] Optimizer state restored.')\n",
    "            except Exception as e:\n",
    "                print(f'[WARN] Failed to load optimizer state: {e}')\n",
    "        elif opt_state is not None:\n",
    "            print('[WARN] Optimizer state present but build_optimizer_fn not provided; skipping restore.')\n",
    "\n",
    "        # Scheduler\n",
    "        sch_state = bundle.get('scheduler_state') or bundle.get('scheduler')\n",
    "        if sch_state is not None and optimizer and build_scheduler_fn is not None:\n",
    "            scheduler = build_scheduler_fn(cfg_obj, optimizer)\n",
    "            try:\n",
    "                scheduler.load_state_dict(sch_state)\n",
    "                print('[IMPORT] Scheduler state restored.')\n",
    "            except Exception as e:\n",
    "                print(f'[WARN] Failed to load scheduler state: {e}')\n",
    "\n",
    "    print(f\"[IMPORT] Loaded model from file: {checkpoint_path}\")\n",
    "    return model, cfg_obj, optimizer, scheduler, epoch, global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:00:32.494166Z",
     "iopub.status.busy": "2025-09-12T14:00:32.493916Z",
     "iopub.status.idle": "2025-09-12T14:00:32.779471Z",
     "shell.execute_reply": "2025-09-12T14:00:32.778764Z",
     "shell.execute_reply.started": "2025-09-12T14:00:32.494148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls /kaggle/working/best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:34:34.627740Z",
     "iopub.status.busy": "2025-09-13T07:34:34.627047Z",
     "iopub.status.idle": "2025-09-13T07:34:34.869215Z",
     "shell.execute_reply": "2025-09-13T07:34:34.868324Z",
     "shell.execute_reply.started": "2025-09-13T07:34:34.627705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:30:26.251733Z",
     "iopub.status.busy": "2025-09-12T14:30:26.251163Z",
     "iopub.status.idle": "2025-09-12T14:30:27.011403Z",
     "shell.execute_reply": "2025-09-12T14:30:27.010742Z",
     "shell.execute_reply.started": "2025-09-12T14:30:26.251712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Clean the GPU/CUDA memory\n",
    "# import torch, gc\n",
    "\n",
    "# def print_gpu_mem(prefix=\"\"):\n",
    "#     print(prefix,\n",
    "#           f\"allocated={torch.cuda.memory_allocated()/1024**2:.1f} MB\",\n",
    "#           f\"reserved={torch.cuda.memory_reserved()/1024**2:.1f} MB\")\n",
    "#     # optional verbose:\n",
    "#     # print(torch.cuda.memory_summary())\n",
    "\n",
    "# print_gpu_mem(\"BEFORE\")\n",
    "# del model_mm\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "# print_gpu_mem(\"AFTER empty_cache()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:34:37.324408Z",
     "iopub.status.busy": "2025-09-13T07:34:37.324091Z",
     "iopub.status.idle": "2025-09-13T07:35:08.849105Z",
     "shell.execute_reply": "2025-09-13T07:35:08.848337Z",
     "shell.execute_reply.started": "2025-09-13T07:34:37.324378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Load the model for inference\n",
    "model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/working/checkpoints/best.pt', device=torch.device('cuda'), resume_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:35:21.660859Z",
     "iopub.status.busy": "2025-09-13T07:35:21.660203Z",
     "iopub.status.idle": "2025-09-13T07:35:22.980384Z",
     "shell.execute_reply": "2025-09-13T07:35:22.979805Z",
     "shell.execute_reply.started": "2025-09-13T07:35:21.660834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test/Inference on the test_dataset\n",
    "import os, random, torch\n",
    "from collections import defaultdict\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_test_set(model, test_dataset, images_root, batch_size=16, top_k_examples=5):\n",
    "    if test_dataset is None or len(test_dataset) == 0:\n",
    "        print(\"No test dataset available.\")\n",
    "        return\n",
    "\n",
    "    # Build references map\n",
    "    refs_map = defaultdict(list)\n",
    "    for fname, cap in test_dataset.samples:\n",
    "        c = str(cap).strip()\n",
    "        if c:\n",
    "            refs_map[fname].append(c)\n",
    "\n",
    "    image_files = list(refs_map.keys())\n",
    "    image_paths = [os.path.join(images_root, f) for f in image_files]\n",
    "\n",
    "    print(f\"Evaluating on {len(image_paths)} test images…\")\n",
    "    model.eval()\n",
    "    hyps, refs = [], []\n",
    "\n",
    "    def _generate_batch(paths):\n",
    "        local_amp = globals().get('amp_dtype', None)\n",
    "        if local_amp is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=local_amp):\n",
    "                return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            outs = _generate_batch(batch_paths)\n",
    "            for pth, pred in zip(batch_paths, outs):\n",
    "                fname = os.path.basename(pth)\n",
    "                hyps.append(pred if pred else \"<EMPTY>\")\n",
    "                refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "    # Prepare for COCO eval\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "    # Compute BLEU, METEOR, ROUGE-L, CIDEr\n",
    "    try:\n",
    "        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] BLEU failed:\", e)\n",
    "        bleu_scores = [0,0,0,0]\n",
    "    try:\n",
    "        meteor_score, _ = Meteor().compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] METEOR failed:\", e)\n",
    "        meteor_score = 0\n",
    "    try:\n",
    "        rouge_score, _ = Rouge().compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] ROUGE-L failed:\", e)\n",
    "        rouge_score = 0\n",
    "    try:\n",
    "        cider_score, _ = Cider().compute_score(gts, res)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] CIDEr failed:\", e)\n",
    "        cider_score = 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nTest Set Metrics:\\nBLEU-1: {bleu_scores[0]:.4f}  BLEU-2: {bleu_scores[1]:.4f}  BLEU-3: {bleu_scores[2]:.4f}  BLEU-4: {bleu_scores[3]:.4f}\")\n",
    "    print(f\"METEOR: {meteor_score:.4f}  ROUGE-L: {rouge_score:.4f}  CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "    # Show qualitative examples\n",
    "    k = min(top_k_examples, len(image_files))\n",
    "    if k > 0:\n",
    "        print(f\"\\nQualitative examples (random {k}):\")\n",
    "        picked = random.sample(image_files, k)\n",
    "        sample_paths = [os.path.join(images_root, f) for f in picked]\n",
    "        preds = _generate_batch(sample_paths)\n",
    "        for pth, pred in zip(sample_paths, preds):\n",
    "            fname = os.path.basename(pth)\n",
    "            gt_refs = refs_map.get(fname, [])\n",
    "            print(\"\\nImage:\", fname)\n",
    "            if gt_refs:\n",
    "                print(\"Ground truths (up to 3):\")\n",
    "                for r in gt_refs[:3]:\n",
    "                    print(\" -\", r)\n",
    "            else:\n",
    "                print(\"(No references found)\")\n",
    "            print(\"Prediction:\")\n",
    "            print(\" -\", pred if pred else \"<EMPTY>\")\n",
    "            try:\n",
    "                img = Image.open(pth).convert(\"RGB\")\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(img); plt.axis(\"off\")\n",
    "                plt.title(fname)\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not display image: {e}\")\n",
    "\n",
    "# Usage:\n",
    "# evaluate_test_set(model_mm, test_dataset, images_root, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T07:35:27.562399Z",
     "iopub.status.busy": "2025-09-13T07:35:27.561531Z",
     "iopub.status.idle": "2025-09-13T07:40:47.803512Z",
     "shell.execute_reply": "2025-09-13T07:40:47.802852Z",
     "shell.execute_reply.started": "2025-09-13T07:35:27.562358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_test_set(model_mm, test_dataset, images_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:17:49.699083Z",
     "iopub.status.busy": "2025-09-12T14:17:49.698799Z",
     "iopub.status.idle": "2025-09-12T14:17:49.847297Z",
     "shell.execute_reply": "2025-09-12T14:17:49.846601Z",
     "shell.execute_reply.started": "2025-09-12T14:17:49.699062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Single Image Inference with Metrics (BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE)\n",
    "from typing import Optional, Dict, Any, List\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, torch\n",
    "from PIL import Image\n",
    "\n",
    "_PYCOCO_SCORERS = {}\n",
    "\n",
    "def _lazy_load_scorers():\n",
    "    global _PYCOCO_SCORERS\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.meteor.meteor import Meteor\n",
    "    from pycocoevalcap.rouge.rouge import Rouge\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    try:\n",
    "        from pycocoevalcap.spice.spice import Spice\n",
    "    except Exception:\n",
    "        Spice = None\n",
    "    if 'bleu' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['bleu'] = Bleu(4)\n",
    "    if 'meteor' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['meteor'] = Meteor()\n",
    "    if 'rouge' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['rouge'] = Rouge()\n",
    "    if 'cider' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['cider'] = Cider()\n",
    "    if Spice and 'spice' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['spice'] = Spice()\n",
    "    return _PYCOCO_SCORERS\n",
    "\n",
    "def _compute_single_caption_metrics(pred: str, refs: List[str]) -> Dict[str, float]:\n",
    "    refs_clean = [r.strip() for r in refs if r and r.strip()]\n",
    "    if not refs_clean:\n",
    "        return {}\n",
    "    scorers = _lazy_load_scorers()\n",
    "    gts = {0: refs_clean}\n",
    "    res = {0: [pred]}\n",
    "    out = {}\n",
    "    try:\n",
    "        bleu_scores, _ = scorers['bleu'].compute_score(gts, res)\n",
    "        out['bleu1'] = float(bleu_scores[0])\n",
    "        out['bleu2'] = float(bleu_scores[1])\n",
    "        out['bleu3'] = float(bleu_scores[2])\n",
    "        out['bleu4'] = float(bleu_scores[3])\n",
    "        out['bleu']  = float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "    try:\n",
    "        meteor_score, _ = scorers['meteor'].compute_score(gts, res)\n",
    "        out['meteor'] = float(meteor_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] METEOR failed:', e)\n",
    "    try:\n",
    "        rouge_score, _ = scorers['rouge'].compute_score(gts, res)\n",
    "        out['rougeL'] = float(rouge_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] ROUGE-L failed:', e)\n",
    "    try:\n",
    "        cider_score, _ = scorers['cider'].compute_score(gts, res)\n",
    "        out['cider'] = float(cider_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] CIDEr failed:', e)\n",
    "    if 'spice' in scorers:\n",
    "        try:\n",
    "            spice_score, spice_scores = scorers['spice'].compute_score(gts, res)\n",
    "            out['spice'] = float(spice_score)\n",
    "        except Exception as e:\n",
    "            print('[WARN] SPICE failed:', e)\n",
    "    return out\n",
    "\n",
    "def predict(\n",
    "    image_path: str,\n",
    "    prompt: str = \"Bu görüntüyü açıkla: \",\n",
    "    mode: str = \"beam\",              # 'beam' or 'sample'\n",
    "    max_refs: Optional[int] = 5,\n",
    "    show_image: bool = True,\n",
    "    show_refs: bool = True,\n",
    "    gen_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    json_file: Optional[str] = None,\n",
    "    ban_sentinels: bool = True,\n",
    "    compute_metrics: bool = True,\n",
    "    print_metrics: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n",
    "    jf = json_file or json_path\n",
    "    refs: List[str] = []\n",
    "    try:\n",
    "        with open(jf) as f:\n",
    "            data = json.load(f)\n",
    "        entries = data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "        target_name = os.path.basename(image_path)\n",
    "        for e in entries:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            if e.get('filename') == target_name:\n",
    "                for s in e.get('sentences', []):\n",
    "                    if isinstance(s, dict) and 'raw' in s:\n",
    "                        cap = s['raw'].strip()\n",
    "                        if cap:\n",
    "                            refs.append(cap)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse references ({e})\")\n",
    "    if max_refs is not None:\n",
    "        refs = refs[:max_refs]\n",
    "    gen_kwargs = (gen_kwargs or {}).copy()\n",
    "    if mode == \"sample\":\n",
    "        defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "    else:\n",
    "        defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3, num_beams=getattr(cfg, \"num_beams_infer\", 4))\n",
    "    for k, v in defaults.items():\n",
    "        gen_kwargs.setdefault(k, v)\n",
    "    local_amp_dtype = globals().get(\"amp_dtype\", None)\n",
    "    model_mm.eval()\n",
    "    if local_amp_dtype is not None:\n",
    "        with torch.amp.autocast('cuda', dtype=local_amp_dtype):\n",
    "            pred = model_mm.generate(\n",
    "                image_paths=[image_path],\n",
    "                prompt=prompt,\n",
    "                ban_sentinels=ban_sentinels,\n",
    "                **gen_kwargs\n",
    "            )[0]\n",
    "    else:\n",
    "        pred = model_mm.generate(\n",
    "            image_paths=[image_path],\n",
    "            prompt=prompt,\n",
    "            ban_sentinels=ban_sentinels,\n",
    "            **gen_kwargs\n",
    "        )[0]\n",
    "    metrics: Dict[str, float] = {}\n",
    "    if compute_metrics and refs:\n",
    "        try:\n",
    "            metrics = _compute_single_caption_metrics(pred if pred else \"<EMPTY>\", refs)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Metric computation failed: {e}\")\n",
    "    if show_image:\n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img); plt.axis(\"off\")\n",
    "            plt.title(\"Image\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not display image ({e})\")\n",
    "    if show_refs:\n",
    "        if refs:\n",
    "            print(\"References:\")\n",
    "            for i, r in enumerate(refs, 1):\n",
    "                print(f\"  {i}. {r}\")\n",
    "        else:\n",
    "            print(\"(No references found)\")\n",
    "    print(\"Prediction:\")\n",
    "    print(\" \", pred)\n",
    "    if print_metrics and metrics:\n",
    "        print(\"\\nMetrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"  {k}: {v}\")\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"prediction\": pred,\n",
    "        \"references\": refs,\n",
    "        \"metrics\": metrics,\n",
    "        \"mode\": mode,\n",
    "    }\n",
    "\n",
    "# Example usage (uncomment and set a valid path):\n",
    "# result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:17:56.424026Z",
     "iopub.status.busy": "2025-09-12T14:17:56.423733Z",
     "iopub.status.idle": "2025-09-12T14:18:17.982071Z",
     "shell.execute_reply": "2025-09-12T14:18:17.981246Z",
     "shell.execute_reply.started": "2025-09-12T14:17:56.424006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Example: Retrain from exported checkpoint on same or new dataset ===\n",
    "# checkpoint_path = '/kaggle/input/your_export/clip_mt5_prefix_latest.pt'\n",
    "# from torch.utils.data import DataLoader\n",
    "# device = torch.device('cuda')\n",
    "# model_re, cfg_re, hparams, epoch_loaded, step_loaded = load_model_for_retrain(\n",
    "#     checkpoint_path,\n",
    "#     device=device,\n",
    "#     new_lr=5e-5,              # optionally override LR\n",
    "#     new_clip_lr_scale=0.02,   # optionally override CLIP LR scale\n",
    "#     freeze_clip=None,         # or True to freeze CLIP now\n",
    "# )\n",
    "# optimizer_re = build_optimizer_from_hparams(model_re, hparams)\n",
    "# steps_per_epoch = len(train_loader)  # after you rebuild train_loader for (same or new) dataset\n",
    "# scheduler_re = build_scheduler_from_hparams(optimizer_re, hparams, steps_per_epoch, total_epochs=5)\n",
    "# # Proceed with standard training loop using model_re / optimizer_re / scheduler_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ==== Resume / Continue Training From Exported or Training Checkpoint (.pt) ====\n",
    "# # Improvements:\n",
    "# #  - Ensures 'checkpoints' directory exists before saving.\n",
    "# #  - Handles tokenizer class mismatch notice by using AutoTokenizer in the pipeline.\n",
    "# #  - Initializes best_val robustly (evaluates validation if inf).\n",
    "# #  - Safe scheduler positioning.\n",
    "# #  - Adds optional immediate validation before continuing to set baseline.\n",
    "\n",
    "# RESUME_PATH = '/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt'  # change as needed\n",
    "# EXTRA_EPOCHS = 5          # number of extra epochs to train\n",
    "# RUN_INITIAL_VAL = True    # run a validation pass right after load to set best_val if missing\n",
    "# SAVE_NAME = 'best_resumed.pt'\n",
    "\n",
    "# import os, math, time, torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# if not os.path.isfile(RESUME_PATH):\n",
    "#     print(f\"[RESUME] Path not found: {RESUME_PATH} -> Skip.\")\n",
    "# else:\n",
    "#     print(f\"[RESUME] Loading: {RESUME_PATH}\")\n",
    "#     bundle = torch.load(RESUME_PATH, map_location='cpu')\n",
    "\n",
    "#     has_export_format = 'model_state' in bundle\n",
    "#     has_train_ckpt_format = 'model' in bundle\n",
    "#     if not (has_export_format or has_train_ckpt_format):\n",
    "#         raise ValueError('Unrecognized checkpoint format.')\n",
    "\n",
    "#     cfg_dict = bundle.get('cfg') or {}\n",
    "#     class _Cfg: ...\n",
    "#     cfg_resume = _Cfg()\n",
    "#     for k, v in base_config.items():\n",
    "#         setattr(cfg_resume, k, cfg_dict.get(k, v))\n",
    "\n",
    "#     # Create pipeline (AutoTokenizer inside ensures consistency with checkpoint)\n",
    "#     model_resume = CLIPmT5Pipeline(cfg_resume).to(device)\n",
    "\n",
    "#     if has_export_format:\n",
    "#         model_resume.load_state_dict(bundle['model_state'], strict=True)\n",
    "#         loaded_epoch = bundle.get('epoch', -1)\n",
    "#         global_step_loaded = bundle.get('global_step', None)\n",
    "#         best_val_loaded = bundle.get('best_val', float('inf'))\n",
    "#     else:\n",
    "#         model_resume.load_state_dict(bundle['model'], strict=True)\n",
    "#         loaded_epoch = bundle.get('epoch', -1)\n",
    "#         global_step_loaded = bundle.get('global_step', None)\n",
    "#         best_val_loaded = bundle.get('best_val', float('inf'))\n",
    "\n",
    "#     print(f\"[RESUME] Weights loaded (epoch={loaded_epoch}, best_val={best_val_loaded})\")\n",
    "\n",
    "#     main_params, clip_params = [], []\n",
    "#     for n, p in model_resume.named_parameters():\n",
    "#         if not p.requires_grad: continue\n",
    "#         (clip_params if n.startswith('clip.') else main_params).append(p)\n",
    "#     param_groups = []\n",
    "#     if main_params: param_groups.append({'params': main_params, 'lr': cfg_resume.lr})\n",
    "#     if clip_params: param_groups.append({'params': clip_params, 'lr': cfg_resume.lr * getattr(cfg_resume, 'clip_lr_scale', 0.05)})\n",
    "#     optimizer_resume = AdamW(param_groups, weight_decay=cfg_resume.weight_decay)\n",
    "\n",
    "#     steps_per_epoch = len(train_loader)\n",
    "#     total_planned_epochs = loaded_epoch + 1 + EXTRA_EPOCHS\n",
    "#     total_steps = steps_per_epoch * total_planned_epochs\n",
    "\n",
    "#     def lr_lambda(step):\n",
    "#         if cfg_resume.warmup_steps > 0 and step < cfg_resume.warmup_steps:\n",
    "#             return float(step) / float(max(1, cfg_resume.warmup_steps))\n",
    "#         progress = (step - cfg_resume.warmup_steps) / float(max(1, total_steps - cfg_resume.warmup_steps))\n",
    "#         progress = min(max(progress, 0.0), 1.0)\n",
    "#         return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "#     scheduler_resume = LambdaLR(optimizer_resume, lr_lambda=lr_lambda)\n",
    "\n",
    "#     opt_key = 'optimizer_state' if has_export_format else 'optimizer'\n",
    "#     sch_key = 'scheduler_state' if has_export_format else 'scheduler'\n",
    "#     if opt_key in bundle:\n",
    "#         try:\n",
    "#             optimizer_resume.load_state_dict(bundle[opt_key])\n",
    "#             print('[RESUME] Optimizer state restored.')\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] Optimizer state load failed:', e)\n",
    "#     if sch_key in bundle:\n",
    "#         try:\n",
    "#             scheduler_resume.load_state_dict(bundle[sch_key])\n",
    "#             print('[RESUME] Scheduler state restored.')\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] Scheduler state load failed:', e)\n",
    "\n",
    "#     if scheduler_resume.last_epoch < 0 and loaded_epoch >= 0:\n",
    "#         completed_steps = (loaded_epoch + 1) * steps_per_epoch\n",
    "#         scheduler_resume.last_epoch = completed_steps\n",
    "#         print(f\"[RESUME] Scheduler last_epoch set to {scheduler_resume.last_epoch}\")\n",
    "\n",
    "#     amp_dtype_resume = None\n",
    "#     if getattr(cfg_resume, 'use_amp', True):\n",
    "#         if getattr(cfg_resume, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "#             amp_dtype_resume = torch.bfloat16\n",
    "#         else:\n",
    "#             amp_dtype_resume = torch.float16\n",
    "#     scaler_resume = torch.amp.GradScaler('cuda', enabled=amp_dtype_resume is not None)\n",
    "\n",
    "#     start_epoch = loaded_epoch + 1\n",
    "#     end_epoch = loaded_epoch + EXTRA_EPOCHS\n",
    "\n",
    "#     best_val = best_val_loaded\n",
    "\n",
    "#     # Optional immediate validation to set best_val if it's inf or user wants baseline\n",
    "#     if RUN_INITIAL_VAL and (best_val == float('inf') or best_val != best_val):  # inf or NaN\n",
    "#         if val_loader:\n",
    "#             model_resume.eval(); v=0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in val_loader:\n",
    "#                     imgs, srcs, tgts = batch\n",
    "#                     imgs = imgs.to(device, non_blocking=True)\n",
    "#                     if amp_dtype_resume is not None:\n",
    "#                         with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                             out = model_resume(imgs, srcs, tgts)\n",
    "#                     else:\n",
    "#                         out = model_resume(imgs, srcs, tgts)\n",
    "#                     v += out.loss.item()\n",
    "#             best_val = v / max(1, len(val_loader))\n",
    "#             print(f\"[RESUME] Initial validation baseline best_val set to {best_val:.4f}\")\n",
    "#         else:\n",
    "#             best_val = float('inf')\n",
    "\n",
    "#     early_patience = getattr(cfg_resume, 'early_stop_patience', None)\n",
    "#     min_delta = getattr(cfg_resume, 'early_stop_min_delta', 0.0)\n",
    "#     _epochs_no_improve = 0\n",
    "\n",
    "#     global_step_resume = global_step_loaded if global_step_loaded is not None else (start_epoch * steps_per_epoch)\n",
    "\n",
    "#     print(f\"[RESUME] Continue training for {EXTRA_EPOCHS} more epochs: {start_epoch} -> {end_epoch}\")\n",
    "\n",
    "#     for epoch in range(start_epoch, end_epoch + 1):\n",
    "#         model_resume.train()\n",
    "#         sum_loss = 0.0\n",
    "#         t0 = time.time()\n",
    "#         for step, batch in enumerate(train_loader, start=1):\n",
    "#             imgs, srcs, tgts = batch\n",
    "#             imgs = imgs.to(device, non_blocking=True)\n",
    "#             optimizer_resume.zero_grad(set_to_none=True)\n",
    "#             if amp_dtype_resume is not None:\n",
    "#                 with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                     out = model_resume(imgs, srcs, tgts)\n",
    "#                     loss = out.loss\n",
    "#                 scaler_resume.scale(loss).backward()\n",
    "#                 if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n",
    "#                     scaler_resume.unscale_(optimizer_resume)\n",
    "#                     torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n",
    "#                 scaler_resume.step(optimizer_resume)\n",
    "#                 scaler_resume.update()\n",
    "#             else:\n",
    "#                 out = model_resume(imgs, srcs, tgts); loss = out.loss\n",
    "#                 loss.backward()\n",
    "#                 if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n",
    "#                     torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n",
    "#                 optimizer_resume.step()\n",
    "#             scheduler_resume.step()\n",
    "#             sum_loss += loss.item()\n",
    "#             global_step_resume += 1\n",
    "#         train_loss = sum_loss / max(1, len(train_loader))\n",
    "\n",
    "#         val_loss = None\n",
    "#         if val_loader:\n",
    "#             model_resume.eval(); v = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in val_loader:\n",
    "#                     imgs, srcs, tgts = batch\n",
    "#                     imgs = imgs.to(device, non_blocking=True)\n",
    "#                     if amp_dtype_resume is not None:\n",
    "#                         with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                             out = model_resume(imgs, srcs, tgts)\n",
    "#                     else:\n",
    "#                         out = model_resume(imgs, srcs, tgts)\n",
    "#                     v += out.loss.item()\n",
    "#             val_loss = v / max(1, len(val_loader))\n",
    "\n",
    "#         dt = time.time() - t0\n",
    "#         lr_cur = scheduler_resume.get_last_lr()[0]\n",
    "#         if val_loss is not None:\n",
    "#             print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} val={val_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n",
    "#         else:\n",
    "#             print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n",
    "\n",
    "#         metric = val_loss if val_loss is not None else train_loss\n",
    "#         improved = metric < (best_val - min_delta)\n",
    "#         if improved:\n",
    "#             best_val = metric\n",
    "#             _epochs_no_improve = 0\n",
    "#             save_obj = {\n",
    "#                 'model': model_resume.state_dict(),\n",
    "#                 'cfg': {k: getattr(cfg_resume, k) for k in base_config.keys()},\n",
    "#                 'epoch': epoch,\n",
    "#                 'optimizer': optimizer_resume.state_dict(),\n",
    "#                 'scheduler': scheduler_resume.state_dict(),\n",
    "#                 'best_val': best_val,\n",
    "#                 'global_step': global_step_resume,\n",
    "#             }\n",
    "#             torch.save(save_obj, os.path.join('checkpoints', SAVE_NAME))\n",
    "#             print(f\"   -> [RESUME] Saved {SAVE_NAME} (metric={best_val:.4f})\")\n",
    "#         else:\n",
    "#             _epochs_no_improve += 1\n",
    "#             if early_patience is not None and _epochs_no_improve >= early_patience:\n",
    "#                 print(f\"[Early Stop - Resume] No improvement for {early_patience} epochs.\")\n",
    "#                 break\n",
    "\n",
    "#     print('[RESUME] Training extension finished. Final best_val=', best_val)\n",
    "#     model_mm = model_resume"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8247064,
     "sourceId": 13025325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8247076,
     "sourceId": 13025342,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
