{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP (ViT-B/32) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze, edit: unfreeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T12:33:53.270456Z",
     "iopub.status.busy": "2025-09-10T12:33:53.270235Z",
     "iopub.status.idle": "2025-09-10T12:36:33.928447Z",
     "shell.execute_reply": "2025-09-10T12:36:33.927578Z",
     "shell.execute_reply.started": "2025-09-10T12:33:53.270437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bigframes 2.8.0\n",
      "Uninstalling bigframes-2.8.0:\n",
      "  Successfully uninstalled bigframes-2.8.0\n",
      "Found existing installation: cesium 0.12.4\n",
      "Uninstalling cesium-0.12.4:\n",
      "  Successfully uninstalled cesium-0.12.4\n",
      "Found existing installation: gcsfs 2025.3.2\n",
      "Uninstalling gcsfs-2025.3.2:\n",
      "  Successfully uninstalled gcsfs-2025.3.2\n",
      "Collecting git+https://github.com/openai/CLIP.git (from -r /kaggle/input/requirements14/requirements.txt (line 20))\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7m7p3ese\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7m7p3ese\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pycocoevalcap@ git+https://github.com/salaniz/pycocoevalcap.git (from -r /kaggle/input/requirements14/requirements.txt (line 23))\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-install-k3g7gl9r/pycocoevalcap_a53ce598acc340b8a3bd63ba9c7641fd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap.git /tmp/pip-install-k3g7gl9r/pycocoevalcap_a53ce598acc340b8a3bd63ba9c7641fd\n",
      "  Resolved https://github.com/salaniz/pycocoevalcap.git to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 2)) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 3)) (0.21.0+cu124)\n",
      "Requirement already satisfied: accelerate>=0.29.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: transformers==4.52.4 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 7)) (4.52.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 8)) (0.33.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: pillow>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 12)) (11.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 13)) (3.7.2)\n",
      "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 16)) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 17)) (1.26.4)\n",
      "Collecting evaluate>=0.4.3 (from -r /kaggle/input/requirements14/requirements.txt (line 26))\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting bert-score>=0.3.13 (from -r /kaggle/input/requirements14/requirements.txt (line 27))\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements14/requirements.txt (line 30)) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (0.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0->-r /kaggle/input/requirements14/requirements.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.32.0->-r /kaggle/input/requirements14/requirements.txt (line 8)) (1.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->-r /kaggle/input/requirements14/requirements.txt (line 16)) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->-r /kaggle/input/requirements14/requirements.txt (line 16)) (1.5.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2.4.1)\n",
      "Collecting ftfy (from clip==1.0->-r /kaggle/input/requirements14/requirements.txt (line 20))\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap@ git+https://github.com/salaniz/pycocoevalcap.git->-r /kaggle/input/requirements14/requirements.txt (line 23)) (2.0.10)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (3.6.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (19.0.1)\n",
      "Collecting fsspec (from torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2))\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (3.12.13)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r /kaggle/input/requirements14/requirements.txt (line 13)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements14/requirements.txt (line 7)) (2025.6.15)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0->-r /kaggle/input/requirements14/requirements.txt (line 20)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->-r /kaggle/input/requirements14/requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2024.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate>=0.4.3->-r /kaggle/input/requirements14/requirements.txt (line 26)) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->-r /kaggle/input/requirements14/requirements.txt (line 17)) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: clip, pycocoevalcap\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=7f7b4e5ca7f3fe78fe2cdb14acb0c790264f0b8b1e2d8061bdb608353ad2638d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ha5v4jki/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=286dd7814f7f0fbce9ae3d0e6201115d28f15b805e37ebe177e0ee6e74740870\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ha5v4jki/wheels/e5/d1/50/82763a91172a5c8058c9efff8692f3a41570e3ddd5b5b2c4b4\n",
      "Successfully built clip pycocoevalcap\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pycocoevalcap, evaluate, clip, bert-score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bert-score-0.3.13 clip-1.0 evaluate-0.4.5 fsspec-2025.3.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pycocoevalcap-1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "2025-09-10 12:35:50.857584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757507751.047706     103 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757507751.102844     103 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cbba79b7d6423f98028eac9d4af1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd3552fd3324d49be29ead5ec380227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddb14d46b654aeeb5ad1a775b2b5f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0147b52028e04daeb60b7fb10027bf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb62ff65b2141559216bbbf2d5e3bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d76fd52ae148f9b9505b37a71bdd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733d0d8e07ad463b91c3e54d4c25ecb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                               | 0.00/335M [00:00<?, ?iB/s]\u001b[A\n",
      "  0%|                                       | 976k/335M [00:00<00:36, 9.54MiB/s]\u001b[A\n",
      "  1%|▏                                     | 1.91M/335M [00:00<00:36, 9.47MiB/s]\u001b[A\n",
      "  3%|▉                                     | 8.41M/335M [00:00<00:09, 35.4MiB/s]\u001b[A\n",
      "  5%|█▋                                    | 15.4M/335M [00:00<00:06, 49.9MiB/s]\u001b[A\n",
      "  6%|██▎                                   | 20.2M/335M [00:00<00:09, 36.5MiB/s]\u001b[A\n",
      "  7%|██▋                                   | 24.1M/335M [00:00<00:09, 34.8MiB/s]\u001b[A\n",
      "  8%|███▏                                  | 27.7M/335M [00:00<00:11, 27.1MiB/s]\u001b[A\n",
      " 10%|███▊                                  | 33.1M/335M [00:01<00:09, 33.5MiB/s]\u001b[A\n",
      " 11%|████▏                                 | 36.8M/335M [00:01<00:09, 33.1MiB/s]\u001b[A\n",
      " 12%|████▌                                 | 40.6M/335M [00:01<00:08, 34.8MiB/s]\u001b[A\n",
      " 13%|█████                                 | 44.2M/335M [00:01<00:09, 31.0MiB/s]\u001b[A\n",
      " 14%|█████▍                                | 48.0M/335M [00:01<00:09, 32.9MiB/s]\u001b[A\n",
      " 15%|█████▊                                | 51.4M/335M [00:01<00:09, 32.5MiB/s]\u001b[A\n",
      " 17%|██████▎                               | 56.0M/335M [00:01<00:07, 36.7MiB/s]\u001b[A\n",
      " 18%|██████▊                               | 59.7M/335M [00:02<00:13, 21.3MiB/s]\u001b[A\n",
      " 19%|███████▎                              | 64.8M/335M [00:02<00:10, 27.1MiB/s]\u001b[A\n",
      " 20%|███████▊                              | 68.3M/335M [00:02<00:09, 28.8MiB/s]\u001b[A\n",
      " 22%|████████▎                             | 72.9M/335M [00:02<00:08, 33.3MiB/s]\u001b[A\n",
      " 23%|████████▋                             | 76.8M/335M [00:02<00:08, 31.3MiB/s]\u001b[A\n",
      " 24%|█████████▏                            | 80.6M/335M [00:02<00:08, 33.2MiB/s]\u001b[A\n",
      " 25%|█████████▌                            | 84.1M/335M [00:02<00:08, 30.8MiB/s]\u001b[A\n",
      " 26%|██████████                            | 88.1M/335M [00:02<00:07, 33.2MiB/s]\u001b[A\n",
      " 27%|██████████▍                           | 91.5M/335M [00:03<00:08, 30.4MiB/s]\u001b[A\n",
      " 29%|██████████▉                           | 96.5M/335M [00:03<00:06, 35.9MiB/s]\u001b[A\n",
      " 30%|███████████▋                           | 101M/335M [00:03<00:06, 37.3MiB/s]\u001b[A\n",
      " 31%|████████████▎                          | 105M/335M [00:03<00:06, 39.8MiB/s]\u001b[A\n",
      " 33%|████████████▉                          | 111M/335M [00:03<00:05, 43.9MiB/s]\u001b[A\n",
      " 34%|█████████████▍                         | 115M/335M [00:03<00:05, 39.9MiB/s]\u001b[A\n",
      " 36%|██████████████                         | 121M/335M [00:03<00:04, 45.0MiB/s]\u001b[A\n",
      " 37%|██████████████▌                        | 125M/335M [00:03<00:05, 40.4MiB/s]\u001b[A\n",
      " 39%|███████████████                        | 129M/335M [00:03<00:05, 40.3MiB/s]\u001b[A\n",
      " 40%|███████████████▌                       | 133M/335M [00:04<00:05, 35.2MiB/s]\u001b[A\n",
      " 41%|████████████████                       | 138M/335M [00:04<00:05, 38.8MiB/s]\u001b[A\n",
      " 43%|████████████████▉                      | 145M/335M [00:04<00:04, 47.8MiB/s]\u001b[A\n",
      " 45%|█████████████████▍                     | 150M/335M [00:04<00:04, 45.8MiB/s]\u001b[A\n",
      " 46%|██████████████████                     | 154M/335M [00:04<00:05, 37.6MiB/s]\u001b[A\n",
      " 48%|██████████████████▋                    | 160M/335M [00:04<00:04, 43.0MiB/s]\u001b[A\n",
      " 49%|███████████████████▏                   | 165M/335M [00:04<00:04, 38.0MiB/s]\u001b[A\n",
      " 51%|███████████████████▋                   | 169M/335M [00:05<00:04, 40.5MiB/s]\u001b[A\n",
      " 52%|████████████████████▏                  | 174M/335M [00:05<00:04, 40.5MiB/s]\u001b[A\n",
      " 53%|████████████████████▋                  | 178M/335M [00:05<00:04, 36.2MiB/s]\u001b[A\n",
      " 54%|█████████████████████▏                 | 182M/335M [00:05<00:04, 38.5MiB/s]\u001b[A\n",
      " 56%|█████████████████████▋                 | 186M/335M [00:05<00:04, 34.8MiB/s]\u001b[A\n",
      " 57%|██████████████████████▏                | 190M/335M [00:05<00:04, 37.7MiB/s]\u001b[A\n",
      " 58%|██████████████████████▌                | 194M/335M [00:05<00:04, 33.9MiB/s]\u001b[A\n",
      " 59%|███████████████████████                | 197M/335M [00:05<00:04, 32.9MiB/s]\u001b[A\n",
      " 60%|███████████████████████▌               | 202M/335M [00:05<00:03, 36.4MiB/s]\u001b[A\n",
      " 61%|███████████████████████▉               | 205M/335M [00:06<00:03, 35.5MiB/s]\u001b[A\n",
      " 63%|████████████████████████▍              | 209M/335M [00:06<00:03, 35.8MiB/s]\u001b[A\n",
      " 64%|████████████████████████▊              | 213M/335M [00:06<00:04, 30.5MiB/s]\u001b[A\n",
      " 65%|█████████████████████████▍             | 218M/335M [00:06<00:03, 36.2MiB/s]\u001b[A\n",
      " 67%|██████████████████████████             | 223M/335M [00:06<00:02, 41.7MiB/s]\u001b[A\n",
      " 68%|██████████████████████████▌            | 227M/335M [00:06<00:03, 29.6MiB/s]\u001b[A\n",
      " 69%|███████████████████████████            | 232M/335M [00:06<00:03, 32.0MiB/s]\u001b[A\n",
      " 70%|███████████████████████████▍           | 236M/335M [00:07<00:08, 12.0MiB/s]\u001b[A\n",
      " 72%|████████████████████████████           | 241M/335M [00:08<00:07, 13.4MiB/s]\u001b[A\n",
      " 74%|████████████████████████████▋          | 246M/335M [00:08<00:04, 18.5MiB/s]\u001b[A\n",
      " 75%|█████████████████████████████▏         | 250M/335M [00:08<00:04, 21.3MiB/s]\u001b[A\n",
      " 76%|█████████████████████████████▋         | 254M/335M [00:08<00:03, 25.0MiB/s]\u001b[A\n",
      " 77%|██████████████████████████████         | 258M/335M [00:08<00:03, 24.5MiB/s]\u001b[A\n",
      " 79%|██████████████████████████████▉        | 265M/335M [00:08<00:02, 34.1MiB/s]\u001b[A\n",
      " 81%|███████████████████████████████▋       | 271M/335M [00:08<00:01, 41.0MiB/s]\u001b[A\n",
      " 83%|████████████████████████████████▏      | 276M/335M [00:08<00:01, 37.2MiB/s]\u001b[A\n",
      " 84%|████████████████████████████████▋      | 281M/335M [00:09<00:01, 37.2MiB/s]\u001b[A\n",
      " 85%|█████████████████████████████████▏     | 285M/335M [00:09<00:01, 34.2MiB/s]\u001b[A\n",
      " 87%|█████████████████████████████████▋     | 289M/335M [00:09<00:01, 37.6MiB/s]\u001b[A\n",
      " 88%|██████████████████████████████████▎    | 294M/335M [00:09<00:01, 39.8MiB/s]\u001b[A\n",
      " 89%|██████████████████████████████████▋    | 298M/335M [00:09<00:01, 37.4MiB/s]\u001b[A\n",
      " 90%|███████████████████████████████████▏   | 302M/335M [00:09<00:01, 32.8MiB/s]\u001b[A\n",
      " 91%|███████████████████████████████████▌   | 305M/335M [00:09<00:00, 31.0MiB/s]\u001b[A\n",
      " 92%|███████████████████████████████████▉   | 308M/335M [00:10<00:00, 30.5MiB/s]\u001b[A\n",
      " 93%|████████████████████████████████████▍  | 312M/335M [00:10<00:00, 33.1MiB/s]\u001b[A\n",
      " 94%|████████████████████████████████████▊  | 316M/335M [00:10<00:00, 27.5MiB/s]\u001b[A\n",
      " 96%|█████████████████████████████████████▍ | 321M/335M [00:10<00:00, 35.1MiB/s]\u001b[A\n",
      " 98%|██████████████████████████████████████▏| 328M/335M [00:10<00:00, 43.4MiB/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 335M/335M [00:10<00:00, 32.8MiB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Done!\n"
     ]
    }
   ],
   "source": [
    "# Rerun this cell at each session start\n",
    "\n",
    "# Uninstall conflicting packages (Kaggle specific)\n",
    "!pip uninstall -y bigframes cesium gcsfs\n",
    "\n",
    "# Performance metrics\n",
    "!pip install -r /kaggle/input/requirements14/requirements.txt\n",
    "\n",
    "# To use nltk\n",
    "import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\n",
    "\n",
    "# Download mT5-small ViT-B/16\n",
    "import clip, torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "mt5 = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "print(\"Setup Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T12:36:33.930837Z",
     "iopub.status.busy": "2025-09-10T12:36:33.930252Z",
     "iopub.status.idle": "2025-09-10T12:36:33.984297Z",
     "shell.execute_reply": "2025-09-10T12:36:33.983522Z",
     "shell.execute_reply.started": "2025-09-10T12:36:33.930810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Local config object created.\n",
      "[INFO] Run the next 'W&B Init' cell to enable Weights & Biases tracking.\n",
      "[DEVICE] Using CUDA device: Tesla P100-PCIE-16GB\n",
      "[DEVICE] TF32 enabled.\n",
      "[DEVICE] bfloat16 support: True\n",
      "Active config (local):\n",
      "  model: google/mt5-small\n",
      "  clip_encoder: ViT-B/16\n",
      "  prefix_tokens: 16\n",
      "  batch_size: 16\n",
      "  grad_accum_steps: 2\n",
      "  enable_t5_gradient_checkpointing: True\n",
      "  lr: 0.0001\n",
      "  epochs: 25\n",
      "  dataset_limit: None\n",
      "  freeze_clip: False\n",
      "  unfreeze_clip_last_n: 0\n",
      "  clip_lr_scale: 0.05\n",
      "  use_clip_patch_tokens: True\n",
      "  freeze_t5_encoder: False\n",
      "  freeze_t5_decoder: False\n",
      "  seed: 42\n",
      "  weight_decay: 0.01\n",
      "  grad_clip: 1.0\n",
      "  warmup_steps: 500\n",
      "  num_beams_infer: 4\n",
      "  max_new_tokens_infer: 32\n",
      "  src_max_len: 64\n",
      "  tgt_max_len: 64\n",
      "  bleu_patience: 5\n",
      "  bleu_min_delta: 0.0\n",
      "  use_amp: True\n",
      "  use_bf16: True\n",
      "  enable_tf32: True\n",
      "  finite_loss_skip: True\n",
      "  save_every: 0\n"
     ]
    }
   ],
   "source": [
    "# Unified configuration (wandb init moved to next cell)\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"bites-tr-image-captioning\"\n",
    "RUN_NAME = \"clip_mt5_prefix_run\"\n",
    "\n",
    "ENABLE_WANDB = True  # Set False to skip W&B entirely\n",
    "\n",
    "WANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/16\",  # backbone\n",
    "    \"prefix_tokens\": 16,          # stronger conditioning\n",
    "    \"batch_size\": 16,              # reduced to mitigate OOM\n",
    "    \"grad_accum_steps\": 2,        # accumulate to simulate larger effective batch\n",
    "    \"enable_t5_gradient_checkpointing\": True,  # reduce memory\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 25,                  # allow a bit longer now that we fine-tune CLIP\n",
    "    \"dataset_limit\": None,\n",
    "    # --- CLIP fine-tuning controls ---\n",
    "    \"freeze_clip\": False,          # set False to allow full CLIP fine-tuning\n",
    "    \"unfreeze_clip_last_n\": 0,     # if >0 unfreezes only last N vision blocks\n",
    "    \"clip_lr_scale\": 0.05,         # scaled LR for ALL CLIP params (lower than main)\n",
    "    \"use_clip_patch_tokens\": True, # richer conditioning (patch tokens path) (set False to save memory)\n",
    "    # --- T5 freezing ---\n",
    "    \"freeze_t5_encoder\": False,    # unfreeze encoder so it can adapt\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    # --- Optimization ---\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 500,           # linear warmup steps before cosine decay\n",
    "    # --- Inference defaults ---\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    # --- Early stopping (BLEU-based) ---\n",
    "    \"bleu_patience\": 5,\n",
    "    \"bleu_min_delta\": 0.0,\n",
    "    \"use_amp\": True,\n",
    "    # Optional extras:\n",
    "    \"use_bf16\": True,\n",
    "    \"enable_tf32\": True,\n",
    "    \"finite_loss_skip\": True,\n",
    "    \"save_every\": 0\n",
    "}\n",
    "\n",
    "# Global flags/handles populated after (optional) wandb init cell\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "\n",
    "# Always create a local cfg object here; W&B init (next cell) can sync/override\n",
    "class _Cfg: ...\n",
    "cfg = _Cfg()\n",
    "for k, v in base_config.items():\n",
    "    setattr(cfg, k, v)\n",
    "print(\"[INFO] Local config object created.\")\n",
    "if ENABLE_WANDB:\n",
    "    print(\"[INFO] Run the next 'W&B Init' cell to enable Weights & Biases tracking.\")\n",
    "else:\n",
    "    print(\"[INFO] W&B disabled (ENABLE_WANDB=False). Using local config only.\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# --- Enforce CUDA-only environment ---\n",
    "assert torch.cuda.is_available(), \"CUDA GPU is required but not detected. Please run in a CUDA-enabled environment.\"\n",
    "device = torch.device('cuda')\n",
    "print(f\"[DEVICE] Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Performance toggles\n",
    "if getattr(cfg, 'enable_tf32', False):\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"[DEVICE] TF32 enabled.\")\n",
    "    except Exception as _e:\n",
    "        print(\"[WARN] Could not enable TF32:\", _e)\n",
    "\n",
    "if getattr(cfg, 'use_bf16', False):\n",
    "    bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    print(f\"[DEVICE] bfloat16 support: {bf16_ok}\")\n",
    "\n",
    "print(\"Active config (local):\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T12:36:33.986208Z",
     "iopub.status.busy": "2025-09-10T12:36:33.985426Z",
     "iopub.status.idle": "2025-09-10T12:36:36.499085Z",
     "shell.execute_reply": "2025-09-10T12:36:36.498040Z",
     "shell.execute_reply.started": "2025-09-10T12:36:33.986173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# (CUDA enforcement handled in config cell; assume cuda device later)\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = MT5Tokenizer.from_pretrained(cfg.model)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        if getattr(cfg, 'enable_t5_gradient_checkpointing', False):\n",
    "            try:\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                print('[MEM] Enabled T5 gradient checkpointing.')\n",
    "            except Exception as e:\n",
    "                print('[WARN] Could not enable gradient checkpointing:', e)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        # CLIP freezing / unfreezing strategy\n",
    "        if cfg.unfreeze_clip_last_n and cfg.unfreeze_clip_last_n > 0:\n",
    "            # Freeze everything first\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "            blocks = list(self.clip.visual.transformer.resblocks)\n",
    "            for block in blocks[-cfg.unfreeze_clip_last_n:]:\n",
    "                for p in block.parameters():\n",
    "                    p.requires_grad = True\n",
    "        else:\n",
    "            # Respect freeze_clip flag (False means fully trainable)\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = not cfg.freeze_clip\n",
    "        # T5 freeze toggles\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine embedding dim dynamically (CLIP projection output)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    def _encode_image_single(self, images: torch.Tensor):\n",
    "        pooled = self.clip.encode_image(images)\n",
    "        return pooled\n",
    "\n",
    "    def _encode_image_patch_tokens(self, images: torch.Tensor):\n",
    "        visual = self.clip.visual\n",
    "        x = visual.conv1(images)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        x = visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        for block in visual.transformer.resblocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        patches = x[:, 1:, :]\n",
    "        pooled = patches.mean(dim=1)\n",
    "        if hasattr(visual, 'ln_post'):\n",
    "            pooled = visual.ln_post(pooled)\n",
    "        if hasattr(visual, 'proj') and visual.proj is not None:\n",
    "            pooled = pooled @ visual.proj\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        images = images.to(device)\n",
    "        clip_emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        model_out = self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "        try:\n",
    "            setattr(model_out, 'labels', tok_tgt.input_ids)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return model_out\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu görüntüyü açıkla: \", ban_sentinels=True, **gen_kwargs):\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        images = images.to(next(self.parameters()).device)\n",
    "        emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        return self.proj(emb)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T12:36:36.501056Z",
     "iopub.status.busy": "2025-09-10T12:36:36.500249Z",
     "iopub.status.idle": "2025-09-10T12:36:54.120300Z",
     "shell.execute_reply": "2025-09-10T12:36:54.119535Z",
     "shell.execute_reply.started": "2025-09-10T12:36:36.501012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulkadirparlak\u001b[0m (\u001b[33mabdulkadirparlak-hacettepe-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250910_123647-5dtuyqr2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/5dtuyqr2' target=\"_blank\">clip_mt5_prefix_run</a></strong> to <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/5dtuyqr2' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/5dtuyqr2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wandb] run initialized and config logged.\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Weights & Biases initialization\n",
    "# Run this AFTER the config cell if you want online logging,\n",
    "# otherwise skip to keep everything offline.\n",
    "if 'ENABLE_WANDB' in globals() and ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config, reinit=True)\n",
    "        cfg = wandb.config  # sync cfg to wandb\n",
    "        # Explicitly log config dict to the run (config, summary, and a one-time log)\n",
    "        try:\n",
    "            cfg_dict = dict(base_config)\n",
    "        except Exception:\n",
    "            cfg_dict = {k: getattr(cfg, k) for k in base_config.keys() if hasattr(cfg, k)}\n",
    "        wandb.config.update(cfg_dict, allow_val_change=True)\n",
    "        # Store a namespaced copy in summary for quick viewing\n",
    "        wandb.summary.update({f\"cfg/{k}\": v for k, v in cfg_dict.items()})\n",
    "        # Also log once at step 0 for time-series traceability\n",
    "        wandb.log({\"cfg\": cfg_dict}, step=0)\n",
    "        use_wandb = True\n",
    "        print('[wandb] run initialized and config logged.')\n",
    "    except Exception as e:\n",
    "        use_wandb = False\n",
    "        print('[wandb] disabled (init failed):', e)\n",
    "else:\n",
    "    print('[wandb] Skipped (ENABLE_WANDB is False).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T12:36:54.121639Z",
     "iopub.status.busy": "2025-09-10T12:36:54.121361Z",
     "iopub.status.idle": "2025-09-10T12:37:18.224580Z",
     "shell.execute_reply": "2025-09-10T12:37:18.223673Z",
     "shell.execute_reply.started": "2025-09-10T12:36:54.121609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEM] Enabled T5 gradient checkpointing.\n",
      "Train samples: 12028  Val: 1000  Test: 2003\n",
      "Clip ViT-B/16 params: 149620737\n",
      "Projection params: 8923136\n",
      "mt5-small params: 300176768\n",
      "Total trainable params: 458720641\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "json_path = '/kaggle/input/tasviret/flickr8k/tasviret8k_captions.json'\n",
    "images_root = '/kaggle/input/tasviret/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=1000)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Clip ViT-B/16 params:\", sum(p.numel() for p in model_mm.clip.parameters() if p.requires_grad))\n",
    "print(\"Projection params:\", sum(p.numel() for p in model_mm.proj.parameters() if p.requires_grad))\n",
    "print(\"mt5-small params:\", sum(p.numel() for p in model_mm.model.parameters() if p.requires_grad))\n",
    "print(\"Total trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-10T14:55:36.778Z",
     "iopub.execute_input": "2025-09-10T12:37:18.225798Z",
     "iopub.status.busy": "2025-09-10T12:37:18.225532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AMP] Using bfloat16 mixed precision\n",
      "[INFO] CLIP fine-tune params: 302 with lr=5.00e-06\n",
      "[TRAIN] Starting training with BLEU-1-based checkpointing + delayed patience (after 10 epochs). GradAccum=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 12646, 'reflen': 4778, 'guess': [12646, 12147, 11654, 11168], 'correct': [535, 1, 0, 0]}\n",
      "ratio: 2.646714106320082\n",
      "Epoch 01/25 train_loss=28.7044 val_loss=8.8328 BLEU1=0.042 BLEU4=0.000 time=840.1s lr=7.52e-05\n",
      "  -> Saved best_bleu.pt (BLEU1=0.0423)\n",
      "{'testlen': 998, 'reflen': 3293, 'guess': [998, 499, 0, 0], 'correct': [0, 0, 0, 0]}\n",
      "ratio: 0.30306711205578407\n",
      "Epoch 02/25 train_loss=6.0496 val_loss=4.4921 BLEU1=0.000 BLEU4=0.000 time=774.5s lr=9.98e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "{'testlen': 1894, 'reflen': 3296, 'guess': [1894, 1395, 896, 397], 'correct': [13, 0, 0, 0]}\n",
      "ratio: 0.5746359223299228\n",
      "Epoch 03/25 train_loss=4.4914 val_loss=4.2170 BLEU1=0.003 BLEU4=0.000 time=778.3s lr=9.88e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "{'testlen': 1996, 'reflen': 3296, 'guess': [1996, 1497, 998, 499], 'correct': [420, 0, 0, 0]}\n",
      "ratio: 0.6055825242716609\n",
      "Epoch 04/25 train_loss=4.2983 val_loss=4.1179 BLEU1=0.110 BLEU4=0.000 time=786.9s lr=9.69e-05\n",
      "  -> Saved best_bleu.pt (BLEU1=0.1097)\n",
      "{'testlen': 1996, 'reflen': 3296, 'guess': [1996, 1497, 998, 499], 'correct': [2, 0, 0, 0]}\n",
      "ratio: 0.6055825242716609\n",
      "Epoch 05/25 train_loss=4.2023 val_loss=4.0518 BLEU1=0.001 BLEU4=0.000 time=782.9s lr=9.42e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "{'testlen': 1990, 'reflen': 3296, 'guess': [1990, 1491, 992, 496], 'correct': [420, 0, 0, 0]}\n",
      "ratio: 0.6037621359221469\n",
      "Epoch 06/25 train_loss=4.1083 val_loss=4.0030 BLEU1=0.109 BLEU4=0.000 time=789.6s lr=9.07e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "{'testlen': 4465, 'reflen': 4122, 'guess': [4465, 3966, 3467, 2968], 'correct': [15, 0, 0, 0]}\n",
      "ratio: 1.0832120329934294\n",
      "Epoch 07/25 train_loss=4.0526 val_loss=3.9695 BLEU1=0.003 BLEU4=0.000 time=787.0s lr=8.65e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "{'testlen': 2994, 'reflen': 3458, 'guess': [2994, 2495, 1996, 1497], 'correct': [2, 0, 0, 0]}\n",
      "ratio: 0.8658183921339312\n",
      "Epoch 08/25 train_loss=4.0222 val_loss=3.9368 BLEU1=0.001 BLEU4=0.000 time=785.1s lr=8.17e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "{'testlen': 6329, 'reflen': 4726, 'guess': [6329, 5830, 5331, 4832], 'correct': [38, 0, 0, 0]}\n",
      "ratio: 1.3391874735502878\n",
      "Epoch 09/25 train_loss=3.9856 val_loss=3.9167 BLEU1=0.006 BLEU4=0.000 time=787.5s lr=7.63e-05\n",
      "  -> No BLEU-1 improvement (patience inactive until epoch 10)\n",
      "[ES] Epoch 10: BLEU-1 patience becomes active; stall counter reset.\n"
     ]
    }
   ],
   "source": [
    "# Training cell (training + validation + BLEU-1-based model selection w/ patience)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch import nn\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required.\"\n",
    "device = torch.device('cuda')\n",
    "model_mm.to(device)\n",
    "\n",
    "grad_accum_steps = getattr(cfg, 'grad_accum_steps', 1)\n",
    "\n",
    "amp_dtype = None\n",
    "if getattr(cfg, 'use_amp', True):\n",
    "    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16\n",
    "        print(\"[AMP] Using bfloat16 mixed precision\")\n",
    "    else:\n",
    "        amp_dtype = torch.float16\n",
    "        print(\"[AMP] Using float16 mixed precision\")\n",
    "else:\n",
    "    print(\"[AMP] Disabled; using full float32\")\n",
    "\n",
    "main_params, clip_params = [], []\n",
    "for name, p in model_mm.named_parameters():\n",
    "    if not p.requires_grad: continue\n",
    "    (clip_params if name.startswith('clip.') else main_params).append(p)\n",
    "param_groups = []\n",
    "if main_params: param_groups.append({\"params\": main_params, \"lr\": cfg.lr})\n",
    "if clip_params:\n",
    "    scaled_lr = cfg.lr * getattr(cfg, 'clip_lr_scale', 0.05)\n",
    "    param_groups.append({\"params\": clip_params, \"lr\": scaled_lr})\n",
    "    print(f\"[INFO] CLIP fine-tune params: {len(clip_params)} with lr={scaled_lr:.2e}\")\n",
    "optimizer = AdamW(param_groups, weight_decay=cfg.weight_decay)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * cfg.epochs // grad_accum_steps\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if cfg.warmup_steps > 0 and step < cfg.warmup_steps:\n",
    "        return float(step) / float(max(1, cfg.warmup_steps))\n",
    "    progress = (step - cfg.warmup_steps) / float(max(1, total_steps - cfg.warmup_steps))\n",
    "    progress = min(max(progress, 0.0), 1.0)\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=amp_dtype is not None)\n",
    "CKPT_DIR = 'checkpoints'; os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# BLEU-1-based selection with early stopping AFTER first N epochs\n",
    "best_bleu = float('-inf')\n",
    "bleu_stall_epochs = 0\n",
    "bleu_patience = getattr(cfg, 'bleu_patience', 3)\n",
    "bleu_min_delta = getattr(cfg, 'bleu_min_delta', 0.0)\n",
    "min_epochs_before_early_stop = 10  # early-stop starts after finishing this many epochs\n",
    "BEST_CKPT_NAME = 'best_bleu.pt'\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_val_caption_metrics(model, val_dataset, images_root, batch_size=16, amp_dtype=None):\n",
    "    if val_dataset is None or len(val_dataset) == 0: return {}\n",
    "    refs_map = defaultdict(list)\n",
    "    for fname, cap in val_dataset.samples:\n",
    "        c = cap.strip()\n",
    "        if c: refs_map[fname].append(c)\n",
    "    if not refs_map: return {}\n",
    "    image_files = list(refs_map.keys())\n",
    "    image_paths = [os.path.join(images_root, f) for f in image_files]\n",
    "    hyps, refs = [], []\n",
    "    def _generate(paths):\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "    model.eval()\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        b = image_paths[i:i+batch_size]\n",
    "        outs = _generate(b)\n",
    "        for pth, pred in zip(b, outs):\n",
    "            fname = os.path.basename(pth)\n",
    "            hyps.append(pred if pred else \"<EMPTY>\")\n",
    "            refs.append(refs_map.get(fname, [\" \"]))\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "    metrics = {}\n",
    "    try:\n",
    "        from pycocoevalcap.bleu.bleu import Bleu\n",
    "        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "        metrics['bleu1'] = float(bleu_scores[0])\n",
    "        metrics['bleu4'] = float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "    return metrics\n",
    "\n",
    "print(f\"[TRAIN] Starting training with BLEU-1-based checkpointing + delayed patience (after {min_epochs_before_early_stop} epochs). GradAccum={grad_accum_steps}\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
    "\n",
    "def compute_loss_with_label_smoothing(model_out):\n",
    "    logits = model_out.logits  # (B, T, V)\n",
    "    labels = getattr(model_out, 'labels', None)\n",
    "    if labels is None:\n",
    "        raise AttributeError('Labels not found on model output; ensure forward attaches them.')\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss\n",
    "\n",
    "global_step = 0\n",
    "stopped_early = False\n",
    "for epoch in range(cfg.epochs):\n",
    "    # Activate patience window after threshold and reset stall counter once\n",
    "    if (epoch + 1) == min_epochs_before_early_stop:\n",
    "        bleu_stall_epochs = 0\n",
    "        print(f\"[ES] Epoch {epoch+1}: BLEU-1 patience becomes active; stall counter reset.\")\n",
    "\n",
    "    model_mm.train()\n",
    "    train_sum = 0.0\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        imgs, srcs, tgts = batch\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                loss = compute_loss_with_label_smoothing(out) / grad_accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            out = model_mm(imgs, srcs, tgts)\n",
    "            loss = compute_loss_with_label_smoothing(out) / grad_accum_steps\n",
    "            loss.backward()\n",
    "        if step % grad_accum_steps == 0:\n",
    "            if amp_dtype is not None:\n",
    "                if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            global_step += 1\n",
    "        train_sum += (loss.item() * grad_accum_steps)\n",
    "    train_epoch_loss = train_sum / max(1, len(train_loader))\n",
    "\n",
    "    val_epoch_loss = None\n",
    "    if val_loader:\n",
    "        model_mm.eval(); v = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                if amp_dtype is not None:\n",
    "                    with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                        out = model_mm(imgs, srcs, tgts)\n",
    "                        loss_val = compute_loss_with_label_smoothing(out)\n",
    "                else:\n",
    "                    out = model_mm(imgs, srcs, tgts)\n",
    "                    loss_val = compute_loss_with_label_smoothing(out)\n",
    "                v += loss_val.item()\n",
    "        val_epoch_loss = v / max(1, len(val_loader))\n",
    "\n",
    "    metrics = compute_val_caption_metrics(model_mm, val_dataset, images_root, batch_size=16, amp_dtype=amp_dtype)\n",
    "    bleu1 = metrics.get('bleu1', float('-inf'))\n",
    "    bleu4 = metrics.get('bleu4', float('nan'))\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    if val_epoch_loss is not None:\n",
    "        print(f\"Epoch {epoch+1:02d}/{cfg.epochs} train_loss={train_epoch_loss:.4f} val_loss={val_epoch_loss:.4f} BLEU1={bleu1:.3f} BLEU4={bleu4:.3f} time={dt:.1f}s lr={current_lr:.2e}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:02d}/{cfg.epochs} train_loss={train_epoch_loss:.4f} BLEU1={bleu1:.3f} BLEU4={bleu4:.3f} time={dt:.1f}s lr={current_lr:.2e}\")\n",
    "\n",
    "    if use_wandb:\n",
    "        log_dict = {\n",
    "            'train/epoch_loss': train_epoch_loss,\n",
    "            'lr': current_lr,\n",
    "            'val/bleu1': bleu1,\n",
    "            'val/bleu4': bleu4,\n",
    "        }\n",
    "        if val_epoch_loss is not None:\n",
    "            log_dict['val/epoch_loss'] = val_epoch_loss\n",
    "        wandb.log(log_dict, step=epoch)\n",
    "\n",
    "    improved_bleu = (bleu1 - best_bleu) > bleu_min_delta\n",
    "    if improved_bleu:\n",
    "        best_bleu = bleu1\n",
    "        bleu_stall_epochs = 0\n",
    "        save_payload = {\n",
    "            'model': model_mm.state_dict(),\n",
    "            'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_bleu1': best_bleu,\n",
    "            'bleu4': bleu4,\n",
    "            'val_loss': val_epoch_loss,\n",
    "            'train_loss': train_epoch_loss,\n",
    "            'global_step': global_step,\n",
    "        }\n",
    "        torch.save(save_payload, os.path.join(CKPT_DIR, BEST_CKPT_NAME))\n",
    "        print(f\"  -> Saved {BEST_CKPT_NAME} (BLEU1={best_bleu:.4f})\")\n",
    "    else:\n",
    "        if (epoch + 1) >= min_epochs_before_early_stop:\n",
    "            bleu_stall_epochs += 1\n",
    "            print(f\"  -> No BLEU-1 improvement (stall epochs={bleu_stall_epochs}/{bleu_patience})\")\n",
    "        else:\n",
    "            print(\"  -> No BLEU-1 improvement (patience inactive until epoch 10)\")\n",
    "\n",
    "    can_early_stop = (epoch + 1) >= min_epochs_before_early_stop\n",
    "    if can_early_stop and bleu_stall_epochs >= bleu_patience:\n",
    "        print(f\"[Early Stop] BLEU-1 not improved for {bleu_patience} consecutive epochs after epoch {min_epochs_before_early_stop}.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "print(f\"Training finished. Best BLEU-1={best_bleu:.4f} {'(early stop)' if stopped_early else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-10T14:55:36.779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Model Export / Import Utilities ===\n",
    "import os, json, torch, math\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# ------------------------------\n",
    "# Export\n",
    "# ------------------------------\n",
    "\n",
    "def export_model(\n",
    "    save_dir: str,\n",
    "    model: torch.nn.Module,\n",
    "    cfg_obj,\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "    epoch: Optional[int] = None,\n",
    "    global_step: Optional[int] = None,\n",
    "    best_val: Optional[float] = None,\n",
    "    tag: str = \"latest\",\n",
    "    use_safetensors: bool = False,\n",
    "    # --- BLEU-aware exporting additions ---\n",
    "    best_bleu: Optional[float] = None,\n",
    "    bleu_scores: Optional[Dict[str, float]] = None,  # e.g., {'bleu1': ..., 'bleu4': ...}\n",
    "    save_best_on_bleu: bool = False,\n",
    "    bleu_min_delta: float = 0.0,\n",
    "    best_registry_filename: str = \"best_bleu.json\",\n",
    ") -> str:\n",
    "    \"\"\"Export model checkpoint + (optional) optimizer/scheduler.\n",
    "\n",
    "    Saves:\n",
    "      - config.json (raw cfg values)\n",
    "      - tokenizer/ (HF tokenizer)\n",
    "      - clip_mt5_prefix_<tag>.pt (bundle) OR .safetensors (+ meta files)\n",
    "\n",
    "    Bundle (.pt) contains:\n",
    "      model_state, cfg, epoch, global_step, tag, export_time,\n",
    "      optimizer_state?, scheduler_state?, best_val?, best_bleu?, bleu_scores?, hyperparams.\n",
    "\n",
    "    If save_best_on_bleu=True, the function will only save when `best_bleu`\n",
    "    strictly improves over the previous best by > bleu_min_delta. Best state is\n",
    "    tracked in <save_dir>/<best_registry_filename>.\n",
    "\n",
    "    hyperparams dict added so retraining can auto‑rebuild optimizer/scheduler:\n",
    "      { lr, clip_lr_scale, weight_decay, warmup_steps, grad_clip, use_amp, use_bf16 }\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Collect base config keys if available\n",
    "    if 'base_config' in globals():\n",
    "        cfg_dict = {k: getattr(cfg_obj, k) for k in base_config.keys() if hasattr(cfg_obj, k)}\n",
    "    else:\n",
    "        cfg_dict = {k: v for k, v in vars(cfg_obj).items() if not k.startswith('_')}\n",
    "\n",
    "    # Persist config separately (human readable)\n",
    "    with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(cfg_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save tokenizer (best effort)\n",
    "    try:\n",
    "        model.tokenizer.save_pretrained(os.path.join(save_dir, 'tokenizer'))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Tokenizer save failed: {e}\")\n",
    "\n",
    "    # Load current best BLEU from registry (if gating by BLEU)\n",
    "    registry_path = os.path.join(save_dir, best_registry_filename)\n",
    "    prev_best_bleu = None\n",
    "    prev_best_path = None\n",
    "    if os.path.isfile(registry_path):\n",
    "        try:\n",
    "            with open(registry_path, 'r') as rf:\n",
    "                reg = json.load(rf)\n",
    "            prev_best_bleu = reg.get('best_bleu')\n",
    "            prev_best_path = reg.get('path')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if save_best_on_bleu:\n",
    "        if best_bleu is None:\n",
    "            print(\"[EXPORT] save_best_on_bleu=True but best_bleu=None; skip saving.\")\n",
    "            return prev_best_path or \"\"\n",
    "        improved = (prev_best_bleu is None) or ((best_bleu - float(prev_best_bleu)) > float(bleu_min_delta))\n",
    "        if not improved:\n",
    "            print(f\"[EXPORT] BLEU not improved (cur={best_bleu}, best={prev_best_bleu}); skip.\")\n",
    "            return prev_best_path or \"\"\n",
    "        # If we're saving by BLEU and tag isn't explicit, use 'best'\n",
    "        if tag in (None, \"\", \"latest\"):\n",
    "            tag = \"best\"\n",
    "\n",
    "    checkpoint_name = f\"clip_mt5_prefix_{tag}\"\n",
    "\n",
    "    hyperparams = {\n",
    "        'lr': cfg_dict.get('lr'),\n",
    "        'clip_lr_scale': cfg_dict.get('clip_lr_scale'),\n",
    "        'weight_decay': cfg_dict.get('weight_decay'),\n",
    "        'warmup_steps': cfg_dict.get('warmup_steps'),\n",
    "        'grad_clip': cfg_dict.get('grad_clip'),\n",
    "        'use_amp': cfg_dict.get('use_amp'),\n",
    "        'use_bf16': cfg_dict.get('use_bf16'),\n",
    "        'batch_size': cfg_dict.get('batch_size'),\n",
    "    }\n",
    "\n",
    "    # Common metadata for both formats\n",
    "    meta = {\n",
    "        'cfg': cfg_dict,\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'export_time': datetime.utcnow().isoformat() + 'Z',\n",
    "        'tag': tag,\n",
    "        'best_val': best_val,\n",
    "        'best_bleu': best_bleu,\n",
    "        'bleu_scores': bleu_scores,\n",
    "        'has_optimizer': optimizer is not None,\n",
    "        'has_scheduler': scheduler is not None,\n",
    "        'hyperparams': hyperparams,\n",
    "    }\n",
    "\n",
    "    saved_path = \"\"\n",
    "\n",
    "    if use_safetensors:\n",
    "        try:\n",
    "            from safetensors.torch import save_file\n",
    "            weights = model.state_dict()\n",
    "            save_path = os.path.join(save_dir, checkpoint_name + '.safetensors')\n",
    "            save_file(weights, save_path)\n",
    "            if optimizer:\n",
    "                torch.save(optimizer.state_dict(), os.path.join(save_dir, checkpoint_name + '.optimizer.pt'))\n",
    "            if scheduler:\n",
    "                torch.save(scheduler.state_dict(), os.path.join(save_dir, checkpoint_name + '.scheduler.pt'))\n",
    "            # Write meta.json (always reflects last export)\n",
    "            with open(os.path.join(save_dir, 'meta.json'), 'w') as f:\n",
    "                json.dump(meta, f, indent=2)\n",
    "            print(f\"[EXPORT] Weights -> {checkpoint_name}.safetensors (meta.json written)\")\n",
    "            saved_path = save_path\n",
    "        except ImportError:\n",
    "            print('[WARN] safetensors not installed; falling back to .pt')\n",
    "            use_safetensors = False\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] safetensors save failed: {e}\")\n",
    "            return prev_best_path or \"\"\n",
    "\n",
    "    if not use_safetensors:\n",
    "        # Standard .pt route\n",
    "        bundle = {\n",
    "            'model_state': model.state_dict(),\n",
    "            'cfg': cfg_dict,\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'export_time': datetime.utcnow().isoformat() + 'Z',\n",
    "            'tag': tag,\n",
    "            'best_val': best_val,\n",
    "            'best_bleu': best_bleu,\n",
    "            'bleu_scores': bleu_scores,\n",
    "            'hyperparams': hyperparams,\n",
    "        }\n",
    "        if optimizer:\n",
    "            bundle['optimizer_state'] = optimizer.state_dict()\n",
    "        if scheduler:\n",
    "            bundle['scheduler_state'] = scheduler.state_dict()\n",
    "        out_path = os.path.join(save_dir, checkpoint_name + '.pt')\n",
    "        torch.save(bundle, out_path)\n",
    "        # Also emit meta.json for parity with safetensors\n",
    "        with open(os.path.join(save_dir, 'meta.json'), 'w') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        print(f\"[EXPORT] Saved checkpoint: {out_path}\")\n",
    "        saved_path = out_path\n",
    "\n",
    "    # Update best BLEU registry if we saved under BLEU gating\n",
    "    if save_best_on_bleu and best_bleu is not None and saved_path:\n",
    "        try:\n",
    "            reg = {\n",
    "                'best_bleu': float(best_bleu),\n",
    "                'bleu_scores': bleu_scores,\n",
    "                'path': saved_path,\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'updated_at': datetime.utcnow().isoformat() + 'Z',\n",
    "            }\n",
    "            with open(registry_path, 'w') as wf:\n",
    "                json.dump(reg, wf, indent=2)\n",
    "            print(f\"[EXPORT] Updated registry {best_registry_filename}: best_bleu={best_bleu:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not update best BLEU registry: {e}\")\n",
    "\n",
    "    return saved_path\n",
    "\n",
    "# Convenience wrapper: export only if BLEU improved, saved as 'best'\n",
    "def export_best_bleu(\n",
    "    save_dir: str,\n",
    "    model: torch.nn.Module,\n",
    "    cfg_obj,\n",
    "    *,\n",
    "    best_bleu: float,\n",
    "    bleu_scores: Optional[Dict[str, float]] = None,\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "    epoch: Optional[int] = None,\n",
    "    global_step: Optional[int] = None,\n",
    "    use_safetensors: bool = False,\n",
    "    bleu_min_delta: float = 0.0,\n",
    "    best_registry_filename: str = \"best_bleu.json\",\n",
    ") -> str:\n",
    "    return export_model(\n",
    "        save_dir=save_dir,\n",
    "        model=model,\n",
    "        cfg_obj=cfg_obj,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        best_val=None,\n",
    "        tag=\"best\",\n",
    "        use_safetensors=use_safetensors,\n",
    "        best_bleu=best_bleu,\n",
    "        bleu_scores=bleu_scores,\n",
    "        save_best_on_bleu=True,\n",
    "        bleu_min_delta=bleu_min_delta,\n",
    "        best_registry_filename=best_registry_filename,\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# Helper: build optimizer (main + clip groups)\n",
    "# ------------------------------\n",
    "\n",
    "def build_optimizer_from_hparams(model: torch.nn.Module, h: Dict[str, Any]):\n",
    "    lr = h.get('lr', 1e-4)\n",
    "    clip_lr_scale = h.get('clip_lr_scale', 0.05) or 0.05\n",
    "    weight_decay = h.get('weight_decay', 0.0)\n",
    "    main_params, clip_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (clip_params if n.startswith('clip.') else main_params).append(p)\n",
    "    param_groups = []\n",
    "    if main_params:\n",
    "        param_groups.append({'params': main_params, 'lr': lr})\n",
    "    if clip_params:\n",
    "        param_groups.append({'params': clip_params, 'lr': lr * clip_lr_scale})\n",
    "    if clip_params:\n",
    "        print(f\"[OPT] CLIP params: {len(clip_params)} lr={lr * clip_lr_scale:.2e}\")\n",
    "    return torch.optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "\n",
    "# ------------------------------\n",
    "# Helper: build cosine scheduler with warmup (same as training)\n",
    "# ------------------------------\n",
    "\n",
    "def build_scheduler_from_hparams(optimizer, h: Dict[str, Any], steps_per_epoch: int, total_epochs: int):\n",
    "    warmup_steps = h.get('warmup_steps', 0) or 0\n",
    "    total_steps = steps_per_epoch * total_epochs\n",
    "    def lr_lambda(step):\n",
    "        if warmup_steps > 0 and step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps)) if total_steps > warmup_steps else 1.0\n",
    "        progress = min(max(progress, 0.0), 1.0)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ------------------------------\n",
    "# Import for finetune (unchanged except now returns hyperparams)\n",
    "# ------------------------------\n",
    "\n",
    "def load_model_for_finetune(\n",
    "    load_dir: str,\n",
    "    device: torch.device,\n",
    "    checkpoint_tag: str = 'latest',\n",
    "    resume_optimizer: bool = True,\n",
    "    build_optimizer_fn=None,\n",
    "    build_scheduler_fn=None,\n",
    "    override_cfg: Optional[Dict[str, Any]] = None,\n",
    "    prefer_safetensors: bool = True\n",
    "):\n",
    "    # Load config\n",
    "    with open(os.path.join(load_dir, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "        cfg_json = json.load(f)\n",
    "    if override_cfg:\n",
    "        cfg_json.update(override_cfg)\n",
    "\n",
    "    class _Cfg: ...\n",
    "    cfg_obj = _Cfg()\n",
    "    for k, v in cfg_json.items():\n",
    "        setattr(cfg_obj, k, v)\n",
    "\n",
    "    model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "\n",
    "    ckpt_base = f\"clip_mt5_prefix_{checkpoint_tag}\"\n",
    "    safepath = os.path.join(load_dir, ckpt_base + '.safetensors')\n",
    "    ptpath = os.path.join(load_dir, ckpt_base + '.pt')\n",
    "\n",
    "    optimizer_state = scheduler_state = None\n",
    "    epoch = global_step = None\n",
    "    hyperparams = None\n",
    "    used_safetensors = False\n",
    "\n",
    "    if prefer_safetensors and os.path.isfile(safepath):\n",
    "        try:\n",
    "            from safetensors.torch import load_file\n",
    "            weights = load_file(safepath, device=device)\n",
    "            model.load_state_dict(weights, strict=True)\n",
    "            used_safetensors = True\n",
    "            meta_path = os.path.join(load_dir, 'meta.json')\n",
    "            meta = {}\n",
    "            if os.path.isfile(meta_path):\n",
    "                with open(meta_path, 'r') as f:\n",
    "                    meta = json.load(f)\n",
    "            epoch = meta.get('epoch'); global_step = meta.get('global_step')\n",
    "            hyperparams = meta.get('hyperparams')\n",
    "            # Optional: read BLEU metadata (not returned)\n",
    "            _best_bleu = meta.get('best_bleu'); _bleu_scores = meta.get('bleu_scores')\n",
    "            if resume_optimizer and meta.get('has_optimizer'):\n",
    "                opt_file = os.path.join(load_dir, ckpt_base + '.optimizer.pt')\n",
    "                if os.path.isfile(opt_file):\n",
    "                    optimizer_state = torch.load(opt_file, map_location='cpu')\n",
    "            if resume_optimizer and meta.get('has_scheduler'):\n",
    "                sch_file = os.path.join(load_dir, ckpt_base + '.scheduler.pt')\n",
    "                if os.path.isfile(sch_file):\n",
    "                    scheduler_state = torch.load(sch_file, map_location='cpu')\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] safetensors load failed ({e}); falling back to .pt\")\n",
    "            used_safetensors = False\n",
    "\n",
    "    if not used_safetensors:\n",
    "        if not os.path.isfile(ptpath):\n",
    "            raise FileNotFoundError(f\"No checkpoint found at {ptpath}\")\n",
    "        bundle = torch.load(ptpath, map_location=device)\n",
    "        model.load_state_dict(bundle['model_state'], strict=True)\n",
    "        epoch = bundle.get('epoch'); global_step = bundle.get('global_step')\n",
    "        hyperparams = bundle.get('hyperparams')\n",
    "        # Optional BLEU metadata\n",
    "        _best_bleu = bundle.get('best_bleu'); _bleu_scores = bundle.get('bleu_scores')\n",
    "        if resume_optimizer:\n",
    "            optimizer_state = bundle.get('optimizer_state')\n",
    "            scheduler_state = bundle.get('scheduler_state')\n",
    "\n",
    "    print(f\"[IMPORT] Model loaded (epoch={epoch}, global_step={global_step})\")\n",
    "    return model, cfg_obj, hyperparams, optimizer_state, scheduler_state, epoch, global_step\n",
    "\n",
    "# ------------------------------\n",
    "# Simple one-shot retrain loader\n",
    "# ------------------------------\n",
    "\n",
    "def load_model_for_retrain(\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    new_lr: Optional[float] = None,\n",
    "    new_clip_lr_scale: Optional[float] = None,\n",
    "    reset_optimizer: bool = True,\n",
    "    freeze_clip: Optional[bool] = None,\n",
    "    override_cfg: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"Load a .pt bundle and prepare model + (fresh) optimizer hyperparams for retraining.\n",
    "\n",
    "    Returns (model, cfg_obj, optimizer_hparams, epoch_loaded, global_step_loaded)\n",
    "    Caller should then: optimizer = build_optimizer_from_hparams(model, optimizer_hparams)\n",
    "    and build a new scheduler with build_scheduler_from_hparams once steps_per_epoch known.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "    bundle = torch.load(checkpoint_path, map_location=device)\n",
    "    if 'model_state' not in bundle:\n",
    "        raise ValueError('Not an export_model bundle (.pt)')\n",
    "    cfg_json = dict(bundle['cfg'])\n",
    "    if override_cfg:\n",
    "        cfg_json.update(override_cfg)\n",
    "\n",
    "    class _Cfg: ...\n",
    "    cfg_obj = _Cfg()\n",
    "    for k, v in cfg_json.items():\n",
    "        setattr(cfg_obj, k, v)\n",
    "    if freeze_clip is not None:\n",
    "        cfg_obj.freeze_clip = freeze_clip\n",
    "\n",
    "    model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "    model.load_state_dict(bundle['model_state'], strict=True)\n",
    "\n",
    "    base_h = bundle.get('hyperparams', {})\n",
    "    # Override learning rates if requested\n",
    "    if new_lr is not None:\n",
    "        base_h['lr'] = new_lr\n",
    "    if new_clip_lr_scale is not None:\n",
    "        base_h['clip_lr_scale'] = new_clip_lr_scale\n",
    "\n",
    "    # If freezing clip newly, we still keep clip_lr_scale but it won't matter\n",
    "    epoch_loaded = bundle.get('epoch')\n",
    "    global_step_loaded = bundle.get('global_step')\n",
    "    # Optional BLEU metadata\n",
    "    _best_bleu = bundle.get('best_bleu'); _bleu_scores = bundle.get('bleu_scores')\n",
    "    print(f\"[RETRAIN] Loaded weights (epoch={epoch_loaded}). Preparing fresh optimizer hyperparams.\")\n",
    "    return model, cfg_obj, base_h, epoch_loaded, global_step_loaded\n",
    "\n",
    "print('[READY] Enhanced export/import + retrain helpers available (BLEU-aware best checkpointing).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "export_best_bleu(\n",
    "    save_dir=CKPT_DIR,\n",
    "    model=model_mm,\n",
    "    cfg_obj=cfg,\n",
    "    best_bleu=best_bleu,\n",
    "    bleu_scores={'bleu1': best_bleu},\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epoch=epoch,\n",
    "    global_step=global_step,\n",
    "    use_safetensors=False,\n",
    "    bleu_min_delta=bleu_min_delta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # --- Validation Subset Metrics (pycocoevalcap) Helper ---\n",
    "# # Use to compute BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE on a fixed subset (default 200) of validation images each epoch.\n",
    "# # Call inside training/resume loops after computing (train/val) losses.\n",
    "\n",
    "# def compute_val_subset_metrics(\n",
    "#     model,\n",
    "#     val_dataset,\n",
    "#     images_root: str,\n",
    "#     device,\n",
    "#     amp_dtype=None,\n",
    "#     sample_size: int = 200,\n",
    "#     seed: int = 42,\n",
    "#     batch_size: int = 8,\n",
    "#     verbose: bool = False,\n",
    "# ):\n",
    "#     import random, time, os, torch\n",
    "#     from collections import defaultdict\n",
    "#     if val_dataset is None or len(val_dataset) == 0:\n",
    "#         return {}\n",
    "\n",
    "#     # Build refs map (filename -> list[captions]) based on dataset.samples (filename, caption)\n",
    "#     refs_map = defaultdict(list)\n",
    "#     for (fname, cap) in val_dataset.samples:\n",
    "#         c = cap.strip()\n",
    "#         if c:\n",
    "#             refs_map[fname].append(c)\n",
    "#     unique_files = list(refs_map.keys())\n",
    "#     if not unique_files:\n",
    "#         return {}\n",
    "#     random.Random(seed).shuffle(unique_files)\n",
    "#     subset_files = unique_files[: min(sample_size, len(unique_files))]\n",
    "\n",
    "#     image_paths = [os.path.join(images_root, f) for f in subset_files]\n",
    "\n",
    "#     hyps, refs = [], []\n",
    "#     model.eval()\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     def _gen(paths):\n",
    "#         if amp_dtype is not None:\n",
    "#             with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "#                 return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "#         return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "#     for i in range(0, len(image_paths), batch_size):\n",
    "#         batch = image_paths[i:i+batch_size]\n",
    "#         outs = _gen(batch)\n",
    "#         for p, pred in zip(batch, outs):\n",
    "#             fname = os.path.basename(p)\n",
    "#             hyps.append(pred if pred else \"<EMPTY>\")\n",
    "#             refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "#     gts = {i: refs[i] for i in range(len(refs))}\n",
    "#     res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "#     from pycocoevalcap.bleu.bleu import Bleu\n",
    "#     from pycocoevalcap.meteor.meteor import Meteor\n",
    "#     from pycocoevalcap.rouge.rouge import Rouge\n",
    "#     from pycocoevalcap.cider.cider import Cider\n",
    "#     try:\n",
    "#         from pycocoevalcap.spice.spice import Spice\n",
    "#     except Exception:\n",
    "#         Spice = None\n",
    "\n",
    "#     metrics = {}\n",
    "#     try:\n",
    "#         bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "#         metrics['bleu1'] = float(bleu_scores[0]); metrics['bleu2'] = float(bleu_scores[1])\n",
    "#         metrics['bleu3'] = float(bleu_scores[2]); metrics['bleu4'] = float(bleu_scores[3])\n",
    "#         metrics['bleu'] = float(bleu_scores[3])\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] BLEU failed:', e)\n",
    "#     try:\n",
    "#         meteor_score, _ = Meteor().compute_score(gts, res)\n",
    "#         metrics['meteor'] = float(meteor_score)\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] METEOR failed:', e)\n",
    "#     try:\n",
    "#         rouge_score, _ = Rouge().compute_score(gts, res)\n",
    "#         metrics['rougeL'] = float(rouge_score)\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] ROUGE-L failed:', e)\n",
    "#     try:\n",
    "#         cider_score, _ = Cider().compute_score(gts, res)\n",
    "#         metrics['cider'] = float(cider_score)\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] CIDEr failed:', e)\n",
    "#     if Spice:\n",
    "#         try:\n",
    "#             spice_score, _ = Spice().compute_score(gts, res)\n",
    "#             metrics['spice'] = float(spice_score)\n",
    "#         except Exception as e:\n",
    "#             if verbose: print('[VAL_SUBSET] SPICE failed:', e)\n",
    "\n",
    "#     metrics['subset_size'] = len(subset_files)\n",
    "#     metrics['eval_time_s'] = round(time.time() - t0, 2)\n",
    "#     if verbose:\n",
    "#         print('[VAL_SUBSET]', ', '.join(f\"{k}={v:.3f}\" for k,v in metrics.items() if isinstance(v,(int,float))))\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # === Direct .pt Checkpoint Loader (file-based) ===\n",
    "# import torch, os, json\n",
    "# from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# def load_model_from_checkpoint_file(\n",
    "#     checkpoint_path: str,\n",
    "#     device: torch.device,\n",
    "#     resume_optimizer: bool = False,\n",
    "#     build_optimizer_fn=None,\n",
    "#     build_scheduler_fn=None,\n",
    "#     override_cfg: Optional[Dict[str, Any]] = None,\n",
    "# ):\n",
    "#     \"\"\"Load model (and optional optimizer/scheduler) directly from a single .pt file.\n",
    "\n",
    "#     Expects the .pt bundle produced by export_model (contains 'model_state' and 'cfg').\n",
    "\n",
    "#     Returns: (model, cfg_obj, optimizer, scheduler, epoch, global_step)\n",
    "#     \"\"\"\n",
    "#     assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "#     bundle = torch.load(checkpoint_path, map_location=device)\n",
    "#     if 'model_state' not in bundle or 'cfg' not in bundle:\n",
    "#         raise ValueError(\"Checkpoint missing required keys 'model_state' or 'cfg'\")\n",
    "\n",
    "#     cfg_json = dict(bundle['cfg'])\n",
    "#     if override_cfg:\n",
    "#         cfg_json.update(override_cfg)\n",
    "\n",
    "#     class _Cfg: ...\n",
    "#     cfg_obj = _Cfg()\n",
    "#     for k, v in cfg_json.items():\n",
    "#         setattr(cfg_obj, k, v)\n",
    "\n",
    "#     model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "#     model.load_state_dict(bundle['model_state'], strict=True)\n",
    "\n",
    "#     epoch = bundle.get('epoch')\n",
    "#     global_step = bundle.get('global_step')\n",
    "\n",
    "#     optimizer = None\n",
    "#     scheduler = None\n",
    "#     if resume_optimizer and 'optimizer_state' in bundle:\n",
    "#         if build_optimizer_fn is None:\n",
    "#             print('[WARN] Optimizer state present but build_optimizer_fn not provided; skipping optimizer restore.')\n",
    "#         else:\n",
    "#             optimizer = build_optimizer_fn(cfg_obj, model)\n",
    "#             try:\n",
    "#                 optimizer.load_state_dict(bundle['optimizer_state'])\n",
    "#                 print('[IMPORT] Optimizer state restored.')\n",
    "#             except Exception as e:\n",
    "#                 print(f'[WARN] Failed to load optimizer state: {e}')\n",
    "#     if resume_optimizer and optimizer and 'scheduler_state' in bundle and build_scheduler_fn:\n",
    "#         scheduler = build_scheduler_fn(cfg_obj, optimizer)\n",
    "#         try:\n",
    "#             scheduler.load_state_dict(bundle['scheduler_state'])\n",
    "#             print('[IMPORT] Scheduler state restored.')\n",
    "#         except Exception as e:\n",
    "#             print(f'[WARN] Failed to load scheduler state: {e}')\n",
    "\n",
    "#     print(f\"[IMPORT] Loaded model from file: {checkpoint_path}\")\n",
    "#     return model, cfg_obj, optimizer, scheduler, epoch, global_step\n",
    "\n",
    "# # One-line usage example (inference only):\n",
    "# # model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt', device=torch.device('cuda'), resume_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/input/25_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt', device=torch.device('cuda'), resume_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Testing / Inference cell (full test set evaluation with BLEU, CIDEr, BERTScore, optional others)\n",
    "import torch, time, os, json, math\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required for inference.\"\n",
    "model_mm.eval()\n",
    "device = torch.device('cuda')\n",
    "model_mm.to(device)\n",
    "\n",
    "# Determine AMP dtype consistent with training\n",
    "amp_dtype = None\n",
    "if getattr(cfg, 'use_amp', True):\n",
    "    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16\n",
    "    else:\n",
    "        amp_dtype = torch.float16\n",
    "\n",
    "# -------------------- Test Loss --------------------\n",
    "test_loss = None\n",
    "if test_loader:\n",
    "    t0 = time.time(); total=0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            imgs, srcs, tgts = batch\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            if amp_dtype is not None:\n",
    "                with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                    out = model_mm(imgs, srcs, tgts)\n",
    "            else:\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "            # Out.loss already computed with original (not smoothed) HF path; we just aggregate\n",
    "            total += out.loss.item()\n",
    "    test_loss = total / max(1,len(test_loader))\n",
    "    print(f\"Test loss={test_loss:.4f} time={(time.time()-t0):.1f}s\")\n",
    "else:\n",
    "    print(\"No test split available.\")\n",
    "\n",
    "#############################################\n",
    "# Caption Quality Metrics (BLEU1-4, CIDEr, BERTScore, optional others)\n",
    "#############################################\n",
    "metrics = {}\n",
    "try:\n",
    "    import nltk\n",
    "    for pkg in ['punkt','wordnet','omw-1.4']:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{pkg}')\n",
    "        except Exception:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Build reference sets (all captions per test image) from original JSON\n",
    "    with open(json_path) as f:\n",
    "        data_json = json.load(f)\n",
    "    img_entries = data_json['images'] if isinstance(data_json, dict) else data_json\n",
    "\n",
    "    # Gather unique test image filenames present in test_dataset\n",
    "    test_image_names = sorted({s[0] for s in test_dataset.samples})\n",
    "    refs_map = {}\n",
    "    for entry in img_entries:\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        fname = entry.get('filename')\n",
    "        if fname in test_image_names:\n",
    "            all_caps = []\n",
    "            for s in entry.get('sentences', []):\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    cap = s['raw'].strip()\n",
    "                    if cap:\n",
    "                        all_caps.append(cap)\n",
    "            if all_caps:\n",
    "                refs_map[fname] = all_caps\n",
    "\n",
    "    # Generate hypotheses (batched)\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    BATCH_GEN = 12\n",
    "    image_paths = [os.path.join(images_root, fname) for fname in test_image_names]\n",
    "\n",
    "    def _generate_batch(paths):\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                return model_mm.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model_mm.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    gen_start = time.time()\n",
    "    for i in range(0, len(image_paths), BATCH_GEN):\n",
    "        batch_paths = image_paths[i:i+BATCH_GEN]\n",
    "        caps = _generate_batch(batch_paths)\n",
    "        for p, pred in zip(batch_paths, caps):\n",
    "            fname = os.path.basename(p)\n",
    "            pred = pred if pred else \"<EMPTY>\"\n",
    "            hyps.append(pred)\n",
    "            cur_refs = refs_map.get(fname, [\" \"])\n",
    "            refs.append([r for r in cur_refs if r.strip()] or [\" \"])\n",
    "    print(f\"[GEN] Generated {len(hyps)} captions in {time.time()-gen_start:.1f}s\")\n",
    "\n",
    "    # COCO-style dicts for pycocoevalcap\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "    # ---- pycocoevalcap metrics ----\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    try:\n",
    "        bleu_scorer = Bleu(4)\n",
    "        bleu_scores, _ = bleu_scorer.compute_score(gts, res)\n",
    "        metrics['bleu1'] = float(bleu_scores[0])\n",
    "        metrics['bleu2'] = float(bleu_scores[1])\n",
    "        metrics['bleu3'] = float(bleu_scores[2])\n",
    "        metrics['bleu4'] = float(bleu_scores[3])\n",
    "        metrics['bleu']  = float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "    try:\n",
    "        cider_scorer = Cider()\n",
    "        cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "        metrics['cider'] = float(cider_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] CIDEr failed:', e)\n",
    "\n",
    "    # ---- BERTScore (evaluate) ----\n",
    "    try:\n",
    "        import evaluate\n",
    "        bertscore = evaluate.load('bertscore')\n",
    "        # Use first reference per image for BERTScore consistency with training loop\n",
    "        single_refs = [r[0] if isinstance(r, list) and r else '' for r in refs]\n",
    "        bs = bertscore.compute(predictions=hyps, references=single_refs, lang='en', device=0)\n",
    "        metrics['bertscore_f1'] = float(sum(bs['f1'])/len(bs['f1']))\n",
    "    except Exception as e:\n",
    "        print('[WARN] BERTScore failed:', e)\n",
    "\n",
    "    if test_loss is not None:\n",
    "        metrics['test_loss'] = test_loss\n",
    "\n",
    "    print(\"=== Caption Metrics (Test Set) ===\")\n",
    "    for k,v in metrics.items():\n",
    "        if isinstance(v,(int,float)):\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log({f\"eval/{k}\": v for k,v in metrics.items()}, commit=True)\n",
    "        print(\"[wandb] Logged evaluation metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Metric computation failed:\", e)\n",
    "\n",
    "# Sample qualitative outputs\n",
    "default_gen_kwargs = dict(repetition_penalty=1.2, no_repeat_ngram_size=3)\n",
    "SAMPLE_PRINTS = 5\n",
    "if test_loader and SAMPLE_PRINTS > 0:\n",
    "    shown = 0\n",
    "    printed_imgs = set()\n",
    "    for img_path, _ in [(os.path.join(images_root, s[0]), s[1]) for s in test_dataset.samples]:\n",
    "        if img_path in printed_imgs:\n",
    "            continue\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n",
    "        else:\n",
    "            caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n",
    "        gt_caption = next(s[1] for s in test_dataset.samples if os.path.join(images_root, s[0]) == img_path)\n",
    "        print('GT:', gt_caption)\n",
    "        print('CAP:', caps[0])\n",
    "        print('---------------')\n",
    "        printed_imgs.add(img_path)\n",
    "        shown += 1\n",
    "        if shown >= SAMPLE_PRINTS:\n",
    "            break\n",
    "\n",
    "# Helper for generating with beam or sampling\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_captions(image_paths: List[str], mode='beam', **kwargs):\n",
    "    if mode == 'sample':\n",
    "        sample_defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "        for k, v in sample_defaults.items():\n",
    "            kwargs.setdefault(k, v)\n",
    "    else:  # beam\n",
    "        beam_defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3)\n",
    "        for k, v in beam_defaults.items():\n",
    "            kwargs.setdefault(k, v)\n",
    "    if amp_dtype is not None:\n",
    "        with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "            return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "    return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "\n",
    "print(\"[Ready] generate_captions([...], mode='beam')  # BERTScore available in metrics above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Single Image Inference with Metrics (BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE, BERTScore)\n",
    "from typing import Optional, Dict, Any, List\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, torch\n",
    "from PIL import Image\n",
    "\n",
    "_PYCOCO_SCORERS = {}\n",
    "\n",
    "def _lazy_load_scorers():\n",
    "    global _PYCOCO_SCORERS\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.meteor.meteor import Meteor\n",
    "    from pycocoevalcap.rouge.rouge import Rouge\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    try:\n",
    "        from pycocoevalcap.spice.spice import Spice\n",
    "    except Exception:\n",
    "        Spice = None\n",
    "    if 'bleu' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['bleu'] = Bleu(4)\n",
    "    if 'meteor' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['meteor'] = Meteor()\n",
    "    if 'rouge' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['rouge'] = Rouge()\n",
    "    if 'cider' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['cider'] = Cider()\n",
    "    if Spice and 'spice' not in _PYCOCO_SCORERS:\n",
    "        _PYCOCO_SCORERS['spice'] = Spice()\n",
    "    return _PYCOCO_SCORERS\n",
    "\n",
    "def _compute_single_caption_metrics(pred: str, refs: List[str], compute_bertscore: bool = True) -> Dict[str, float]:\n",
    "    refs_clean = [r.strip() for r in refs if r and r.strip()]\n",
    "    if not refs_clean:\n",
    "        return {}\n",
    "    scorers = _lazy_load_scorers()\n",
    "    gts = {0: refs_clean}\n",
    "    res = {0: [pred]}\n",
    "    out = {}\n",
    "    try:\n",
    "        bleu_scores, _ = scorers['bleu'].compute_score(gts, res)\n",
    "        out['bleu1'] = float(bleu_scores[0])\n",
    "        out['bleu2'] = float(bleu_scores[1])\n",
    "        out['bleu3'] = float(bleu_scores[2])\n",
    "        out['bleu4'] = float(bleu_scores[3])\n",
    "        out['bleu']  = float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "    try:\n",
    "        meteor_score, _ = scorers['meteor'].compute_score(gts, res)\n",
    "        out['meteor'] = float(meteor_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] METEOR failed:', e)\n",
    "    try:\n",
    "        rouge_score, _ = scorers['rouge'].compute_score(gts, res)\n",
    "        out['rougeL'] = float(rouge_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] ROUGE-L failed:', e)\n",
    "    try:\n",
    "        cider_score, _ = scorers['cider'].compute_score(gts, res)\n",
    "        out['cider'] = float(cider_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] CIDEr failed:', e)\n",
    "    if 'spice' in scorers:\n",
    "        try:\n",
    "            spice_score, spice_scores = scorers['spice'].compute_score(gts, res)\n",
    "            out['spice'] = float(spice_score)\n",
    "        except Exception as e:\n",
    "            print('[WARN] SPICE failed:', e)\n",
    "    if compute_bertscore:\n",
    "        try:\n",
    "            import evaluate\n",
    "            bertscore = evaluate.load('bertscore')\n",
    "            bs = bertscore.compute(predictions=[pred], references=[refs_clean[0]], lang='en', device=0)\n",
    "            out['bertscore_f1'] = float(bs['f1'][0])\n",
    "        except Exception as e:\n",
    "            print('[WARN] BERTScore failed:', e)\n",
    "    return out\n",
    "\n",
    "def predict(\n",
    "    image_path: str,\n",
    "    prompt: str = \"Bu fotoğrafı açıkla: \",\n",
    "    mode: str = \"beam\",              # 'beam' or 'sample'\n",
    "    max_refs: Optional[int] = 5,\n",
    "    show_image: bool = True,\n",
    "    show_refs: bool = True,\n",
    "    gen_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    json_file: Optional[str] = None,\n",
    "    ban_sentinels: bool = True,\n",
    "    compute_metrics: bool = True,\n",
    "    print_metrics: bool = True,\n",
    "    compute_bertscore: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n",
    "    jf = json_file or json_path\n",
    "    refs: List[str] = []\n",
    "    try:\n",
    "        with open(jf) as f:\n",
    "            data = json.load(f)\n",
    "        entries = data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "        target_name = os.path.basename(image_path)\n",
    "        for e in entries:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            if e.get('filename') == target_name:\n",
    "                for s in e.get('sentences', []):\n",
    "                    if isinstance(s, dict) and 'raw' in s:\n",
    "                        cap = s['raw'].strip()\n",
    "                        if cap:\n",
    "                            refs.append(cap)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse references ({e})\")\n",
    "    if max_refs is not None:\n",
    "        refs = refs[:max_refs]\n",
    "    gen_kwargs = (gen_kwargs or {}).copy()\n",
    "    if mode == \"sample\":\n",
    "        defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "    else:\n",
    "        defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3, num_beams=getattr(cfg, \"num_beams_infer\", 4))\n",
    "    for k, v in defaults.items():\n",
    "        gen_kwargs.setdefault(k, v)\n",
    "    local_amp_dtype = globals().get(\"amp_dtype\", None)\n",
    "    model_mm.eval()\n",
    "    if local_amp_dtype is not None:\n",
    "        with torch.amp.autocast('cuda', dtype=local_amp_dtype):\n",
    "            pred = model_mm.generate(\n",
    "                image_paths=[image_path],\n",
    "                prompt=prompt,\n",
    "                ban_sentinels=ban_sentinels,\n",
    "                **gen_kwargs\n",
    "            )[0]\n",
    "    else:\n",
    "        pred = model_mm.generate(\n",
    "            image_paths=[image_path],\n",
    "            prompt=prompt,\n",
    "            ban_sentinels=ban_sentinels,\n",
    "            **gen_kwargs\n",
    "        )[0]\n",
    "    metrics: Dict[str, float] = {}\n",
    "    if compute_metrics and refs:\n",
    "        try:\n",
    "            metrics = _compute_single_caption_metrics(pred if pred else \"<EMPTY>\", refs, compute_bertscore=compute_bertscore)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Metric computation failed: {e}\")\n",
    "    if show_image:\n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img); plt.axis(\"off\")\n",
    "            plt.title(\"Image\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not display image ({e})\")\n",
    "    if show_refs:\n",
    "        if refs:\n",
    "            print(\"References:\")\n",
    "            for i, r in enumerate(refs, 1):\n",
    "                print(f\"  {i}. {r}\")\n",
    "        else:\n",
    "            print(\"(No references found)\")\n",
    "    print(\"Prediction:\")\n",
    "    print(\" \", pred)\n",
    "    if print_metrics and metrics:\n",
    "        print(\"\\nMetrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"  {k}: {v}\")\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"prediction\": pred,\n",
    "        \"references\": refs,\n",
    "        \"metrics\": metrics,\n",
    "        \"mode\": mode,\n",
    "    }\n",
    "\n",
    "# Example usage (uncomment and set a valid path):\n",
    "# result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Example: Retrain from exported checkpoint on same or new dataset ===\n",
    "# checkpoint_path = '/kaggle/input/your_export/clip_mt5_prefix_latest.pt'\n",
    "# from torch.utils.data import DataLoader\n",
    "# device = torch.device('cuda')\n",
    "# model_re, cfg_re, hparams, epoch_loaded, step_loaded = load_model_for_retrain(\n",
    "#     checkpoint_path,\n",
    "#     device=device,\n",
    "#     new_lr=5e-5,              # optionally override LR\n",
    "#     new_clip_lr_scale=0.02,   # optionally override CLIP LR scale\n",
    "#     freeze_clip=None,         # or True to freeze CLIP now\n",
    "# )\n",
    "# optimizer_re = build_optimizer_from_hparams(model_re, hparams)\n",
    "# steps_per_epoch = len(train_loader)  # after you rebuild train_loader for (same or new) dataset\n",
    "# scheduler_re = build_scheduler_from_hparams(optimizer_re, hparams, steps_per_epoch, total_epochs=5)\n",
    "# # Proceed with standard training loop using model_re / optimizer_re / scheduler_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ==== Resume / Continue Training From Exported or Training Checkpoint (.pt) ====\n",
    "# # Improvements:\n",
    "# #  - Ensures 'checkpoints' directory exists before saving.\n",
    "# #  - Handles tokenizer class mismatch notice (mt5 vs t5) by reloading MT5 tokenizer explicitly.\n",
    "# #  - Initializes best_val robustly (evaluates validation if inf).\n",
    "# #  - Safe scheduler positioning.\n",
    "# #  - Adds optional immediate validation before continuing to set baseline.\n",
    "\n",
    "# RESUME_PATH = '/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt'  # change as needed\n",
    "# EXTRA_EPOCHS = 5          # number of extra epochs to train\n",
    "# RUN_INITIAL_VAL = True    # run a validation pass right after load to set best_val if missing\n",
    "# SAVE_NAME = 'best_resumed.pt'\n",
    "\n",
    "# import os, math, time, torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# if not os.path.isfile(RESUME_PATH):\n",
    "#     print(f\"[RESUME] Path not found: {RESUME_PATH} -> Skip.\")\n",
    "# else:\n",
    "#     print(f\"[RESUME] Loading: {RESUME_PATH}\")\n",
    "#     bundle = torch.load(RESUME_PATH, map_location='cpu')\n",
    "\n",
    "#     has_export_format = 'model_state' in bundle\n",
    "#     has_train_ckpt_format = 'model' in bundle\n",
    "#     if not (has_export_format or has_train_ckpt_format):\n",
    "#         raise ValueError('Unrecognized checkpoint format.')\n",
    "\n",
    "#     cfg_dict = bundle.get('cfg') or {}\n",
    "#     class _Cfg: ...\n",
    "#     cfg_resume = _Cfg()\n",
    "#     for k, v in base_config.items():\n",
    "#         setattr(cfg_resume, k, cfg_dict.get(k, v))\n",
    "\n",
    "#     # Force consistent tokenizer type (MT5Tokenizer) regardless of original class\n",
    "#     model_resume = CLIPmT5Pipeline(cfg_resume).to(device)\n",
    "\n",
    "#     if has_export_format:\n",
    "#         model_resume.load_state_dict(bundle['model_state'], strict=True)\n",
    "#         loaded_epoch = bundle.get('epoch', -1)\n",
    "#         global_step_loaded = bundle.get('global_step', None)\n",
    "#         best_val_loaded = bundle.get('best_val', float('inf'))\n",
    "#     else:\n",
    "#         model_resume.load_state_dict(bundle['model'], strict=True)\n",
    "#         loaded_epoch = bundle.get('epoch', -1)\n",
    "#         global_step_loaded = bundle.get('global_step', None)\n",
    "#         best_val_loaded = bundle.get('best_val', float('inf'))\n",
    "\n",
    "#     print(f\"[RESUME] Weights loaded (epoch={loaded_epoch}, best_val={best_val_loaded})\")\n",
    "\n",
    "#     main_params, clip_params = [], []\n",
    "#     for n, p in model_resume.named_parameters():\n",
    "#         if not p.requires_grad: continue\n",
    "#         (clip_params if n.startswith('clip.') else main_params).append(p)\n",
    "#     param_groups = []\n",
    "#     if main_params: param_groups.append({'params': main_params, 'lr': cfg_resume.lr})\n",
    "#     if clip_params: param_groups.append({'params': clip_params, 'lr': cfg_resume.lr * getattr(cfg_resume, 'clip_lr_scale', 0.05)})\n",
    "#     optimizer_resume = AdamW(param_groups, weight_decay=cfg_resume.weight_decay)\n",
    "\n",
    "#     steps_per_epoch = len(train_loader)\n",
    "#     total_planned_epochs = loaded_epoch + 1 + EXTRA_EPOCHS\n",
    "#     total_steps = steps_per_epoch * total_planned_epochs\n",
    "\n",
    "#     def lr_lambda(step):\n",
    "#         if cfg_resume.warmup_steps > 0 and step < cfg_resume.warmup_steps:\n",
    "#             return float(step) / float(max(1, cfg_resume.warmup_steps))\n",
    "#         progress = (step - cfg_resume.warmup_steps) / float(max(1, total_steps - cfg_resume.warmup_steps))\n",
    "#         progress = min(max(progress, 0.0), 1.0)\n",
    "#         return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "#     scheduler_resume = LambdaLR(optimizer_resume, lr_lambda=lr_lambda)\n",
    "\n",
    "#     opt_key = 'optimizer_state' if has_export_format else 'optimizer'\n",
    "#     sch_key = 'scheduler_state' if has_export_format else 'scheduler'\n",
    "#     if opt_key in bundle:\n",
    "#         try:\n",
    "#             optimizer_resume.load_state_dict(bundle[opt_key])\n",
    "#             print('[RESUME] Optimizer state restored.')\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] Optimizer state load failed:', e)\n",
    "#     if sch_key in bundle:\n",
    "#         try:\n",
    "#             scheduler_resume.load_state_dict(bundle[sch_key])\n",
    "#             print('[RESUME] Scheduler state restored.')\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] Scheduler state load failed:', e)\n",
    "\n",
    "#     if scheduler_resume.last_epoch < 0 and loaded_epoch >= 0:\n",
    "#         completed_steps = (loaded_epoch + 1) * steps_per_epoch\n",
    "#         scheduler_resume.last_epoch = completed_steps\n",
    "#         print(f\"[RESUME] Scheduler last_epoch set to {scheduler_resume.last_epoch}\")\n",
    "\n",
    "#     amp_dtype_resume = None\n",
    "#     if getattr(cfg_resume, 'use_amp', True):\n",
    "#         if getattr(cfg_resume, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "#             amp_dtype_resume = torch.bfloat16\n",
    "#         else:\n",
    "#             amp_dtype_resume = torch.float16\n",
    "#     scaler_resume = torch.amp.GradScaler('cuda', enabled=amp_dtype_resume is not None)\n",
    "\n",
    "#     start_epoch = loaded_epoch + 1\n",
    "#     end_epoch = loaded_epoch + EXTRA_EPOCHS\n",
    "\n",
    "#     best_val = best_val_loaded\n",
    "\n",
    "#     # Optional immediate validation to set best_val if it's inf or user wants baseline\n",
    "#     if RUN_INITIAL_VAL and (best_val == float('inf') or best_val != best_val):  # inf or NaN\n",
    "#         if val_loader:\n",
    "#             model_resume.eval(); v=0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in val_loader:\n",
    "#                     imgs, srcs, tgts = batch\n",
    "#                     imgs = imgs.to(device, non_blocking=True)\n",
    "#                     if amp_dtype_resume is not None:\n",
    "#                         with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                             out = model_resume(imgs, srcs, tgts)\n",
    "#                     else:\n",
    "#                         out = model_resume(imgs, srcs, tgts)\n",
    "#                     v += out.loss.item()\n",
    "#             best_val = v / max(1, len(val_loader))\n",
    "#             print(f\"[RESUME] Initial validation baseline best_val set to {best_val:.4f}\")\n",
    "#         else:\n",
    "#             best_val = float('inf')\n",
    "\n",
    "#     early_patience = getattr(cfg_resume, 'early_stop_patience', None)\n",
    "#     min_delta = getattr(cfg_resume, 'early_stop_min_delta', 0.0)\n",
    "#     _epochs_no_improve = 0\n",
    "\n",
    "#     global_step_resume = global_step_loaded if global_step_loaded is not None else (start_epoch * steps_per_epoch)\n",
    "\n",
    "#     print(f\"[RESUME] Continue training for {EXTRA_EPOCHS} more epochs: {start_epoch} -> {end_epoch}\")\n",
    "\n",
    "#     for epoch in range(start_epoch, end_epoch + 1):\n",
    "#         model_resume.train()\n",
    "#         sum_loss = 0.0\n",
    "#         t0 = time.time()\n",
    "#         for step, batch in enumerate(train_loader, start=1):\n",
    "#             imgs, srcs, tgts = batch\n",
    "#             imgs = imgs.to(device, non_blocking=True)\n",
    "#             optimizer_resume.zero_grad(set_to_none=True)\n",
    "#             if amp_dtype_resume is not None:\n",
    "#                 with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                     out = model_resume(imgs, srcs, tgts)\n",
    "#                     loss = out.loss\n",
    "#                 scaler_resume.scale(loss).backward()\n",
    "#                 if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n",
    "#                     scaler_resume.unscale_(optimizer_resume)\n",
    "#                     torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n",
    "#                 scaler_resume.step(optimizer_resume)\n",
    "#                 scaler_resume.update()\n",
    "#             else:\n",
    "#                 out = model_resume(imgs, srcs, tgts); loss = out.loss\n",
    "#                 loss.backward()\n",
    "#                 if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n",
    "#                     torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n",
    "#                 optimizer_resume.step()\n",
    "#             scheduler_resume.step()\n",
    "#             sum_loss += loss.item()\n",
    "#             global_step_resume += 1\n",
    "#         train_loss = sum_loss / max(1, len(train_loader))\n",
    "\n",
    "#         val_loss = None\n",
    "#         if val_loader:\n",
    "#             model_resume.eval(); v = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in val_loader:\n",
    "#                     imgs, srcs, tgts = batch\n",
    "#                     imgs = imgs.to(device, non_blocking=True)\n",
    "#                     if amp_dtype_resume is not None:\n",
    "#                         with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                             out = model_resume(imgs, srcs, tgts)\n",
    "                        \n",
    "#                     else:\n",
    "#                         out = model_resume(imgs, srcs, tgts)\n",
    "#                     v += out.loss.item()\n",
    "#             val_loss = v / max(1, len(val_loader))\n",
    "\n",
    "#         dt = time.time() - t0\n",
    "#         lr_cur = scheduler_resume.get_last_lr()[0]\n",
    "#         if val_loss is not None:\n",
    "#             print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} val={val_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n",
    "#         else:\n",
    "#             print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n",
    "\n",
    "#         metric = val_loss if val_loss is not None else train_loss\n",
    "#         improved = metric < (best_val - min_delta)\n",
    "#         if improved:\n",
    "#             best_val = metric\n",
    "#             _epochs_no_improve = 0\n",
    "#             save_obj = {\n",
    "#                 'model': model_resume.state_dict(),\n",
    "#                 'cfg': {k: getattr(cfg_resume, k) for k in base_config.keys()},\n",
    "#                 'epoch': epoch,\n",
    "#                 'optimizer': optimizer_resume.state_dict(),\n",
    "#                 'scheduler': scheduler_resume.state_dict(),\n",
    "#                 'best_val': best_val,\n",
    "#                 'global_step': global_step_resume,\n",
    "#             }\n",
    "#             torch.save(save_obj, os.path.join('checkpoints', SAVE_NAME))\n",
    "#             print(f\"   -> [RESUME] Saved {SAVE_NAME} (metric={best_val:.4f})\")\n",
    "#         else:\n",
    "#             _epochs_no_improve += 1\n",
    "#             if early_patience is not None and _epochs_no_improve >= early_patience:\n",
    "#                 print(f\"[Early Stop - Resume] No improvement for {early_patience} epochs.\")\n",
    "#                 break\n",
    "\n",
    "#     print('[RESUME] Training extension finished. Final best_val=', best_val)\n",
    "#     model_mm = model_resume"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2808179,
     "sourceId": 4845244,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8193626,
     "sourceId": 12947536,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8200149,
     "sourceId": 12956829,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8240491,
     "sourceId": 13015764,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 442058,
     "modelInstanceId": 424569,
     "sourceId": 559988,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 444390,
     "modelInstanceId": 427385,
     "sourceId": 567111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
