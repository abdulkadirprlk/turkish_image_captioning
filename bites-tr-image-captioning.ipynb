{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179},{"sourceId":12947536,"sourceType":"datasetVersion","datasetId":8193626},{"sourceId":12956829,"sourceType":"datasetVersion","datasetId":8200149},{"sourceId":12992543,"sourceType":"datasetVersion","datasetId":8223778},{"sourceId":559988,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":424569,"modelId":442058},{"sourceId":567111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":427385,"modelId":444390}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CLIP (ViT-B/32) + Projection MLP + mT5-Small Decoder Pipeline\n\nBu bölüm: \n- CLIP ViT-L/14 image encoder (tamamen freeze, edit: unfreeze)\n- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n\nStrateji (prefix approach):\n1. Image -> CLIP encode_image -> (B,512)\n2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n\nSeçilebilir dondurma opsiyonları:\n- freeze_clip = True (zorunlu senaryon)\n- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n\nAşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n","metadata":{}},{"cell_type":"code","source":"# Rerun this cell at each session start\n\n# Uninstall conflicting packages (Kaggle specific)\n!pip uninstall -y bigframes cesium gcsfs\n\n# Performance metrics\n!pip install -r /kaggle/input/requirements8/requirements.txt\n\n# To use nltk\nimport nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\n\n# Download mT5-small ViT-B/32\nimport clip, torch\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"google/mt5-small\"\n\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmt5 = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\nclip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n\nprint(\"Setup Done!\")","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:06:05.697558Z","iopub.execute_input":"2025-09-08T12:06:05.697804Z","iopub.status.idle":"2025-09-08T12:08:20.707973Z","shell.execute_reply.started":"2025-09-08T12:06:05.697782Z","shell.execute_reply":"2025-09-08T12:08:20.707154Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: bigframes 2.8.0\nUninstalling bigframes-2.8.0:\n  Successfully uninstalled bigframes-2.8.0\nFound existing installation: cesium 0.12.4\nUninstalling cesium-0.12.4:\n  Successfully uninstalled cesium-0.12.4\nFound existing installation: gcsfs 2025.3.2\nUninstalling gcsfs-2025.3.2:\n  Successfully uninstalled gcsfs-2025.3.2\nCollecting git+https://github.com/openai/CLIP.git (from -r /kaggle/input/requirements8/requirements.txt (line 20))\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-61x507zk\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-61x507zk\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting pycocoevalcap@ git+https://github.com/salaniz/pycocoevalcap.git (from -r /kaggle/input/requirements8/requirements.txt (line 23))\n  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-install-v6rhzyw8/pycocoevalcap_59372026b1f64c0297d6ab96a17ab7df\n  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap.git /tmp/pip-install-v6rhzyw8/pycocoevalcap_59372026b1f64c0297d6ab96a17ab7df\n  Resolved https://github.com/salaniz/pycocoevalcap.git to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 2)) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 3)) (0.21.0+cu124)\nRequirement already satisfied: accelerate>=0.29.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 4)) (1.8.1)\nRequirement already satisfied: transformers==4.52.4 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 7)) (4.52.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 8)) (0.33.1)\nRequirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 9)) (0.2.0)\nRequirement already satisfied: pillow>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 12)) (11.2.1)\nRequirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 13)) (3.7.2)\nRequirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 16)) (3.9.1)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 17)) (1.26.4)\nRequirement already satisfied: safetensors>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 26)) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (3.18.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (0.21.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0->-r /kaggle/input/requirements8/requirements.txt (line 4)) (7.0.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.32.0->-r /kaggle/input/requirements8/requirements.txt (line 8)) (1.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (2.9.0.post0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->-r /kaggle/input/requirements8/requirements.txt (line 16)) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->-r /kaggle/input/requirements8/requirements.txt (line 16)) (1.5.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2.4.1)\nCollecting ftfy (from clip==1.0->-r /kaggle/input/requirements8/requirements.txt (line 20))\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap@ git+https://github.com/salaniz/pycocoevalcap.git->-r /kaggle/input/requirements8/requirements.txt (line 23)) (2.0.10)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (1.17.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0->-r /kaggle/input/requirements8/requirements.txt (line 20)) (0.2.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip, pycocoevalcap\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=20321f9fa6839223e193cea593335951c85526bbc64b4a96aa14bd5ba525276c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kkbjpz3l/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=eb0f6bff64af7ac37f4bcb20b83d6b3acd3faedc8dbf026e2dc4ae48e51d82c9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kkbjpz3l/wheels/e5/d1/50/82763a91172a5c8058c9efff8692f3a41570e3ddd5b5b2c4b4\nSuccessfully built clip pycocoevalcap\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pycocoevalcap, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pycocoevalcap-1.2\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n2025-09-08 12:07:47.910379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757333268.076287      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757333268.132819      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26213ed572744aed9f7b16588d5f4e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abdd24fb4604219bb33aa83d4d0908c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d458271f4efd40f481390f1514b06ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da08e3b23ec54ddd8e55ab77adbdcb96"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'T5Tokenizer'. \nThe class this function is called from is 'MT5Tokenizer'.\nYou are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd1f5c7f23a43618ade95596eb04662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358420bbc4464877aafb269d147396c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d0a5739d3a4396bdec3e5462c01703"}},"metadata":{}},{"name":"stderr","text":"\n  0%|                                               | 0.00/338M [00:00<?, ?iB/s]\u001b[A\n  2%|▋                                     | 6.00M/338M [00:00<00:05, 61.7MiB/s]\u001b[A\n  4%|█▎                                    | 12.2M/338M [00:00<00:05, 62.7MiB/s]\u001b[A\n  5%|██                                    | 18.3M/338M [00:00<00:05, 63.6MiB/s]\u001b[A\n  7%|██▋                                   | 24.4M/338M [00:00<00:05, 56.6MiB/s]\u001b[A\n 10%|███▋                                  | 32.9M/338M [00:00<00:04, 67.2MiB/s]\u001b[A\n 13%|████▊                                 | 42.5M/338M [00:00<00:03, 78.2MiB/s]\u001b[A\n 16%|█████▉                                | 52.7M/338M [00:00<00:03, 87.1MiB/s]\u001b[A\n 19%|███████                               | 62.7M/338M [00:00<00:03, 92.5MiB/s]\u001b[A\n 21%|████████                              | 71.6M/338M [00:00<00:03, 89.4MiB/s]\u001b[A\n 24%|█████████                             | 80.6M/338M [00:01<00:02, 90.8MiB/s]\u001b[A\n 26%|██████████                            | 89.3M/338M [00:01<00:02, 89.2MiB/s]\u001b[A\n 29%|███████████▏                          | 99.1M/338M [00:01<00:02, 93.0MiB/s]\u001b[A\n 32%|████████████▌                          | 108M/338M [00:01<00:02, 94.1MiB/s]\u001b[A\n 35%|█████████████▌                         | 117M/338M [00:01<00:02, 81.9MiB/s]\u001b[A\n 37%|██████████████▍                        | 125M/338M [00:01<00:02, 75.6MiB/s]\u001b[A\n 40%|███████████████▌                       | 134M/338M [00:01<00:02, 80.7MiB/s]\u001b[A\n 43%|████████████████▋                      | 144M/338M [00:01<00:02, 86.1MiB/s]\u001b[A\n 46%|██████████████████                     | 156M/338M [00:01<00:01, 97.2MiB/s]\u001b[A\n 49%|███████████████████▏                   | 166M/338M [00:02<00:01, 93.7MiB/s]\u001b[A\n 52%|████████████████████▎                  | 176M/338M [00:02<00:01, 97.4MiB/s]\u001b[A\n 55%|█████████████████████▍                 | 185M/338M [00:02<00:01, 91.5MiB/s]\u001b[A\n 58%|██████████████████████▍                | 194M/338M [00:02<00:01, 92.8MiB/s]\u001b[A\n 60%|███████████████████████▌               | 203M/338M [00:02<00:01, 92.3MiB/s]\u001b[A\n 63%|████████████████████████▌              | 213M/338M [00:02<00:01, 93.1MiB/s]\u001b[A\n 67%|██████████████████████████▋             | 225M/338M [00:02<00:01, 105MiB/s]\u001b[A\n 70%|████████████████████████████            | 237M/338M [00:02<00:00, 109MiB/s]\u001b[A\n 73%|█████████████████████████████▎          | 248M/338M [00:02<00:00, 111MiB/s]\u001b[A\n 77%|██████████████████████████████▊         | 260M/338M [00:02<00:00, 116MiB/s]\u001b[A\n 80%|████████████████████████████████▏       | 271M/338M [00:03<00:00, 117MiB/s]\u001b[A\n 84%|█████████████████████████████████▍      | 283M/338M [00:03<00:00, 115MiB/s]\u001b[A\n 87%|██████████████████████████████████▊     | 294M/338M [00:03<00:00, 112MiB/s]\u001b[A\n 90%|████████████████████████████████████    | 305M/338M [00:03<00:00, 113MiB/s]\u001b[A\n 94%|█████████████████████████████████████▍  | 316M/338M [00:03<00:00, 116MiB/s]\u001b[A\n100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 95.7MiB/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Setup Done!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Unified configuration + optional wandb init\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\nPROJECT_NAME = \"bites-tr-image-captioning\"\nRUN_NAME = \"clip_mt5_prefix_run\"\n\nENABLE_WANDB = True\n\nWANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n\nbase_config = {\n    \"model\": \"google/mt5-small\",\n    \"clip_encoder\": \"ViT-B/32\",  # backbone\n    \"prefix_tokens\": 32,          # stronger conditioning\n    \"batch_size\": 16,\n    \"lr\": 1e-4,\n    \"epochs\": 25,                  # allow a bit longer now that we fine-tune CLIP\n    \"dataset_limit\": None,\n    # --- CLIP fine-tuning controls ---\n    \"freeze_clip\": False,          # set False to allow full CLIP fine-tuning\n    \"unfreeze_clip_last_n\": 0,     # if >0 unfreezes only last N vision blocks (overrides freeze_clip); 0 means ignore and use freeze_clip flag\n    \"clip_lr_scale\": 0.05,         # scaled LR for ALL CLIP params (set lower than main to avoid destroying pretrained space)\n    \"use_clip_patch_tokens\": True, # richer conditioning (patch tokens path)\n    # --- T5 freezing ---\n    \"freeze_t5_encoder\": False,    # unfreeze encoder so it can adapt to visual prefix distribution\n    \"freeze_t5_decoder\": False,\n    # --- Optimization ---\n    \"seed\": 42,\n    \"weight_decay\": 0.01,\n    \"grad_clip\": 1.0,\n    \"warmup_steps\": 500,           # linear warmup steps before cosine decay\n    # --- Inference defaults ---\n    \"num_beams_infer\": 4,\n    \"max_new_tokens_infer\": 32,\n    \"src_max_len\": 64,\n    \"tgt_max_len\": 64,\n    # --- Early stopping hyperparameters ---\n    \"early_stop_patience\": 4,\n    \"early_stop_min_delta\": 0.01,\n    \n    \"use_amp\": True,              # force AMP on CUDA for speed\n    # Optional extras:\n    \"use_bf16\": True,\n    \"enable_tf32\": True,\n    \"finite_loss_skip\": True,\n    \"save_every\": 0\n}\n\nuse_wandb = False\ncfg = None\nif ENABLE_WANDB:\n    try:\n        import wandb\n        wandb.login(key=WANDB_API_KEY)\n        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config)\n        cfg = wandb.config\n        use_wandb = True\n        print(\"[wandb] run initialized\")\n    except Exception as e:\n        print(\"[wandb] disabled (init failed):\", e)\n\nif cfg is None:\n    class _Cfg: pass\n    cfg = _Cfg()\n    for k, v in base_config.items():\n        setattr(cfg, k, v)\n    print(\"[INFO] Using local cfg (wandb off)\")\n\nimport random, numpy as np, torch\nrandom.seed(cfg.seed)\nnp.random.seed(cfg.seed)\ntorch.manual_seed(cfg.seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(cfg.seed)\n\n# --- Enforce CUDA-only environment ---\nassert torch.cuda.is_available(), \"CUDA GPU is required but not detected. Please run in a CUDA-enabled environment.\"\ndevice = torch.device('cuda')\nprint(f\"[DEVICE] Using CUDA device: {torch.cuda.get_device_name(device)}\")\n\n# Performance toggles\nif getattr(cfg, 'enable_tf32', False):\n    try:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        print(\"[DEVICE] TF32 enabled.\")\n    except Exception as _e:\n        print(\"[WARN] Could not enable TF32:\", _e)\n\nif getattr(cfg, 'use_bf16', False):\n    bf16_ok = torch.cuda.is_bf16_supported()\n    print(f\"[DEVICE] bfloat16 support: {bf16_ok}\")\n\nprint(\"Active config:\")\nfor k, v in base_config.items():\n    print(f\"  {k}: {getattr(cfg, k)}\")","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:08:27.007730Z","iopub.execute_input":"2025-09-08T12:08:27.008755Z","iopub.status.idle":"2025-09-08T12:08:44.402506Z","shell.execute_reply.started":"2025-09-08T12:08:27.008725Z","shell.execute_reply":"2025-09-08T12:08:44.401913Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulkadirparlak\u001b[0m (\u001b[33mabdulkadirparlak-hacettepe-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250908_120836-h1yg4igm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/h1yg4igm' target=\"_blank\">clip_mt5_prefix_run</a></strong> to <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/h1yg4igm' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/h1yg4igm</a>"},"metadata":{}},{"name":"stdout","text":"[wandb] run initialized\n[DEVICE] Using CUDA device: Tesla P100-PCIE-16GB\n[DEVICE] TF32 enabled.\n[DEVICE] bfloat16 support: True\nActive config:\n  model: google/mt5-small\n  clip_encoder: ViT-B/32\n  prefix_tokens: 32\n  batch_size: 16\n  lr: 0.0001\n  epochs: 25\n  dataset_limit: None\n  freeze_clip: False\n  unfreeze_clip_last_n: 0\n  clip_lr_scale: 0.05\n  use_clip_patch_tokens: True\n  freeze_t5_encoder: False\n  freeze_t5_decoder: False\n  seed: 42\n  weight_decay: 0.01\n  grad_clip: 1\n  warmup_steps: 500\n  num_beams_infer: 4\n  max_new_tokens_infer: 32\n  src_max_len: 64\n  tgt_max_len: 64\n  early_stop_patience: 4\n  early_stop_min_delta: 0.01\n  use_amp: True\n  use_bf16: True\n  enable_tf32: True\n  finite_loss_skip: True\n  save_every: 0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Model, dataset and pipeline definitions (data + model init only)\nimport torch, os, json\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport clip\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\n# (CUDA enforcement handled in config cell; assume cuda device later)\n\nclass ProjectionMLP(nn.Module):\n    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n        super().__init__()\n        self.prefix_tokens = prefix_tokens\n        self.fc1 = nn.Linear(in_dim, hidden)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n        self.ln = nn.LayerNorm(out_dim)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n        x = x.view(x.size(0), self.prefix_tokens, -1)\n        x = self.ln(x)\n        return self.dropout(x)\n\nclass CLIPmT5Pipeline(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.tokenizer = MT5Tokenizer.from_pretrained(cfg.model)\n        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n        # CLIP freezing / unfreezing strategy\n        if cfg.unfreeze_clip_last_n and cfg.unfreeze_clip_last_n > 0:\n            # Freeze everything first\n            for p in self.clip.parameters():\n                p.requires_grad = False\n            blocks = list(self.clip.visual.transformer.resblocks)\n            for block in blocks[-cfg.unfreeze_clip_last_n:]:\n                for p in block.parameters():\n                    p.requires_grad = True\n        else:\n            # Respect freeze_clip flag (False means fully trainable)\n            for p in self.clip.parameters():\n                p.requires_grad = not cfg.freeze_clip\n        # T5 freeze toggles\n        if cfg.freeze_t5_encoder:\n            for p in self.model.encoder.parameters():\n                p.requires_grad = False\n        if cfg.freeze_t5_decoder:\n            for p in self.model.decoder.parameters():\n                p.requires_grad = False\n        self.prefix_tokens = cfg.prefix_tokens\n        # Determine embedding dim dynamically (CLIP projection output)\n        with torch.no_grad():\n            dummy = torch.randn(1, 3, 224, 224)\n            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n        self._cached_sentinel_ids = None  # lazy cache\n\n    def _encode_image_single(self, images: torch.Tensor):\n        # (B, D) pooled embedding (already passed through ln_post + proj inside encode_image)\n        pooled = self.clip.encode_image(images)\n        return pooled\n\n    def _encode_image_patch_tokens(self, images: torch.Tensor):\n        \"\"\"Return patch-averaged embedding projected into CLIP joint space.\n        We manually replicate encode_image path but average patch tokens (excluding CLS),\n        then apply ln_post and proj so final dim == visual.output_dim (e.g. 512) to match ProjectionMLP in_dim.\"\"\"\n        visual = self.clip.visual\n        x = visual.conv1(images)                      # (B, width, grid, grid)\n        x = x.reshape(x.shape[0], x.shape[1], -1)     # (B, width, patches)\n        x = x.permute(0, 2, 1)                        # (B, patches, width)\n        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n        x = torch.cat([cls_tokens, x], dim=1)         # prepend CLS\n        x = x + visual.positional_embedding.to(x.dtype)\n        x = visual.ln_pre(x)\n        x = x.permute(1, 0, 2)                        # (sequence, B, width)\n        for block in visual.transformer.resblocks:\n            x = block(x)\n        x = x.permute(1, 0, 2)                        # (B, sequence, width)\n        patches = x[:, 1:, :]                         # drop CLS\n        pooled = patches.mean(dim=1)                  # (B, width)\n        if hasattr(visual, 'ln_post'):\n            pooled = visual.ln_post(pooled)\n        if hasattr(visual, 'proj') and visual.proj is not None:\n            pooled = pooled @ visual.proj             # (B, output_dim)\n        return pooled\n\n    def forward(self, images, src_texts, tgt_texts):\n        device = next(self.parameters()).device\n        images = images.to(device)\n        clip_emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n        prefix_emb = self.proj(clip_emb)\n        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n        full_attn = torch.cat([\n            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n            tok_src.attention_mask\n        ], dim=1)\n        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n\n    def _prepare_prefix(self, images: torch.Tensor):\n        images = images.to(next(self.parameters()).device)\n        emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n        return self.proj(emb)\n\n    def _get_sentinel_bad_words(self, n=50):\n        if self._cached_sentinel_ids is None:\n            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n            self._cached_sentinel_ids = [[i] for i in ids]\n        return self._cached_sentinel_ids\n\n    @torch.inference_mode()\n    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu görüntüyü açıkla: \", ban_sentinels=True, **gen_kwargs):\n        device = next(self.parameters()).device\n        num_beams = num_beams or self.cfg.num_beams_infer\n        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n        if images is None:\n            assert image_paths is not None, \"Provide image_paths or images tensor\"\n            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]\n            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n            images = torch.stack([preprocess(im) for im in pil_images])\n        images = images.to(device)\n        prefix_tokens = self._prepare_prefix(images)\n        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n        full_attn = torch.cat([\n            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n            tok.attention_mask\n        ], dim=1)\n        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n        gen_ids = self.model.generate(\n            inputs_embeds=full_emb,\n            attention_mask=full_attn,\n            num_beams=num_beams,\n            max_new_tokens=max_new_tokens,\n            bad_words_ids=bad_words_ids,\n            **gen_kwargs\n        )\n        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n        return [c if c else \"<EMPTY>\" for c in captions]\n\nclass Flickr8kCaptions(Dataset):\n    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n        self.images_root = images_root\n        raw = json.load(open(json_path))\n        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n        self.samples = []\n        for row in rows:\n            if not isinstance(row, dict): continue\n            if split and row.get('split') != split: continue\n            img = row.get('filename') or row.get('image') or row.get('img')\n            sentences = row.get('sentences')\n            if not img or not sentences: continue\n            for s in sentences:\n                if isinstance(s, dict) and 'raw' in s:\n                    self.samples.append((img, s['raw']))\n        if limit: self.samples = self.samples[:limit]\n        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        img_name, cap = self.samples[idx]\n        path = os.path.join(self.images_root, img_name)\n        image = Image.open(path).convert('RGB')\n        return self.transform(image), \" \", cap","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:08:51.524210Z","iopub.execute_input":"2025-09-08T12:08:51.524538Z","iopub.status.idle":"2025-09-08T12:08:51.549159Z","shell.execute_reply.started":"2025-09-08T12:08:51.524513Z","shell.execute_reply":"2025-09-08T12:08:51.548453Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Data Loading\njson_path = '/kaggle/input/tasviret/flickr8k/tasviret8k_captions.json'\nimages_root = '/kaggle/input/tasviret/flickr8k/Images'\ntrain_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\nval_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=None)\ntest_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=None)\n\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\ntest_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n\nmodel_mm = CLIPmT5Pipeline(cfg)\nprint(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\nprint(\"Trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:08:56.937627Z","iopub.execute_input":"2025-09-08T12:08:56.937901Z","iopub.status.idle":"2025-09-08T12:09:18.408971Z","shell.execute_reply.started":"2025-09-08T12:08:56.937881Z","shell.execute_reply":"2025-09-08T12:09:18.408360Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'T5Tokenizer'. \nThe class this function is called from is 'MT5Tokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"Train samples: 12028  Val: 2006  Test: 2003\nTrainable params: 468774017\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Training cell (only training + validation)\nimport math, time, torch, warnings, os\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nwarnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n\n# CUDA-only enforcement (already asserted earlier, but double-check for isolation run)\nassert torch.cuda.is_available(), \"CUDA GPU required.\"\ndevice = torch.device('cuda')\nmodel_mm.to(device)\n\n# Decide AMP dtype\namp_dtype = None\nif getattr(cfg, 'use_amp', True):\n    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n        amp_dtype = torch.bfloat16\n        print(\"[AMP] Using bfloat16 mixed precision\")\n    else:\n        amp_dtype = torch.float16\n        print(\"[AMP] Using float16 mixed precision\")\nelse:\n    print(\"[AMP] Disabled; using full float32\")\n\n# Parameter groups: separate CLIP vs non-CLIP for LR scaling\nmain_params = []\nclip_params = []\nfor name, p in model_mm.named_parameters():\n    if not p.requires_grad:\n        continue\n    if name.startswith('clip.'):\n        clip_params.append(p)\n    else:\n        main_params.append(p)\nparam_groups = []\nif main_params:\n    param_groups.append({\"params\": main_params, \"lr\": cfg.lr})\nif clip_params:\n    scaled_lr = cfg.lr * getattr(cfg, 'clip_lr_scale', 0.05)\n    param_groups.append({\"params\": clip_params, \"lr\": scaled_lr})\n    print(f\"[INFO] CLIP fine-tune params: {len(clip_params)} with lr={scaled_lr:.2e}\")\n\noptimizer = AdamW(param_groups, weight_decay=cfg.weight_decay)\n\nsteps_per_epoch = len(train_loader)\ntotal_steps = steps_per_epoch * cfg.epochs\n\ndef lr_lambda(step):\n    # linear warmup then cosine decay\n    if cfg.warmup_steps > 0 and step < cfg.warmup_steps:\n        return float(step) / float(max(1, cfg.warmup_steps))\n    progress = (step - cfg.warmup_steps) / float(max(1, total_steps - cfg.warmup_steps))\n    progress = min(max(progress, 0.0), 1.0)\n    return 0.5 * (1 + math.cos(math.pi * progress))\n\nscheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\nscaler = torch.amp.GradScaler('cuda', enabled=amp_dtype is not None)\nbest_val = float('inf')\nCKPT_DIR = 'checkpoints'; os.makedirs(CKPT_DIR, exist_ok=True)\n\n# Early stopping state\nearly_patience = getattr(cfg, 'early_stop_patience', None)\nmin_delta = getattr(cfg, 'early_stop_min_delta', 0.0)\n_epochs_no_improve = 0\nstopped_early = False\n\nmonitor_history = []  # (epoch, metric)\n\nglobal_step = 0\nfor epoch in range(cfg.epochs):\n    model_mm.train()\n    train_sum = 0.0\n    t0 = time.time()\n    for step, batch in enumerate(train_loader, start=1):\n        imgs, srcs, tgts = batch\n        imgs = imgs.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n\n        if amp_dtype is not None:\n            with torch.amp.autocast('cuda', dtype=amp_dtype):\n                out = model_mm(imgs, srcs, tgts)\n                loss = out.loss\n            scaler.scale(loss).backward()\n            if cfg.grad_clip and cfg.grad_clip > 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            out = model_mm(imgs, srcs, tgts); loss = out.loss\n            loss.backward()\n            if cfg.grad_clip and cfg.grad_clip > 0:\n                torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n            optimizer.step()\n\n        scheduler.step()\n        train_sum += loss.item()\n        global_step += 1\n    train_epoch_loss = train_sum / max(1, len(train_loader))\n\n    val_epoch_loss = None\n    if val_loader:\n        model_mm.eval(); v = 0.0\n        with torch.no_grad():\n            for batch in val_loader:\n                imgs, srcs, tgts = batch\n                imgs = imgs.to(device, non_blocking=True)\n                if amp_dtype is not None:\n                    with torch.amp.autocast('cuda', dtype=amp_dtype):\n                        out = model_mm(imgs, srcs, tgts)\n                else:\n                    out = model_mm(imgs, srcs, tgts)\n                v += out.loss.item()\n        val_epoch_loss = v / max(1, len(val_loader))\n\n    dt = time.time() - t0\n    current_lr = scheduler.get_last_lr()[0]\n    if val_epoch_loss is not None:\n        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} val_loss={val_epoch_loss:.4f} time={dt:.1f}s lr={current_lr:.2e}\", flush=True)\n        if use_wandb:\n            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"val/epoch_loss\": val_epoch_loss, \"lr\": current_lr}, step=epoch)\n    else:\n        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} time={dt:.1f}s lr={current_lr:.2e}\", flush=True)\n        if use_wandb:\n            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"lr\": current_lr}, step=epoch)\n\n    metric = val_epoch_loss if val_epoch_loss is not None else train_epoch_loss\n    monitor_history.append((epoch, metric))\n\n    improved = metric < (best_val - min_delta)\n    if improved:\n        best_val = metric\n        _epochs_no_improve = 0\n        torch.save({\n            'model': model_mm.state_dict(),\n            'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n            'epoch': epoch,\n            'optimizer': optimizer.state_dict(),\n            'scheduler': scheduler.state_dict(),\n            'best_val': best_val,\n        }, os.path.join(CKPT_DIR, 'best.pt'))\n        print(f\"  -> Saved checkpoint (metric {best_val:.4f})\")\n    else:\n        _epochs_no_improve += 1\n\n    if early_patience is not None and _epochs_no_improve >= early_patience:\n        print(f\"[Early Stop] No improvement (>{min_delta} delta) for {early_patience} consecutive epochs. Stopping at epoch {epoch+1}.\")\n        stopped_early = True\n        break\n\nprint(\"Training finished. Best metric:\", best_val, \"(early stop)\" if stopped_early else \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T06:37:41.228958Z","iopub.execute_input":"2025-09-08T06:37:41.229247Z","iopub.status.idle":"2025-09-08T09:01:20.628646Z","shell.execute_reply.started":"2025-09-08T06:37:41.229225Z","shell.execute_reply":"2025-09-08T09:01:20.628012Z"}},"outputs":[{"name":"stdout","text":"[AMP] Using bfloat16 mixed precision\n[INFO] CLIP fine-tune params: 302 with lr=5.00e-06\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25 train_loss=17.2407 val_loss=1.9684 time=423.6s lr=1.00e-04\n  -> Saved checkpoint (metric 1.9684)\nEpoch 2/25 train_loss=2.0191 val_loss=1.6931 time=385.3s lr=9.93e-05\n  -> Saved checkpoint (metric 1.6931)\nEpoch 3/25 train_loss=1.8182 val_loss=1.6248 time=393.7s lr=9.77e-05\n  -> Saved checkpoint (metric 1.6248)\nEpoch 4/25 train_loss=1.7422 val_loss=1.5870 time=384.6s lr=9.54e-05\n  -> Saved checkpoint (metric 1.5870)\nEpoch 5/25 train_loss=1.6813 val_loss=1.5650 time=384.4s lr=9.24e-05\n  -> Saved checkpoint (metric 1.5650)\nEpoch 6/25 train_loss=1.6351 val_loss=1.5442 time=382.9s lr=8.86e-05\n  -> Saved checkpoint (metric 1.5442)\nEpoch 7/25 train_loss=1.5986 val_loss=1.5256 time=381.9s lr=8.42e-05\n  -> Saved checkpoint (metric 1.5256)\nEpoch 8/25 train_loss=1.5640 val_loss=1.5167 time=380.4s lr=7.92e-05\nEpoch 9/25 train_loss=1.5397 val_loss=1.5039 time=383.6s lr=7.37e-05\n  -> Saved checkpoint (metric 1.5039)\nEpoch 10/25 train_loss=1.5208 val_loss=1.4943 time=384.0s lr=6.79e-05\nEpoch 11/25 train_loss=1.4960 val_loss=1.4853 time=381.8s lr=6.17e-05\n  -> Saved checkpoint (metric 1.4853)\nEpoch 12/25 train_loss=1.4767 val_loss=1.4811 time=381.9s lr=5.54e-05\nEpoch 13/25 train_loss=1.4737 val_loss=1.4751 time=383.7s lr=4.89e-05\n  -> Saved checkpoint (metric 1.4751)\nEpoch 14/25 train_loss=1.4555 val_loss=1.4713 time=386.8s lr=4.25e-05\nEpoch 15/25 train_loss=1.4466 val_loss=1.4660 time=381.3s lr=3.62e-05\nEpoch 16/25 train_loss=1.4327 val_loss=1.4624 time=381.0s lr=3.01e-05\n  -> Saved checkpoint (metric 1.4624)\nEpoch 17/25 train_loss=1.4303 val_loss=1.4591 time=382.3s lr=2.44e-05\nEpoch 18/25 train_loss=1.4066 val_loss=1.4522 time=383.2s lr=1.91e-05\n  -> Saved checkpoint (metric 1.4522)\nEpoch 19/25 train_loss=1.4069 val_loss=1.4531 time=391.4s lr=1.43e-05\nEpoch 20/25 train_loss=1.4144 val_loss=1.4514 time=389.0s lr=1.01e-05\nEpoch 21/25 train_loss=1.3973 val_loss=1.4488 time=382.3s lr=6.52e-06\nEpoch 22/25 train_loss=1.4001 val_loss=1.4476 time=379.4s lr=3.70e-06\n[Early Stop] No improvement (>0.01 delta) for 4 consecutive epochs. Stopping at epoch 22.\nTraining finished. Best metric: 1.4522269883799175 (early stop)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === Model Export / Import Utilities ===\nimport os, json, torch, math\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any, Tuple\n\n# ------------------------------\n# Export\n# ------------------------------\n\ndef export_model(\n    save_dir: str,\n    model: torch.nn.Module,\n    cfg_obj,\n    optimizer: Optional[torch.optim.Optimizer] = None,\n    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n    epoch: Optional[int] = None,\n    global_step: Optional[int] = None,\n    best_val: Optional[float] = None,\n    tag: str = \"latest\",\n    use_safetensors: bool = False,\n) -> str:\n    \"\"\"Export model checkpoint + (optional) optimizer/scheduler.\n\n    Saves:\n      - config.json (raw cfg values)\n      - tokenizer/ (HF tokenizer)\n      - clip_mt5_prefix_<tag>.pt (bundle) OR .safetensors (+ meta files)\n\n    Bundle (.pt) contains:\n      model_state, cfg, epoch, global_step, tag, export_time,\n      optimizer_state?, scheduler_state?, best_val?, hyperparams.\n\n    hyperparams dict added so retraining can auto‑rebuild optimizer/scheduler:\n      { lr, clip_lr_scale, weight_decay, warmup_steps, grad_clip, use_amp, use_bf16 }\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Collect base config keys if available\n    if 'base_config' in globals():\n        cfg_dict = {k: getattr(cfg_obj, k) for k in base_config.keys() if hasattr(cfg_obj, k)}\n    else:\n        cfg_dict = {k: v for k, v in vars(cfg_obj).items() if not k.startswith('_')}\n\n    # Persist config separately (human readable)\n    with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n        json.dump(cfg_dict, f, ensure_ascii=False, indent=2)\n\n    # Save tokenizer (best effort)\n    try:\n        model.tokenizer.save_pretrained(os.path.join(save_dir, 'tokenizer'))\n    except Exception as e:\n        print(f\"[WARN] Tokenizer save failed: {e}\")\n\n    checkpoint_name = f\"clip_mt5_prefix_{tag}\"\n\n    hyperparams = {\n        'lr': cfg_dict.get('lr'),\n        'clip_lr_scale': cfg_dict.get('clip_lr_scale'),\n        'weight_decay': cfg_dict.get('weight_decay'),\n        'warmup_steps': cfg_dict.get('warmup_steps'),\n        'grad_clip': cfg_dict.get('grad_clip'),\n        'use_amp': cfg_dict.get('use_amp'),\n        'use_bf16': cfg_dict.get('use_bf16'),\n        'batch_size': cfg_dict.get('batch_size'),\n    }\n\n    if use_safetensors:\n        try:\n            from safetensors.torch import save_file\n            weights = model.state_dict()\n            save_file(weights, os.path.join(save_dir, checkpoint_name + '.safetensors'))\n            meta = {\n                'cfg': cfg_dict,\n                'epoch': epoch,\n                'global_step': global_step,\n                'export_time': datetime.utcnow().isoformat() + 'Z',\n                'tag': tag,\n                'best_val': best_val,\n                'has_optimizer': optimizer is not None,\n                'has_scheduler': scheduler is not None,\n                'hyperparams': hyperparams,\n            }\n            if optimizer:\n                torch.save(optimizer.state_dict(), os.path.join(save_dir, checkpoint_name + '.optimizer.pt'))\n            if scheduler:\n                torch.save(scheduler.state_dict(), os.path.join(save_dir, checkpoint_name + '.scheduler.pt'))\n            with open(os.path.join(save_dir, 'meta.json'), 'w') as f:\n                json.dump(meta, f, indent=2)\n            print(f\"[EXPORT] Weights -> {checkpoint_name}.safetensors (meta.json written)\")\n            return os.path.join(save_dir, checkpoint_name + '.safetensors')\n        except ImportError:\n            print('[WARN] safetensors not installed; falling back to .pt')\n\n    # Standard .pt route\n    bundle = {\n        'model_state': model.state_dict(),\n        'cfg': cfg_dict,\n        'epoch': epoch,\n        'global_step': global_step,\n        'export_time': datetime.utcnow().isoformat() + 'Z',\n        'tag': tag,\n        'best_val': best_val,\n        'hyperparams': hyperparams,\n    }\n    if optimizer:\n        bundle['optimizer_state'] = optimizer.state_dict()\n    if scheduler:\n        bundle['scheduler_state'] = scheduler.state_dict()\n    out_path = os.path.join(save_dir, checkpoint_name + '.pt')\n    torch.save(bundle, out_path)\n    print(f\"[EXPORT] Saved checkpoint: {out_path}\")\n    return out_path\n\n# ------------------------------\n# Helper: build optimizer (main + clip groups)\n# ------------------------------\n\ndef build_optimizer_from_hparams(model: torch.nn.Module, h: Dict[str, Any]):\n    lr = h.get('lr', 1e-4)\n    clip_lr_scale = h.get('clip_lr_scale', 0.05) or 0.05\n    weight_decay = h.get('weight_decay', 0.0)\n    main_params, clip_params = [], []\n    for n, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        (clip_params if n.startswith('clip.') else main_params).append(p)\n    param_groups = []\n    if main_params:\n        param_groups.append({'params': main_params, 'lr': lr})\n    if clip_params:\n        param_groups.append({'params': clip_params, 'lr': lr * clip_lr_scale})\n    if clip_params:\n        print(f\"[OPT] CLIP params: {len(clip_params)} lr={lr * clip_lr_scale:.2e}\")\n    return torch.optim.AdamW(param_groups, weight_decay=weight_decay)\n\n# ------------------------------\n# Helper: build cosine scheduler with warmup (same as training)\n# ------------------------------\n\ndef build_scheduler_from_hparams(optimizer, h: Dict[str, Any], steps_per_epoch: int, total_epochs: int):\n    warmup_steps = h.get('warmup_steps', 0) or 0\n    total_steps = steps_per_epoch * total_epochs\n    def lr_lambda(step):\n        if warmup_steps > 0 and step < warmup_steps:\n            return float(step) / float(max(1, warmup_steps))\n        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps)) if total_steps > warmup_steps else 1.0\n        progress = min(max(progress, 0.0), 1.0)\n        return 0.5 * (1 + math.cos(math.pi * progress))\n    from torch.optim.lr_scheduler import LambdaLR\n    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# ------------------------------\n# Import for finetune (unchanged except now returns hyperparams)\n# ------------------------------\n\ndef load_model_for_finetune(\n    load_dir: str,\n    device: torch.device,\n    checkpoint_tag: str = 'latest',\n    resume_optimizer: bool = True,\n    build_optimizer_fn=None,\n    build_scheduler_fn=None,\n    override_cfg: Optional[Dict[str, Any]] = None,\n    prefer_safetensors: bool = True\n):\n    # Load config\n    with open(os.path.join(load_dir, 'config.json'), 'r', encoding='utf-8') as f:\n        cfg_json = json.load(f)\n    if override_cfg:\n        cfg_json.update(override_cfg)\n\n    class _Cfg: ...\n    cfg_obj = _Cfg()\n    for k, v in cfg_json.items():\n        setattr(cfg_obj, k, v)\n\n    model = CLIPmT5Pipeline(cfg_obj).to(device)\n\n    ckpt_base = f\"clip_mt5_prefix_{checkpoint_tag}\"\n    safepath = os.path.join(load_dir, ckpt_base + '.safetensors')\n    ptpath = os.path.join(load_dir, ckpt_base + '.pt')\n\n    optimizer_state = scheduler_state = None\n    epoch = global_step = None\n    hyperparams = None\n    used_safetensors = False\n\n    if prefer_safetensors and os.path.isfile(safepath):\n        try:\n            from safetensors.torch import load_file\n            weights = load_file(safepath, device=device)\n            model.load_state_dict(weights, strict=True)\n            used_safetensors = True\n            meta_path = os.path.join(load_dir, 'meta.json')\n            meta = {}\n            if os.path.isfile(meta_path):\n                with open(meta_path, 'r') as f:\n                    meta = json.load(f)\n            epoch = meta.get('epoch'); global_step = meta.get('global_step')\n            hyperparams = meta.get('hyperparams')\n            if resume_optimizer and meta.get('has_optimizer'):\n                opt_file = os.path.join(load_dir, ckpt_base + '.optimizer.pt')\n                if os.path.isfile(opt_file):\n                    optimizer_state = torch.load(opt_file, map_location='cpu')\n            if resume_optimizer and meta.get('has_scheduler'):\n                sch_file = os.path.join(load_dir, ckpt_base + '.scheduler.pt')\n                if os.path.isfile(sch_file):\n                    scheduler_state = torch.load(sch_file, map_location='cpu')\n        except Exception as e:\n            print(f\"[WARN] safetensors load failed ({e}); falling back to .pt\")\n            used_safetensors = False\n\n    if not used_safetensors:\n        if not os.path.isfile(ptpath):\n            raise FileNotFoundError(f\"No checkpoint found at {ptpath}\")\n        bundle = torch.load(ptpath, map_location=device)\n        model.load_state_dict(bundle['model_state'], strict=True)\n        epoch = bundle.get('epoch'); global_step = bundle.get('global_step')\n        hyperparams = bundle.get('hyperparams')\n        if resume_optimizer:\n            optimizer_state = bundle.get('optimizer_state')\n            scheduler_state = bundle.get('scheduler_state')\n\n    print(f\"[IMPORT] Model loaded (epoch={epoch}, global_step={global_step})\")\n    return model, cfg_obj, hyperparams, optimizer_state, scheduler_state, epoch, global_step\n\n# ------------------------------\n# Simple one-shot retrain loader\n# ------------------------------\n\ndef load_model_for_retrain(\n    checkpoint_path: str,\n    device: torch.device,\n    new_lr: Optional[float] = None,\n    new_clip_lr_scale: Optional[float] = None,\n    reset_optimizer: bool = True,\n    freeze_clip: Optional[bool] = None,\n    override_cfg: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Load a .pt bundle and prepare model + (fresh) optimizer hyperparams for retraining.\n\n    Returns (model, cfg_obj, optimizer_hparams, epoch_loaded, global_step_loaded)\n    Caller should then: optimizer = build_optimizer_from_hparams(model, optimizer_hparams)\n    and build a new scheduler with build_scheduler_from_hparams once steps_per_epoch known.\n    \"\"\"\n    assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n    bundle = torch.load(checkpoint_path, map_location=device)\n    if 'model_state' not in bundle:\n        raise ValueError('Not an export_model bundle (.pt)')\n    cfg_json = dict(bundle['cfg'])\n    if override_cfg:\n        cfg_json.update(override_cfg)\n\n    class _Cfg: ...\n    cfg_obj = _Cfg()\n    for k, v in cfg_json.items():\n        setattr(cfg_obj, k, v)\n    if freeze_clip is not None:\n        cfg_obj.freeze_clip = freeze_clip\n\n    model = CLIPmT5Pipeline(cfg_obj).to(device)\n    model.load_state_dict(bundle['model_state'], strict=True)\n\n    base_h = bundle.get('hyperparams', {})\n    # Override learning rates if requested\n    if new_lr is not None:\n        base_h['lr'] = new_lr\n    if new_clip_lr_scale is not None:\n        base_h['clip_lr_scale'] = new_clip_lr_scale\n\n    # If freezing clip newly, we still keep clip_lr_scale but it won't matter\n    epoch_loaded = bundle.get('epoch')\n    global_step_loaded = bundle.get('global_step')\n    print(f\"[RETRAIN] Loaded weights (epoch={epoch_loaded}). Preparing fresh optimizer hyperparams.\")\n    return model, cfg_obj, base_h, epoch_loaded, global_step_loaded\n\nprint('[READY] Enhanced export/import + retrain helpers available.')","metadata":{"execution":{"iopub.status.busy":"2025-09-08T11:27:48.952622Z","iopub.execute_input":"2025-09-08T11:27:48.953267Z","iopub.status.idle":"2025-09-08T11:27:48.982753Z","shell.execute_reply.started":"2025-09-08T11:27:48.953242Z","shell.execute_reply":"2025-09-08T11:27:48.981998Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[READY] Enhanced export/import + retrain helpers available.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Export the model\nexport_model(\"/kaggle/working/exports/run1\", model_mm, cfg, optimizer, scheduler, epoch=epoch, global_step=global_step, tag=\"epoch_last\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:03:21.051496Z","iopub.execute_input":"2025-09-08T09:03:21.051772Z","iopub.status.idle":"2025-09-08T09:03:30.834545Z","shell.execute_reply.started":"2025-09-08T09:03:21.051748Z","shell.execute_reply":"2025-09-08T09:03:30.833822Z"}},"outputs":[{"name":"stdout","text":"[EXPORT] Saved checkpoint: /kaggle/working/exports/run1/clip_mt5_prefix_epoch_last.pt\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/exports/run1/clip_mt5_prefix_epoch_last.pt'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# --- Validation Subset Metrics (pycocoevalcap) Helper ---\n# Use to compute BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE on a fixed subset (default 200) of validation images each epoch.\n# Call inside training/resume loops after computing (train/val) losses.\n\ndef compute_val_subset_metrics(\n    model,\n    val_dataset,\n    images_root: str,\n    device,\n    amp_dtype=None,\n    sample_size: int = 200,\n    seed: int = 42,\n    batch_size: int = 8,\n    verbose: bool = False,\n):\n    import random, time, os, torch\n    from collections import defaultdict\n    if val_dataset is None or len(val_dataset) == 0:\n        return {}\n\n    # Build refs map (filename -> list[captions]) based on dataset.samples (filename, caption)\n    refs_map = defaultdict(list)\n    for (fname, cap) in val_dataset.samples:\n        c = cap.strip()\n        if c:\n            refs_map[fname].append(c)\n    unique_files = list(refs_map.keys())\n    if not unique_files:\n        return {}\n    random.Random(seed).shuffle(unique_files)\n    subset_files = unique_files[: min(sample_size, len(unique_files))]\n\n    image_paths = [os.path.join(images_root, f) for f in subset_files]\n\n    hyps, refs = [], []\n    model.eval()\n    t0 = time.time()\n\n    def _gen(paths):\n        if amp_dtype is not None:\n            with torch.amp.autocast('cuda', dtype=amp_dtype):\n                return model.generate(image_paths=paths, ban_sentinels=True)\n        return model.generate(image_paths=paths, ban_sentinels=True)\n\n    for i in range(0, len(image_paths), batch_size):\n        batch = image_paths[i:i+batch_size]\n        outs = _gen(batch)\n        for p, pred in zip(batch, outs):\n            fname = os.path.basename(p)\n            hyps.append(pred if pred else \"<EMPTY>\")\n            refs.append(refs_map.get(fname, [\" \"]))\n\n    gts = {i: refs[i] for i in range(len(refs))}\n    res = {i: [hyps[i]] for i in range(len(hyps))}\n\n    from pycocoevalcap.bleu.bleu import Bleu\n    from pycocoevalcap.meteor.meteor import Meteor\n    from pycocoevalcap.rouge.rouge import Rouge\n    from pycocoevalcap.cider.cider import Cider\n    try:\n        from pycocoevalcap.spice.spice import Spice\n    except Exception:\n        Spice = None\n\n    metrics = {}\n    try:\n        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n        metrics['bleu1'] = float(bleu_scores[0]); metrics['bleu2'] = float(bleu_scores[1])\n        metrics['bleu3'] = float(bleu_scores[2]); metrics['bleu4'] = float(bleu_scores[3])\n        metrics['bleu'] = float(bleu_scores[3])\n    except Exception as e:\n        if verbose: print('[VAL_SUBSET] BLEU failed:', e)\n    try:\n        meteor_score, _ = Meteor().compute_score(gts, res)\n        metrics['meteor'] = float(meteor_score)\n    except Exception as e:\n        if verbose: print('[VAL_SUBSET] METEOR failed:', e)\n    try:\n        rouge_score, _ = Rouge().compute_score(gts, res)\n        metrics['rougeL'] = float(rouge_score)\n    except Exception as e:\n        if verbose: print('[VAL_SUBSET] ROUGE-L failed:', e)\n    try:\n        cider_score, _ = Cider().compute_score(gts, res)\n        metrics['cider'] = float(cider_score)\n    except Exception as e:\n        if verbose: print('[VAL_SUBSET] CIDEr failed:', e)\n    if Spice:\n        try:\n            spice_score, _ = Spice().compute_score(gts, res)\n            metrics['spice'] = float(spice_score)\n        except Exception as e:\n            if verbose: print('[VAL_SUBSET] SPICE failed:', e)\n\n    metrics['subset_size'] = len(subset_files)\n    metrics['eval_time_s'] = round(time.time() - t0, 2)\n    if verbose:\n        print('[VAL_SUBSET]', ', '.join(f\"{k}={v:.3f}\" for k,v in metrics.items() if isinstance(v,(int,float))))\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T10:15:18.218420Z","iopub.execute_input":"2025-09-08T10:15:18.218933Z","iopub.status.idle":"2025-09-08T10:15:18.235423Z","shell.execute_reply.started":"2025-09-08T10:15:18.218905Z","shell.execute_reply":"2025-09-08T10:15:18.234595Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# === Direct .pt Checkpoint Loader (file-based) ===\nimport torch, os, json\nfrom typing import Optional, Dict, Any, Tuple\n\ndef load_model_from_checkpoint_file(\n    checkpoint_path: str,\n    device: torch.device,\n    resume_optimizer: bool = False,\n    build_optimizer_fn=None,\n    build_scheduler_fn=None,\n    override_cfg: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Load model (and optional optimizer/scheduler) directly from a single .pt file.\n\n    Expects the .pt bundle produced by export_model (contains 'model_state' and 'cfg').\n\n    Returns: (model, cfg_obj, optimizer, scheduler, epoch, global_step)\n    \"\"\"\n    assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n    bundle = torch.load(checkpoint_path, map_location=device)\n    if 'model_state' not in bundle or 'cfg' not in bundle:\n        raise ValueError(\"Checkpoint missing required keys 'model_state' or 'cfg'\")\n\n    cfg_json = dict(bundle['cfg'])\n    if override_cfg:\n        cfg_json.update(override_cfg)\n\n    class _Cfg: ...\n    cfg_obj = _Cfg()\n    for k, v in cfg_json.items():\n        setattr(cfg_obj, k, v)\n\n    model = CLIPmT5Pipeline(cfg_obj).to(device)\n    model.load_state_dict(bundle['model_state'], strict=True)\n\n    epoch = bundle.get('epoch')\n    global_step = bundle.get('global_step')\n\n    optimizer = None\n    scheduler = None\n    if resume_optimizer and 'optimizer_state' in bundle:\n        if build_optimizer_fn is None:\n            print('[WARN] Optimizer state present but build_optimizer_fn not provided; skipping optimizer restore.')\n        else:\n            optimizer = build_optimizer_fn(cfg_obj, model)\n            try:\n                optimizer.load_state_dict(bundle['optimizer_state'])\n                print('[IMPORT] Optimizer state restored.')\n            except Exception as e:\n                print(f'[WARN] Failed to load optimizer state: {e}')\n    if resume_optimizer and optimizer and 'scheduler_state' in bundle and build_scheduler_fn:\n        scheduler = build_scheduler_fn(cfg_obj, optimizer)\n        try:\n            scheduler.load_state_dict(bundle['scheduler_state'])\n            print('[IMPORT] Scheduler state restored.')\n        except Exception as e:\n            print(f'[WARN] Failed to load scheduler state: {e}')\n\n    print(f\"[IMPORT] Loaded model from file: {checkpoint_path}\")\n    return model, cfg_obj, optimizer, scheduler, epoch, global_step\n\n# One-line usage example (inference only):\n# model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt', device=torch.device('cuda'), resume_optimizer=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:10:02.564721Z","iopub.execute_input":"2025-09-08T12:10:02.565396Z","iopub.status.idle":"2025-09-08T12:10:02.574612Z","shell.execute_reply.started":"2025-09-08T12:10:02.565372Z","shell.execute_reply":"2025-09-08T12:10:02.573847Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the model\nmodel_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/input/25_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt', device=torch.device('cuda'), resume_optimizer=False)","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:10:09.215042Z","iopub.execute_input":"2025-09-08T12:10:09.215575Z","iopub.status.idle":"2025-09-08T12:10:41.876589Z","shell.execute_reply.started":"2025-09-08T12:10:09.215549Z","shell.execute_reply":"2025-09-08T12:10:41.875905Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'T5Tokenizer'. \nThe class this function is called from is 'MT5Tokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"[IMPORT] Loaded model from file: /kaggle/input/25_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Testing / Inference cell (evaluate test set + pycocoevalcap metrics only)\nimport torch, time, os, json, math\nfrom typing import List\nfrom PIL import Image\n\nassert torch.cuda.is_available(), \"CUDA GPU required for inference.\"\nmodel_mm.eval()\ndevice = torch.device('cuda')\nmodel_mm.to(device)\n\n# Determine AMP dtype consistent with training\namp_dtype = None\nif getattr(cfg, 'use_amp', True):\n    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n        amp_dtype = torch.bfloat16\n    else:\n        amp_dtype = torch.float16\n\n# -------------------- Test Loss --------------------\ntest_loss = None\nif test_loader:\n    t0 = time.time(); total=0.0\n    with torch.no_grad():\n        for batch in test_loader:\n            imgs, srcs, tgts = batch\n            imgs = imgs.to(device, non_blocking=True)\n            if amp_dtype is not None:\n                with torch.amp.autocast('cuda', dtype=amp_dtype):\n                    out = model_mm(imgs, srcs, tgts)\n            else:\n                out = model_mm(imgs, srcs, tgts)\n            total += out.loss.item()\n    test_loss = total / max(1,len(test_loader))\n    print(f\"Test loss={test_loss:.4f} time={(time.time()-t0):.1f}s\")\nelse:\n    print(\"No test split available.\")\n\n############################################\n# Caption Quality Metrics (BLEU1-4, METEOR, CIDEr, ROUGE-L, SPICE) + W&B logging\n# All via pycocoevalcap (no evaluate dependency)\n############################################\nmetrics = {}\ntry:\n    import nltk\n    for pkg in ['punkt', 'wordnet', 'omw-1.4']:\n        try:\n            nltk.data.find(f'tokenizers/{pkg}')\n        except Exception:\n            try:\n                nltk.download(pkg, quiet=True)\n            except Exception:\n                pass\n\n    # Build reference sets (all captions per test image) from original JSON\n    with open(json_path) as f:\n        data_json = json.load(f)\n    img_entries = data_json['images'] if isinstance(data_json, dict) else data_json\n\n    # Gather unique test image filenames present in test_dataset\n    test_image_names = sorted({s[0] for s in test_dataset.samples})\n    refs_map = {}\n    for entry in img_entries:\n        if not isinstance(entry, dict):\n            continue\n        fname = entry.get('filename')\n        if fname in test_image_names:\n            all_caps = []\n            for s in entry.get('sentences', []):\n                if isinstance(s, dict) and 'raw' in s:\n                    cap = s['raw'].strip()\n                    if cap:\n                        all_caps.append(cap)\n            if all_caps:\n                refs_map[fname] = all_caps\n\n    # Generate hypotheses (batched for speed)\n    hyps = []\n    refs = []\n    BATCH_GEN = 12  # moderate batch size; adjust as GPU allows\n    image_paths = [os.path.join(images_root, fname) for fname in test_image_names]\n\n    def _generate_batch(paths):\n        if amp_dtype is not None:\n            with torch.amp.autocast('cuda', dtype=amp_dtype):\n                return model_mm.generate(image_paths=paths, ban_sentinels=True)\n        return model_mm.generate(image_paths=paths, ban_sentinels=True)\n\n    gen_start = time.time()\n    for i in range(0, len(image_paths), BATCH_GEN):\n        batch_paths = image_paths[i:i+BATCH_GEN]\n        caps = _generate_batch(batch_paths)\n        for p, pred in zip(batch_paths, caps):\n            fname = os.path.basename(p)\n            pred = pred if pred else \"<EMPTY>\"\n            hyps.append(pred)\n            # list of refs (avoid empties)\n            cur_refs = refs_map.get(fname, [\" \"])\n            refs.append([r for r in cur_refs if r.strip()] or [\" \"])\n    print(f\"[GEN] Generated {len(hyps)} captions in {time.time()-gen_start:.1f}s\")\n\n    # Build COCO-style dicts for pycocoevalcap\n    gts = {i: refs[i] for i in range(len(refs))}\n    res = {i: [hyps[i]] for i in range(len(hyps))}\n\n    # ---- pycocoevalcap metrics ----\n    from pycocoevalcap.bleu.bleu import Bleu\n    from pycocoevalcap.meteor.meteor import Meteor\n    from pycocoevalcap.rouge.rouge import Rouge\n    from pycocoevalcap.cider.cider import Cider\n    from pycocoevalcap.spice.spice import Spice\n\n    metrics = {}\n    # BLEU (returns list of 4 scores)\n    try:\n        bleu_scorer = Bleu(4)\n        bleu_scores, _ = bleu_scorer.compute_score(gts, res)\n        metrics['bleu1'] = float(bleu_scores[0])\n        metrics['bleu2'] = float(bleu_scores[1])\n        metrics['bleu3'] = float(bleu_scores[2])\n        metrics['bleu4'] = float(bleu_scores[3])\n        metrics['bleu'] = float(bleu_scores[3])  # treat BLEU-4 as aggregate\n    except Exception as e:\n        print('[WARN] BLEU failed:', e)\n\n    try:\n        meteor_scorer = Meteor()\n        meteor_score, _ = meteor_scorer.compute_score(gts, res)\n        metrics['meteor'] = float(meteor_score)\n    except Exception as e:\n        print('[WARN] METEOR failed:', e)\n\n    try:\n        rouge_scorer = Rouge()\n        rouge_score, _ = rouge_scorer.compute_score(gts, res)\n        metrics['rougeL'] = float(rouge_score)\n    except Exception as e:\n        print('[WARN] ROUGE-L failed:', e)\n\n    try:\n        cider_scorer = Cider()\n        cider_score, _ = cider_scorer.compute_score(gts, res)\n        metrics['cider'] = float(cider_score)\n    except Exception as e:\n        print('[WARN] CIDEr failed:', e)\n\n    try:\n        spice_scorer = Spice()\n        spice_score, spice_scores = spice_scorer.compute_score(gts, res)\n        metrics['spice'] = float(spice_score)\n    except Exception as e:\n        print('[WARN] SPICE failed (Java required?):', e)\n\n    if test_loss is not None:\n        metrics['test_loss'] = test_loss\n\n    print(\"=== Caption Metrics (pycocoevalcap) ===\")\n    for k,v in metrics.items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"{k}: {v}\")\n\n    if use_wandb:\n        wandb.log({f\"eval/{k}\": v for k,v in metrics.items()}, commit=True)\n        print(\"[wandb] Logged caption metrics.\")\n\nexcept Exception as e:\n    print(\"[ERROR] Metric computation failed:\", e)\n\n# Show a few sample generated captions from first N test images with diversity controls\ndefault_gen_kwargs = dict(\n    repetition_penalty=1.2,\n    no_repeat_ngram_size=3,\n)\nSAMPLE_PRINTS = 5\nif test_loader and SAMPLE_PRINTS > 0:\n    shown = 0\n    printed_imgs = set()\n    for img_path, _ in [(os.path.join(images_root, s[0]), s[1]) for s in test_dataset.samples]:\n        if img_path in printed_imgs:\n            continue\n        if amp_dtype is not None:\n            with torch.amp.autocast('cuda', dtype=amp_dtype):\n                caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n        else:\n            caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n        gt_caption = next(s[1] for s in test_dataset.samples if os.path.join(images_root, s[0]) == img_path)\n        print('GT:', gt_caption)\n        print('CAP:', caps[0])\n        print('---------------')\n        printed_imgs.add(img_path)\n        shown += 1\n        if shown >= SAMPLE_PRINTS:\n            break\n\n# Optional: stochastic sampling helper for more diverse outputs\ndef generate_captions(image_paths: List[str], mode='beam', **kwargs):\n    if mode == 'sample':\n        sample_defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n        for k, v in sample_defaults.items():\n            kwargs.setdefault(k, v)\n    else:  # beam\n        beam_defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3)\n        for k, v in beam_defaults.items():\n            kwargs.setdefault(k, v)\n    if amp_dtype is not None:\n        with torch.amp.autocast('cuda', dtype=amp_dtype):\n            return model_mm.generate(image_paths=image_paths, **kwargs)\n    return model_mm.generate(image_paths=image_paths, **kwargs)\n\nprint(\"[Ready] generate_captions(['path/to/img.jpg'], mode='beam')\")","metadata":{"execution":{"iopub.status.busy":"2025-09-08T12:10:53.384909Z","iopub.execute_input":"2025-09-08T12:10:53.385548Z","iopub.status.idle":"2025-09-08T12:20:27.242584Z","shell.execute_reply.started":"2025-09-08T12:10:53.385524Z","shell.execute_reply":"2025-09-08T12:20:27.241926Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Test loss=1.4448 time=32.9s\n[GEN] Generated 1000 captions in 382.1s\n{'testlen': 7001, 'reflen': 7497, 'guess': [7001, 6001, 5001, 4001], 'correct': [1203, 154, 20, 2]}\nratio: 0.9338402027476411\nDownloading stanford-corenlp-3.6.0 for SPICE ...\nProgress: 384.5M / 384.5M (100.0%)\nExtracting stanford-corenlp-3.6.0 ...\nDone.\n","output_type":"stream"},{"name":"stderr","text":"WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\nWARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nParsing reference captions\nInitiating Stanford parsing pipeline\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \ndone [0.7 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\nLoading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\nThreads( StanfordCoreNLP ) [35.599 seconds]\nParsing test captions\nThreads( StanfordCoreNLP ) [8.740 seconds]\nWarning: Nashorn engine is planned to be removed from a future JDK release\n","output_type":"stream"},{"name":"stdout","text":"SPICE evaluation took: 52.13 s\n=== Caption Metrics (pycocoevalcap) ===\nbleu1: 0.1601\nbleu2: 0.0619\nbleu3: 0.0242\nbleu4: 0.0090\nbleu: 0.0090\nmeteor: 0.0883\nrougeL: 0.1482\ncider: 0.0643\nspice: 0.0267\ntest_loss: 1.4448\n[wandb] Logged caption metrics.\nGT: İki kahverengi köpek kar üstünde kavga ediyor.\nCAP: Kırmızı tişörtlü bir adam yeşil bir topu havada yakalamış.\n---------------\nGT: Havuzda sahibine doğru yüzen küçük bir köpek.\nCAP: İki köpek çimlerin üzerinde koşuyor.\n---------------\nGT: Bir sokak festivalinde bir kadın ve bir erkek sambacı dans ediyor, kadın tüy şapka giymiş, arkada bir adam fotoğraf çekiyor.\nCAP: Kırmızı tişörtlü bir çocuk yeşillikler içerisinde koşuyor.\n---------------\nGT: İki kadınla bir adam tahta masa sandalyede bir şemsiyenin altında oturmuş içeceklerini içiyor.\nCAP: Kırmızı tişörtlü bir çocuk yeşillikler içerisinde koşuyor.\n---------------\nGT: Maç sırasında odaklanmış belli bir noktaya bakmakta olan bir Amerikan futbol oyuncusu.\nCAP: Kırmızı tişörtlü bir çocuk yeşillikler içerisinde koşuyor.\n---------------\n[Ready] generate_captions(['path/to/img.jpg'], mode='beam')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Single Image Inference with Optional pycocoevalcap Metrics (BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE)\nfrom typing import Optional, Dict, Any, List\nimport matplotlib.pyplot as plt\nimport os, json, torch\n\n# Cache scorers globally to avoid repeated Java/METEOR/Spice init costs\n_PYCOCO_SCORERS = {}\n\n\ndef _lazy_load_scorers():\n    global _PYCOCO_SCORERS\n    from pycocoevalcap.bleu.bleu import Bleu\n    from pycocoevalcap.meteor.meteor import Meteor\n    from pycocoevalcap.rouge.rouge import Rouge\n    from pycocoevalcap.cider.cider import Cider\n    try:\n        from pycocoevalcap.spice.spice import Spice\n    except Exception:\n        Spice = None\n    if 'bleu' not in _PYCOCO_SCORERS:\n        _PYCOCO_SCORERS['bleu'] = Bleu(4)\n    if 'meteor' not in _PYCOCO_SCORERS:\n        _PYCOCO_SCORERS['meteor'] = Meteor()\n    if 'rouge' not in _PYCOCO_SCORERS:\n        _PYCOCO_SCORERS['rouge'] = Rouge()\n    if 'cider' not in _PYCOCO_SCORERS:\n        _PYCOCO_SCORERS['cider'] = Cider()\n    if Spice and 'spice' not in _PYCOCO_SCORERS:\n        _PYCOCO_SCORERS['spice'] = Spice()\n    return _PYCOCO_SCORERS\n\n\ndef _compute_single_caption_metrics(pred: str, refs: List[str]) -> Dict[str, float]:\n    refs_clean = [r.strip() for r in refs if r and r.strip()]\n    if not refs_clean:\n        return {}\n    scorers = _lazy_load_scorers()\n    gts = {0: refs_clean}\n    res = {0: [pred]}\n    out = {}\n    # BLEU\n    try:\n        bleu_scores, _ = scorers['bleu'].compute_score(gts, res)\n        out['bleu1'] = float(bleu_scores[0])\n        out['bleu2'] = float(bleu_scores[1])\n        out['bleu3'] = float(bleu_scores[2])\n        out['bleu4'] = float(bleu_scores[3])\n        out['bleu'] = float(bleu_scores[3])\n    except Exception as e:\n        print('[WARN] BLEU failed:', e)\n    # METEOR\n    try:\n        meteor_score, _ = scorers['meteor'].compute_score(gts, res)\n        out['meteor'] = float(meteor_score)\n    except Exception as e:\n        print('[WARN] METEOR failed:', e)\n    # ROUGE-L\n    try:\n        rouge_score, _ = scorers['rouge'].compute_score(gts, res)\n        out['rougeL'] = float(rouge_score)\n    except Exception as e:\n        print('[WARN] ROUGE-L failed:', e)\n    # CIDEr\n    try:\n        cider_score, _ = scorers['cider'].compute_score(gts, res)\n        out['cider'] = float(cider_score)\n    except Exception as e:\n        print('[WARN] CIDEr failed:', e)\n    # SPICE (may require Java)\n    if 'spice' in scorers:\n        try:\n            spice_score, spice_scores = scorers['spice'].compute_score(gts, res)\n            out['spice'] = float(spice_score)\n        except Exception as e:\n            print('[WARN] SPICE failed:', e)\n    return out\n\n\ndef predict(\n    image_path: str,\n    prompt: str = \"Bu fotoğrafı açıkla: \",\n    mode: str = \"beam\",              # 'beam' or 'sample'\n    max_refs: Optional[int] = 5,\n    show_image: bool = True,\n    show_refs: bool = True,\n    gen_kwargs: Optional[Dict[str, Any]] = None,\n    json_file: Optional[str] = None,\n    ban_sentinels: bool = True,\n    compute_metrics: bool = True,\n    print_metrics: bool = True,\n) -> Dict[str, Any]:\n    \"\"\"Generate a caption and optionally compute pycocoevalcap metrics.\n\n    Returns dict with: image_path, prediction, references (list), metrics (dict), mode.\n    \"\"\"\n    assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n    jf = json_file or json_path  # use global json_path defined earlier\n\n    # Collect reference captions\n    refs: List[str] = []\n    try:\n        with open(jf) as f:\n            data = json.load(f)\n        entries = data['images'] if isinstance(data, dict) and 'images' in data else data\n        target_name = os.path.basename(image_path)\n        for e in entries:\n            if not isinstance(e, dict):\n                continue\n            if e.get('filename') == target_name:\n                for s in e.get('sentences', []):\n                    if isinstance(s, dict) and 'raw' in s:\n                        cap = s['raw'].strip()\n                        if cap:\n                            refs.append(cap)\n                break\n    except Exception as e:\n        print(f\"[WARN] Could not parse references ({e})\")\n\n    if max_refs is not None:\n        refs = refs[:max_refs]\n\n    # Defaults for decoding\n    gen_kwargs = (gen_kwargs or {}).copy()\n    if mode == \"sample\":\n        defaults = dict(temperature=0.8, top_p=0.9, do_sample=True,\n                        repetition_penalty=1.15, no_repeat_ngram_size=3)\n    else:\n        defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3,\n                        num_beams=getattr(cfg, \"num_beams_infer\", 4))\n    for k, v in defaults.items():\n        gen_kwargs.setdefault(k, v)\n\n    local_amp_dtype = globals().get(\"amp_dtype\", None)\n    model_mm.eval()\n    if local_amp_dtype is not None:\n        with torch.amp.autocast('cuda', dtype=local_amp_dtype):\n            pred = model_mm.generate(\n                image_paths=[image_path],\n                prompt=prompt,\n                ban_sentinels=ban_sentinels,\n                **gen_kwargs\n            )[0]\n    else:\n        pred = model_mm.generate(\n            image_paths=[image_path],\n            prompt=prompt,\n            ban_sentinels=ban_sentinels,\n            **gen_kwargs\n        )[0]\n\n    metrics: Dict[str, float] = {}\n    if compute_metrics and refs:\n        try:\n            metrics = _compute_single_caption_metrics(pred if pred else \"<EMPTY>\", refs)\n        except Exception as e:\n            print(f\"[WARN] Metric computation failed: {e}\")\n\n    # Visualization\n    if show_image:\n        try:\n            img = Image.open(image_path).convert(\"RGB\")\n            plt.figure(figsize=(5,5))\n            plt.imshow(img); plt.axis(\"off\")\n            plt.title(\"Image\")\n            plt.show()\n        except Exception as e:\n            print(f\"[WARN] Could not display image ({e})\")\n\n    if show_refs:\n        if refs:\n            print(\"References:\")\n            for i, r in enumerate(refs, 1):\n                print(f\"  {i}. {r}\")\n        else:\n            print(\"(No references found)\")\n\n    print(\"Prediction:\")\n    print(\" \", pred)\n\n    if print_metrics and metrics:\n        print(\"\\nMetrics (pycocoevalcap):\")\n        for k, v in metrics.items():\n            print(f\"  {k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"  {k}: {v}\")\n\n    return {\n        \"image_path\": image_path,\n        \"prediction\": pred,\n        \"references\": refs,\n        \"metrics\": metrics,\n        \"mode\": mode,\n    }\n\n# Example:\n# result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode=\"beam\")\n# print(result)","metadata":{"execution":{"iopub.execute_input":"2025-09-05T14:24:44.831581Z","iopub.status.busy":"2025-09-05T14:24:44.830831Z","iopub.status.idle":"2025-09-05T14:24:44.971196Z","shell.execute_reply":"2025-09-05T14:24:44.970588Z","shell.execute_reply.started":"2025-09-05T14:24:44.831556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')","metadata":{"execution":{"iopub.execute_input":"2025-09-05T14:24:49.969990Z","iopub.status.busy":"2025-09-05T14:24:49.969480Z","iopub.status.idle":"2025-09-05T14:25:03.923031Z","shell.execute_reply":"2025-09-05T14:25:03.922458Z","shell.execute_reply.started":"2025-09-05T14:24:49.969968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Example: Retrain from exported checkpoint on same or new dataset ===\n# checkpoint_path = '/kaggle/input/your_export/clip_mt5_prefix_latest.pt'\n# from torch.utils.data import DataLoader\n# device = torch.device('cuda')\n# model_re, cfg_re, hparams, epoch_loaded, step_loaded = load_model_for_retrain(\n#     checkpoint_path,\n#     device=device,\n#     new_lr=5e-5,              # optionally override LR\n#     new_clip_lr_scale=0.02,   # optionally override CLIP LR scale\n#     freeze_clip=None,         # or True to freeze CLIP now\n# )\n# optimizer_re = build_optimizer_from_hparams(model_re, hparams)\n# steps_per_epoch = len(train_loader)  # after you rebuild train_loader for (same or new) dataset\n# scheduler_re = build_scheduler_from_hparams(optimizer_re, hparams, steps_per_epoch, total_epochs=5)\n# # Proceed with standard training loop using model_re / optimizer_re / scheduler_re","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Resume / Continue Training From Exported or Training Checkpoint (.pt) ====\n# Improvements:\n#  - Ensures 'checkpoints' directory exists before saving.\n#  - Handles tokenizer class mismatch notice (mt5 vs t5) by reloading MT5 tokenizer explicitly.\n#  - Initializes best_val robustly (evaluates validation if inf).\n#  - Safe scheduler positioning.\n#  - Adds optional immediate validation before continuing to set baseline.\n\nRESUME_PATH = '/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt'  # change as needed\nEXTRA_EPOCHS = 5          # number of extra epochs to train\nRUN_INITIAL_VAL = True    # run a validation pass right after load to set best_val if missing\nSAVE_NAME = 'best_resumed.pt'\n\nimport os, math, time, torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\n\nos.makedirs('checkpoints', exist_ok=True)\n\nif not os.path.isfile(RESUME_PATH):\n    print(f\"[RESUME] Path not found: {RESUME_PATH} -> Skip.\")\nelse:\n    print(f\"[RESUME] Loading: {RESUME_PATH}\")\n    bundle = torch.load(RESUME_PATH, map_location='cpu')\n\n    has_export_format = 'model_state' in bundle\n    has_train_ckpt_format = 'model' in bundle\n    if not (has_export_format or has_train_ckpt_format):\n        raise ValueError('Unrecognized checkpoint format.')\n\n    cfg_dict = bundle.get('cfg') or {}\n    class _Cfg: ...\n    cfg_resume = _Cfg()\n    for k, v in base_config.items():\n        setattr(cfg_resume, k, cfg_dict.get(k, v))\n\n    # Force consistent tokenizer type (MT5Tokenizer) regardless of original class\n    model_resume = CLIPmT5Pipeline(cfg_resume).to(device)\n\n    if has_export_format:\n        model_resume.load_state_dict(bundle['model_state'], strict=True)\n        loaded_epoch = bundle.get('epoch', -1)\n        global_step_loaded = bundle.get('global_step', None)\n        best_val_loaded = bundle.get('best_val', float('inf'))\n    else:\n        model_resume.load_state_dict(bundle['model'], strict=True)\n        loaded_epoch = bundle.get('epoch', -1)\n        global_step_loaded = bundle.get('global_step', None)\n        best_val_loaded = bundle.get('best_val', float('inf'))\n\n    print(f\"[RESUME] Weights loaded (epoch={loaded_epoch}, best_val={best_val_loaded})\")\n\n    main_params, clip_params = [], []\n    for n, p in model_resume.named_parameters():\n        if not p.requires_grad: continue\n        (clip_params if n.startswith('clip.') else main_params).append(p)\n    param_groups = []\n    if main_params: param_groups.append({'params': main_params, 'lr': cfg_resume.lr})\n    if clip_params: param_groups.append({'params': clip_params, 'lr': cfg_resume.lr * getattr(cfg_resume, 'clip_lr_scale', 0.05)})\n    optimizer_resume = AdamW(param_groups, weight_decay=cfg_resume.weight_decay)\n\n    steps_per_epoch = len(train_loader)\n    total_planned_epochs = loaded_epoch + 1 + EXTRA_EPOCHS\n    total_steps = steps_per_epoch * total_planned_epochs\n\n    def lr_lambda(step):\n        if cfg_resume.warmup_steps > 0 and step < cfg_resume.warmup_steps:\n            return float(step) / float(max(1, cfg_resume.warmup_steps))\n        progress = (step - cfg_resume.warmup_steps) / float(max(1, total_steps - cfg_resume.warmup_steps))\n        progress = min(max(progress, 0.0), 1.0)\n        return 0.5 * (1 + math.cos(math.pi * progress))\n\n    scheduler_resume = LambdaLR(optimizer_resume, lr_lambda=lr_lambda)\n\n    opt_key = 'optimizer_state' if has_export_format else 'optimizer'\n    sch_key = 'scheduler_state' if has_export_format else 'scheduler'\n    if opt_key in bundle:\n        try:\n            optimizer_resume.load_state_dict(bundle[opt_key])\n            print('[RESUME] Optimizer state restored.')\n        except Exception as e:\n            print('[WARN] Optimizer state load failed:', e)\n    if sch_key in bundle:\n        try:\n            scheduler_resume.load_state_dict(bundle[sch_key])\n            print('[RESUME] Scheduler state restored.')\n        except Exception as e:\n            print('[WARN] Scheduler state load failed:', e)\n\n    if scheduler_resume.last_epoch < 0 and loaded_epoch >= 0:\n        completed_steps = (loaded_epoch + 1) * steps_per_epoch\n        scheduler_resume.last_epoch = completed_steps\n        print(f\"[RESUME] Scheduler last_epoch set to {scheduler_resume.last_epoch}\")\n\n    amp_dtype_resume = None\n    if getattr(cfg_resume, 'use_amp', True):\n        if getattr(cfg_resume, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n            amp_dtype_resume = torch.bfloat16\n        else:\n            amp_dtype_resume = torch.float16\n    scaler_resume = torch.amp.GradScaler('cuda', enabled=amp_dtype_resume is not None)\n\n    start_epoch = loaded_epoch + 1\n    end_epoch = loaded_epoch + EXTRA_EPOCHS\n\n    best_val = best_val_loaded\n\n    # Optional immediate validation to set best_val if it's inf or user wants baseline\n    if RUN_INITIAL_VAL and (best_val == float('inf') or best_val != best_val):  # inf or NaN\n        if val_loader:\n            model_resume.eval(); v=0.0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs, srcs, tgts = batch\n                    imgs = imgs.to(device, non_blocking=True)\n                    if amp_dtype_resume is not None:\n                        with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n                            out = model_resume(imgs, srcs, tgts)\n                    else:\n                        out = model_resume(imgs, srcs, tgts)\n                    v += out.loss.item()\n            best_val = v / max(1, len(val_loader))\n            print(f\"[RESUME] Initial validation baseline best_val set to {best_val:.4f}\")\n        else:\n            best_val = float('inf')\n\n    early_patience = getattr(cfg_resume, 'early_stop_patience', None)\n    min_delta = getattr(cfg_resume, 'early_stop_min_delta', 0.0)\n    _epochs_no_improve = 0\n\n    global_step_resume = global_step_loaded if global_step_loaded is not None else (start_epoch * steps_per_epoch)\n\n    print(f\"[RESUME] Continue training for {EXTRA_EPOCHS} more epochs: {start_epoch} -> {end_epoch}\")\n\n    for epoch in range(start_epoch, end_epoch + 1):\n        model_resume.train()\n        sum_loss = 0.0\n        t0 = time.time()\n        for step, batch in enumerate(train_loader, start=1):\n            imgs, srcs, tgts = batch\n            imgs = imgs.to(device, non_blocking=True)\n            optimizer_resume.zero_grad(set_to_none=True)\n            if amp_dtype_resume is not None:\n                with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n                    out = model_resume(imgs, srcs, tgts)\n                    loss = out.loss\n                scaler_resume.scale(loss).backward()\n                if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n                    scaler_resume.unscale_(optimizer_resume)\n                    torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n                scaler_resume.step(optimizer_resume)\n                scaler_resume.update()\n            else:\n                out = model_resume(imgs, srcs, tgts); loss = out.loss\n                loss.backward()\n                if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n                    torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n                optimizer_resume.step()\n            scheduler_resume.step()\n            sum_loss += loss.item()\n            global_step_resume += 1\n        train_loss = sum_loss / max(1, len(train_loader))\n\n        val_loss = None\n        if val_loader:\n            model_resume.eval(); v = 0.0\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs, srcs, tgts = batch\n                    imgs = imgs.to(device, non_blocking=True)\n                    if amp_dtype_resume is not None:\n                        with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n                            out = model_resume(imgs, srcs, tgts)\n                        \n                    else:\n                        out = model_resume(imgs, srcs, tgts)\n                    v += out.loss.item()\n            val_loss = v / max(1, len(val_loader))\n\n        dt = time.time() - t0\n        lr_cur = scheduler_resume.get_last_lr()[0]\n        if val_loss is not None:\n            print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} val={val_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n        else:\n            print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n\n        metric = val_loss if val_loss is not None else train_loss\n        improved = metric < (best_val - min_delta)\n        if improved:\n            best_val = metric\n            _epochs_no_improve = 0\n            save_obj = {\n                'model': model_resume.state_dict(),\n                'cfg': {k: getattr(cfg_resume, k) for k in base_config.keys()},\n                'epoch': epoch,\n                'optimizer': optimizer_resume.state_dict(),\n                'scheduler': scheduler_resume.state_dict(),\n                'best_val': best_val,\n                'global_step': global_step_resume,\n            }\n            torch.save(save_obj, os.path.join('checkpoints', SAVE_NAME))\n            print(f\"   -> [RESUME] Saved {SAVE_NAME} (metric={best_val:.4f})\")\n        else:\n            _epochs_no_improve += 1\n            if early_patience is not None and _epochs_no_improve >= early_patience:\n                print(f\"[Early Stop - Resume] No improvement for {early_patience} epochs.\")\n                break\n\n    print('[RESUME] Training extension finished. Final best_val=', best_val)\n    model_mm = model_resume","metadata":{"execution":{"iopub.execute_input":"2025-09-05T13:00:16.009153Z","iopub.status.busy":"2025-09-05T13:00:16.008548Z","iopub.status.idle":"2025-09-05T13:47:52.130937Z","shell.execute_reply":"2025-09-05T13:47:52.130165Z","shell.execute_reply.started":"2025-09-05T13:00:16.009099Z"},"trusted":true},"outputs":[],"execution_count":null}]}