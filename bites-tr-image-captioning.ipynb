{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP (ViT-B/32) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze, edit: unfreeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T13:14:04.777734Z",
     "iopub.status.busy": "2025-09-08T13:14:04.777146Z",
     "iopub.status.idle": "2025-09-08T13:14:38.484377Z",
     "shell.execute_reply": "2025-09-08T13:14:38.483510Z",
     "shell.execute_reply.started": "2025-09-08T13:14:04.777706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping bigframes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping cesium as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping gcsfs as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/openai/CLIP.git (from -r /kaggle/input/requirements8/requirements.txt (line 20))\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-er1x5q73\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-er1x5q73\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pycocoevalcap@ git+https://github.com/salaniz/pycocoevalcap.git (from -r /kaggle/input/requirements8/requirements.txt (line 23))\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-install-jk_9fpye/pycocoevalcap_288868bc0fc944a28ea62655628604b5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap.git /tmp/pip-install-jk_9fpye/pycocoevalcap_288868bc0fc944a28ea62655628604b5\n",
      "  Resolved https://github.com/salaniz/pycocoevalcap.git to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 2)) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 3)) (0.21.0+cu124)\n",
      "Requirement already satisfied: accelerate>=0.29.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: transformers==4.52.4 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 7)) (4.52.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 8)) (0.33.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: pillow>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 12)) (11.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 13)) (3.7.2)\n",
      "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 16)) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 17)) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/input/requirements8/requirements.txt (line 26)) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (0.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0->-r /kaggle/input/requirements8/requirements.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.32.0->-r /kaggle/input/requirements8/requirements.txt (line 8)) (1.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->-r /kaggle/input/requirements8/requirements.txt (line 16)) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->-r /kaggle/input/requirements8/requirements.txt (line 16)) (1.5.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2.4.1)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0->-r /kaggle/input/requirements8/requirements.txt (line 20)) (6.3.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap@ git+https://github.com/salaniz/pycocoevalcap.git->-r /kaggle/input/requirements8/requirements.txt (line 23)) (2.0.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r /kaggle/input/requirements8/requirements.txt (line 13)) (1.17.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0->-r /kaggle/input/requirements8/requirements.txt (line 20)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->-r /kaggle/input/requirements8/requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2024.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->-r /kaggle/input/requirements8/requirements.txt (line 7)) (2025.6.15)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->-r /kaggle/input/requirements8/requirements.txt (line 17)) (2024.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "2025-09-08 13:14:25.058280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757337265.080598     553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757337265.087522     553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Done!\n"
     ]
    }
   ],
   "source": [
    "# Rerun this cell at each session start\n",
    "\n",
    "# Uninstall conflicting packages (Kaggle specific)\n",
    "!pip uninstall -y bigframes cesium gcsfs\n",
    "\n",
    "# Performance metrics\n",
    "!pip install -r /kaggle/input/requirements8/requirements.txt\n",
    "\n",
    "# To use nltk\n",
    "import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\n",
    "\n",
    "# Download mT5-small ViT-B/32\n",
    "import clip, torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "mt5 = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"Setup Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T13:15:00.013903Z",
     "iopub.status.busy": "2025-09-08T13:15:00.013032Z",
     "iopub.status.idle": "2025-09-08T13:15:14.989178Z",
     "shell.execute_reply": "2025-09-08T13:15:14.988387Z",
     "shell.execute_reply.started": "2025-09-08T13:15:00.013870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulkadirparlak\u001b[0m (\u001b[33mabdulkadirparlak-hacettepe-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250908_131507-si4wwnt8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/si4wwnt8' target=\"_blank\">clip_mt5_prefix_run</a></strong> to <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/si4wwnt8' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/si4wwnt8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wandb] run initialized\n",
      "[DEVICE] Using CUDA device: Tesla P100-PCIE-16GB\n",
      "[DEVICE] TF32 enabled.\n",
      "[DEVICE] bfloat16 support: True\n",
      "Active config:\n",
      "  model: google/mt5-small\n",
      "  clip_encoder: ViT-B/32\n",
      "  prefix_tokens: 32\n",
      "  batch_size: 16\n",
      "  lr: 0.0001\n",
      "  epochs: 20\n",
      "  dataset_limit: None\n",
      "  freeze_clip: False\n",
      "  unfreeze_clip_last_n: 0\n",
      "  clip_lr_scale: 0.05\n",
      "  use_clip_patch_tokens: True\n",
      "  freeze_t5_encoder: False\n",
      "  freeze_t5_decoder: False\n",
      "  seed: 42\n",
      "  weight_decay: 0.01\n",
      "  grad_clip: 1\n",
      "  warmup_steps: 500\n",
      "  num_beams_infer: 4\n",
      "  max_new_tokens_infer: 32\n",
      "  src_max_len: 64\n",
      "  tgt_max_len: 64\n",
      "  early_stop_patience: 4\n",
      "  early_stop_min_delta: 0.01\n",
      "  cider_patience: 3\n",
      "  cider_min_delta: 0\n",
      "  use_amp: True\n",
      "  use_bf16: True\n",
      "  enable_tf32: True\n",
      "  finite_loss_skip: True\n",
      "  save_every: 0\n"
     ]
    }
   ],
   "source": [
    "# Unified configuration + optional wandb init\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"bites-tr-image-captioning\"\n",
    "RUN_NAME = \"clip_mt5_prefix_run\"\n",
    "\n",
    "ENABLE_WANDB = True\n",
    "\n",
    "WANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/32\",  # backbone\n",
    "    \"prefix_tokens\": 32,          # stronger conditioning\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 20,                  # allow a bit longer now that we fine-tune CLIP\n",
    "    \"dataset_limit\": None,\n",
    "    # --- CLIP fine-tuning controls ---\n",
    "    \"freeze_clip\": False,          # set False to allow full CLIP fine-tuning\n",
    "    \"unfreeze_clip_last_n\": 0,     # if >0 unfreezes only last N vision blocks (overrides freeze_clip); 0 means ignore and use freeze_clip flag\n",
    "    \"clip_lr_scale\": 0.05,         # scaled LR for ALL CLIP params (set lower than main to avoid destroying pretrained space)\n",
    "    \"use_clip_patch_tokens\": True, # richer conditioning (patch tokens path)\n",
    "    # --- T5 freezing ---\n",
    "    \"freeze_t5_encoder\": False,    # unfreeze encoder so it can adapt to visual prefix distribution\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    # --- Optimization ---\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 500,           # linear warmup steps before cosine decay\n",
    "    # --- Inference defaults ---\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    # --- Legacy loss-based early stopping (still available if reused elsewhere) ---\n",
    "    \"early_stop_patience\": 4,\n",
    "    \"early_stop_min_delta\": 0.01,\n",
    "    # --- NEW: CIDEr-based patience early stopping ---\n",
    "    \"cider_patience\": 3,           # stop if no CIDEr improvement for these many consecutive epochs\n",
    "    \"cider_min_delta\": 0.0,        # require at least this absolute improvement to reset patience\n",
    "    \n",
    "    \"use_amp\": True,              # force AMP on CUDA for speed\n",
    "    # Optional extras:\n",
    "    \"use_bf16\": True,\n",
    "    \"enable_tf32\": True,\n",
    "    \"finite_loss_skip\": True,\n",
    "    \"save_every\": 0\n",
    "}\n",
    "\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "if ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config)\n",
    "        cfg = wandb.config\n",
    "        use_wandb = True\n",
    "        print(\"[wandb] run initialized\")\n",
    "    except Exception as e:\n",
    "        print(\"[wandb] disabled (init failed):\", e)\n",
    "\n",
    "if cfg is None:\n",
    "    class _Cfg: pass\n",
    "    cfg = _Cfg()\n",
    "    for k, v in base_config.items():\n",
    "        setattr(cfg, k, v)\n",
    "    print(\"[INFO] Using local cfg (wandb off)\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# --- Enforce CUDA-only environment ---\n",
    "assert torch.cuda.is_available(), \"CUDA GPU is required but not detected. Please run in a CUDA-enabled environment.\"\n",
    "device = torch.device('cuda')\n",
    "print(f\"[DEVICE] Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Performance toggles\n",
    "if getattr(cfg, 'enable_tf32', False):\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"[DEVICE] TF32 enabled.\")\n",
    "    except Exception as _e:\n",
    "        print(\"[WARN] Could not enable TF32:\", _e)\n",
    "\n",
    "if getattr(cfg, 'use_bf16', False):\n",
    "    bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    print(f\"[DEVICE] bfloat16 support: {bf16_ok}\")\n",
    "\n",
    "print(\"Active config:\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T13:15:23.830395Z",
     "iopub.status.busy": "2025-09-08T13:15:23.829650Z",
     "iopub.status.idle": "2025-09-08T13:15:23.855026Z",
     "shell.execute_reply": "2025-09-08T13:15:23.854269Z",
     "shell.execute_reply.started": "2025-09-08T13:15:23.830367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# (CUDA enforcement handled in config cell; assume cuda device later)\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = MT5Tokenizer.from_pretrained(cfg.model)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        # CLIP freezing / unfreezing strategy\n",
    "        if cfg.unfreeze_clip_last_n and cfg.unfreeze_clip_last_n > 0:\n",
    "            # Freeze everything first\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "            blocks = list(self.clip.visual.transformer.resblocks)\n",
    "            for block in blocks[-cfg.unfreeze_clip_last_n:]:\n",
    "                for p in block.parameters():\n",
    "                    p.requires_grad = True\n",
    "        else:\n",
    "            # Respect freeze_clip flag (False means fully trainable)\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = not cfg.freeze_clip\n",
    "        # T5 freeze toggles\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine embedding dim dynamically (CLIP projection output)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    def _encode_image_single(self, images: torch.Tensor):\n",
    "        # (B, D) pooled embedding (already passed through ln_post + proj inside encode_image)\n",
    "        pooled = self.clip.encode_image(images)\n",
    "        return pooled\n",
    "\n",
    "    def _encode_image_patch_tokens(self, images: torch.Tensor):\n",
    "        \"\"\"Return patch-averaged embedding projected into CLIP joint space.\n",
    "        We manually replicate encode_image path but average patch tokens (excluding CLS),\n",
    "        then apply ln_post and proj so final dim == visual.output_dim (e.g. 512) to match ProjectionMLP in_dim.\"\"\"\n",
    "        visual = self.clip.visual\n",
    "        x = visual.conv1(images)                      # (B, width, grid, grid)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)     # (B, width, patches)\n",
    "        x = x.permute(0, 2, 1)                        # (B, patches, width)\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)         # prepend CLS\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        x = visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)                        # (sequence, B, width)\n",
    "        for block in visual.transformer.resblocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(1, 0, 2)                        # (B, sequence, width)\n",
    "        patches = x[:, 1:, :]                         # drop CLS\n",
    "        pooled = patches.mean(dim=1)                  # (B, width)\n",
    "        if hasattr(visual, 'ln_post'):\n",
    "            pooled = visual.ln_post(pooled)\n",
    "        if hasattr(visual, 'proj') and visual.proj is not None:\n",
    "            pooled = pooled @ visual.proj             # (B, output_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        images = images.to(device)\n",
    "        clip_emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        images = images.to(next(self.parameters()).device)\n",
    "        emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        return self.proj(emb)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu görüntüyü açıkla: \", ban_sentinels=True, **gen_kwargs):\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T13:15:33.521806Z",
     "iopub.status.busy": "2025-09-08T13:15:33.521195Z",
     "iopub.status.idle": "2025-09-08T13:15:55.528554Z",
     "shell.execute_reply": "2025-09-08T13:15:55.527850Z",
     "shell.execute_reply.started": "2025-09-08T13:15:33.521775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 12028  Val: 2006  Test: 2003\n",
      "Trainable params: 468774017\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "json_path = '/kaggle/input/tasviret/flickr8k/tasviret8k_captions.json'\n",
    "images_root = '/kaggle/input/tasviret/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=None)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-08T14:13:30.902Z",
     "iopub.execute_input": "2025-09-08T13:15:59.816061Z",
     "iopub.status.busy": "2025-09-08T13:15:59.815714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AMP] Using bfloat16 mixed precision\n",
      "[INFO] CLIP fine-tune params: 302 with lr=5.00e-06\n",
      "[TRAIN] Starting training with CIDEr-based checkpointing + patience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 3000, 'reflen': 6513, 'guess': [3000, 2000, 1000, 0], 'correct': [423, 52, 0, 0]}\n",
      "ratio: 0.4606172270842222\n",
      "Epoch 01/20 train_loss=17.2405 val_loss=1.9551 BLEU1=0.044 BLEU4=0.000 CIDEr=0.040 time=724.1s lr=9.99e-05\n",
      "  -> Saved best_cider.pt (CIDEr=0.040)\n"
     ]
    }
   ],
   "source": [
    "# Training cell (training + validation + CIDEr-based model selection w/ patience)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "# CUDA-only enforcement (already asserted earlier, but double-check for isolation run)\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required.\"\n",
    "device = torch.device('cuda')\n",
    "model_mm.to(device)\n",
    "\n",
    "# Decide AMP dtype\n",
    "amp_dtype = None\n",
    "if getattr(cfg, 'use_amp', True):\n",
    "    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16\n",
    "        print(\"[AMP] Using bfloat16 mixed precision\")\n",
    "    else:\n",
    "        amp_dtype = torch.float16\n",
    "        print(\"[AMP] Using float16 mixed precision\")\n",
    "else:\n",
    "    print(\"[AMP] Disabled; using full float32\")\n",
    "\n",
    "# Parameter groups: separate CLIP vs non-CLIP for LR scaling\n",
    "main_params = []\n",
    "clip_params = []\n",
    "for name, p in model_mm.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith('clip.'):\n",
    "        clip_params.append(p)\n",
    "    else:\n",
    "        main_params.append(p)\n",
    "param_groups = []\n",
    "if main_params:\n",
    "    param_groups.append({\"params\": main_params, \"lr\": cfg.lr})\n",
    "if clip_params:\n",
    "    scaled_lr = cfg.lr * getattr(cfg, 'clip_lr_scale', 0.05)\n",
    "    param_groups.append({\"params\": clip_params, \"lr\": scaled_lr})\n",
    "    print(f\"[INFO] CLIP fine-tune params: {len(clip_params)} with lr={scaled_lr:.2e}\")\n",
    "\n",
    "optimizer = AdamW(param_groups, weight_decay=cfg.weight_decay)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * cfg.epochs\n",
    "\n",
    "def lr_lambda(step):\n",
    "    # linear warmup then cosine decay\n",
    "    if cfg.warmup_steps > 0 and step < cfg.warmup_steps:\n",
    "        return float(step) / float(max(1, cfg.warmup_steps))\n",
    "    progress = (step - cfg.warmup_steps) / float(max(1, total_steps - cfg.warmup_steps))\n",
    "    progress = min(max(progress, 0.0), 1.0)\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=amp_dtype is not None)\n",
    "CKPT_DIR = 'checkpoints'; os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# --- CIDEr-based model selection with patience ---\n",
    "best_cider = float('-inf')\n",
    "cider_stall_epochs = 0  # epochs since last meaningful improvement\n",
    "cider_patience = getattr(cfg, 'cider_patience', 3)\n",
    "cider_min_delta = getattr(cfg, 'cider_min_delta', 0.0)\n",
    "BEST_CKPT_NAME = 'best_cider.pt'\n",
    "\n",
    "# Helper: compute BLEU-1 / BLEU-4 / CIDEr on full validation set\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_val_caption_metrics(model, val_dataset, images_root, batch_size=16, amp_dtype=None):\n",
    "    if val_dataset is None or len(val_dataset) == 0:\n",
    "        return {}\n",
    "    refs_map = defaultdict(list)\n",
    "    for fname, cap in val_dataset.samples:\n",
    "        cap = cap.strip()\n",
    "        if cap:\n",
    "            refs_map[fname].append(cap)\n",
    "    if not refs_map:\n",
    "        return {}\n",
    "    image_files = list(refs_map.keys())\n",
    "    image_paths = [os.path.join(images_root, f) for f in image_files]\n",
    "\n",
    "    hyps, refs = [], []\n",
    "\n",
    "    def _generate(paths):\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        outs = _generate(batch_paths)\n",
    "        for pth, pred in zip(batch_paths, outs):\n",
    "            fname = os.path.basename(pth)\n",
    "            hyps.append(pred if pred else \"<EMPTY>\")\n",
    "            refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "    gts = {i: refs[i] for i in range(len(refs))}\n",
    "    res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "    metrics = {}\n",
    "    try:\n",
    "        from pycocoevalcap.bleu.bleu import Bleu\n",
    "        bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "        metrics['bleu1'] = float(bleu_scores[0])\n",
    "        metrics['bleu4'] = float(bleu_scores[3])\n",
    "    except Exception as e:\n",
    "        print('[WARN] BLEU failed:', e)\n",
    "    try:\n",
    "        from pycocoevalcap.cider.cider import Cider\n",
    "        cider_score, _ = Cider().compute_score(gts, res)\n",
    "        metrics['cider'] = float(cider_score)\n",
    "    except Exception as e:\n",
    "        print('[WARN] CIDEr failed:', e)\n",
    "    return metrics\n",
    "\n",
    "print(\"[TRAIN] Starting training with CIDEr-based checkpointing + patience.\")\n",
    "\n",
    "global_step = 0\n",
    "stopped_early = False\n",
    "for epoch in range(cfg.epochs):\n",
    "    model_mm.train()\n",
    "    train_sum = 0.0\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        imgs, srcs, tgts = batch\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                loss = out.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model_mm(imgs, srcs, tgts); loss = out.loss\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_sum += loss.item()\n",
    "        global_step += 1\n",
    "    train_epoch_loss = train_sum / max(1, len(train_loader))\n",
    "\n",
    "    # Validation loss (for monitoring only)\n",
    "    val_epoch_loss = None\n",
    "    if val_loader:\n",
    "        model_mm.eval(); v = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                if amp_dtype is not None:\n",
    "                    with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                        out = model_mm(imgs, srcs, tgts)\n",
    "                else:\n",
    "                    out = model_mm(imgs, srcs, tgts)\n",
    "                v += out.loss.item()\n",
    "        val_epoch_loss = v / max(1, len(val_loader))\n",
    "\n",
    "    # Caption metrics\n",
    "    metrics = compute_val_caption_metrics(model_mm, val_dataset, images_root, batch_size=16, amp_dtype=amp_dtype)\n",
    "    cider = metrics.get('cider', float('-inf'))\n",
    "    bleu1 = metrics.get('bleu1', float('nan'))\n",
    "    bleu4 = metrics.get('bleu4', float('nan'))\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # Print summary line\n",
    "    if val_epoch_loss is not None:\n",
    "        print(f\"Epoch {epoch+1:02d}/{cfg.epochs} train_loss={train_epoch_loss:.4f} val_loss={val_epoch_loss:.4f} BLEU1={bleu1:.3f} BLEU4={bleu4:.3f} CIDEr={cider:.3f} time={dt:.1f}s lr={current_lr:.2e}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:02d}/{cfg.epochs} train_loss={train_epoch_loss:.4f} BLEU1={bleu1:.3f} BLEU4={bleu4:.3f} CIDEr={cider:.3f} time={dt:.1f}s lr={current_lr:.2e}\")\n",
    "\n",
    "    # W&B logging\n",
    "    if use_wandb:\n",
    "        log_dict = {\n",
    "            'train/epoch_loss': train_epoch_loss,\n",
    "            'lr': current_lr,\n",
    "            'val/bleu1': bleu1,\n",
    "            'val/bleu4': bleu4,\n",
    "            'val/cider': cider,\n",
    "        }\n",
    "        if val_epoch_loss is not None:\n",
    "            log_dict['val/epoch_loss'] = val_epoch_loss\n",
    "        wandb.log(log_dict, step=epoch)\n",
    "\n",
    "    # Checkpoint by CIDEr + update patience counters\n",
    "    improved_cider = (cider - best_cider) > cider_min_delta\n",
    "    if improved_cider:\n",
    "        best_cider = cider\n",
    "        cider_stall_epochs = 0\n",
    "        save_payload = {\n",
    "            'model': model_mm.state_dict(),\n",
    "            'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_cider': best_cider,\n",
    "            'bleu1': bleu1,\n",
    "            'bleu4': bleu4,\n",
    "            'val_loss': val_epoch_loss,\n",
    "            'train_loss': train_epoch_loss,\n",
    "            'global_step': global_step,\n",
    "        }\n",
    "        torch.save(save_payload, os.path.join(CKPT_DIR, BEST_CKPT_NAME))\n",
    "        print(f\"  -> Saved {BEST_CKPT_NAME} (CIDEr={best_cider:.3f})\")\n",
    "    else:\n",
    "        cider_stall_epochs += 1\n",
    "        print(f\"  -> No CIDEr improvement (stall epochs={cider_stall_epochs}/{cider_patience})\")\n",
    "\n",
    "    # Patience early stopping\n",
    "    if cider_stall_epochs >= cider_patience:\n",
    "        print(f\"[Early Stop] CIDEr not improved for {cider_patience} consecutive epochs (min_delta={cider_min_delta}).\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "print(f\"Training finished. Best CIDEr={best_cider:.4f} {'(early stop)' if stopped_early else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Model Export / Import Utilities ===\n",
    "import os, json, torch, math\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# ------------------------------\n",
    "# Export\n",
    "# ------------------------------\n",
    "\n",
    "def export_model(\n",
    "    save_dir: str,\n",
    "    model: torch.nn.Module,\n",
    "    cfg_obj,\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "    epoch: Optional[int] = None,\n",
    "    global_step: Optional[int] = None,\n",
    "    best_val: Optional[float] = None,\n",
    "    tag: str = \"latest\",\n",
    "    use_safetensors: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Export model checkpoint + (optional) optimizer/scheduler.\n",
    "\n",
    "    Saves:\n",
    "      - config.json (raw cfg values)\n",
    "      - tokenizer/ (HF tokenizer)\n",
    "      - clip_mt5_prefix_<tag>.pt (bundle) OR .safetensors (+ meta files)\n",
    "\n",
    "    Bundle (.pt) contains:\n",
    "      model_state, cfg, epoch, global_step, tag, export_time,\n",
    "      optimizer_state?, scheduler_state?, best_val?, hyperparams.\n",
    "\n",
    "    hyperparams dict added so retraining can auto‑rebuild optimizer/scheduler:\n",
    "      { lr, clip_lr_scale, weight_decay, warmup_steps, grad_clip, use_amp, use_bf16 }\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Collect base config keys if available\n",
    "    if 'base_config' in globals():\n",
    "        cfg_dict = {k: getattr(cfg_obj, k) for k in base_config.keys() if hasattr(cfg_obj, k)}\n",
    "    else:\n",
    "        cfg_dict = {k: v for k, v in vars(cfg_obj).items() if not k.startswith('_')}\n",
    "\n",
    "    # Persist config separately (human readable)\n",
    "    with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(cfg_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save tokenizer (best effort)\n",
    "    try:\n",
    "        model.tokenizer.save_pretrained(os.path.join(save_dir, 'tokenizer'))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Tokenizer save failed: {e}\")\n",
    "\n",
    "    checkpoint_name = f\"clip_mt5_prefix_{tag}\"\n",
    "\n",
    "    hyperparams = {\n",
    "        'lr': cfg_dict.get('lr'),\n",
    "        'clip_lr_scale': cfg_dict.get('clip_lr_scale'),\n",
    "        'weight_decay': cfg_dict.get('weight_decay'),\n",
    "        'warmup_steps': cfg_dict.get('warmup_steps'),\n",
    "        'grad_clip': cfg_dict.get('grad_clip'),\n",
    "        'use_amp': cfg_dict.get('use_amp'),\n",
    "        'use_bf16': cfg_dict.get('use_bf16'),\n",
    "        'batch_size': cfg_dict.get('batch_size'),\n",
    "    }\n",
    "\n",
    "    if use_safetensors:\n",
    "        try:\n",
    "            from safetensors.torch import save_file\n",
    "            weights = model.state_dict()\n",
    "            save_file(weights, os.path.join(save_dir, checkpoint_name + '.safetensors'))\n",
    "            meta = {\n",
    "                'cfg': cfg_dict,\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'export_time': datetime.utcnow().isoformat() + 'Z',\n",
    "                'tag': tag,\n",
    "                'best_val': best_val,\n",
    "                'has_optimizer': optimizer is not None,\n",
    "                'has_scheduler': scheduler is not None,\n",
    "                'hyperparams': hyperparams,\n",
    "            }\n",
    "            if optimizer:\n",
    "                torch.save(optimizer.state_dict(), os.path.join(save_dir, checkpoint_name + '.optimizer.pt'))\n",
    "            if scheduler:\n",
    "                torch.save(scheduler.state_dict(), os.path.join(save_dir, checkpoint_name + '.scheduler.pt'))\n",
    "            with open(os.path.join(save_dir, 'meta.json'), 'w') as f:\n",
    "                json.dump(meta, f, indent=2)\n",
    "            print(f\"[EXPORT] Weights -> {checkpoint_name}.safetensors (meta.json written)\")\n",
    "            return os.path.join(save_dir, checkpoint_name + '.safetensors')\n",
    "        except ImportError:\n",
    "            print('[WARN] safetensors not installed; falling back to .pt')\n",
    "\n",
    "    # Standard .pt route\n",
    "    bundle = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'cfg': cfg_dict,\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'export_time': datetime.utcnow().isoformat() + 'Z',\n",
    "        'tag': tag,\n",
    "        'best_val': best_val,\n",
    "        'hyperparams': hyperparams,\n",
    "    }\n",
    "    if optimizer:\n",
    "        bundle['optimizer_state'] = optimizer.state_dict()\n",
    "    if scheduler:\n",
    "        bundle['scheduler_state'] = scheduler.state_dict()\n",
    "    out_path = os.path.join(save_dir, checkpoint_name + '.pt')\n",
    "    torch.save(bundle, out_path)\n",
    "    print(f\"[EXPORT] Saved checkpoint: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# ------------------------------\n",
    "# Helper: build optimizer (main + clip groups)\n",
    "# ------------------------------\n",
    "\n",
    "def build_optimizer_from_hparams(model: torch.nn.Module, h: Dict[str, Any]):\n",
    "    lr = h.get('lr', 1e-4)\n",
    "    clip_lr_scale = h.get('clip_lr_scale', 0.05) or 0.05\n",
    "    weight_decay = h.get('weight_decay', 0.0)\n",
    "    main_params, clip_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (clip_params if n.startswith('clip.') else main_params).append(p)\n",
    "    param_groups = []\n",
    "    if main_params:\n",
    "        param_groups.append({'params': main_params, 'lr': lr})\n",
    "    if clip_params:\n",
    "        param_groups.append({'params': clip_params, 'lr': lr * clip_lr_scale})\n",
    "    if clip_params:\n",
    "        print(f\"[OPT] CLIP params: {len(clip_params)} lr={lr * clip_lr_scale:.2e}\")\n",
    "    return torch.optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "\n",
    "# ------------------------------\n",
    "# Helper: build cosine scheduler with warmup (same as training)\n",
    "# ------------------------------\n",
    "\n",
    "def build_scheduler_from_hparams(optimizer, h: Dict[str, Any], steps_per_epoch: int, total_epochs: int):\n",
    "    warmup_steps = h.get('warmup_steps', 0) or 0\n",
    "    total_steps = steps_per_epoch * total_epochs\n",
    "    def lr_lambda(step):\n",
    "        if warmup_steps > 0 and step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps)) if total_steps > warmup_steps else 1.0\n",
    "        progress = min(max(progress, 0.0), 1.0)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# ------------------------------\n",
    "# Import for finetune (unchanged except now returns hyperparams)\n",
    "# ------------------------------\n",
    "\n",
    "def load_model_for_finetune(\n",
    "    load_dir: str,\n",
    "    device: torch.device,\n",
    "    checkpoint_tag: str = 'latest',\n",
    "    resume_optimizer: bool = True,\n",
    "    build_optimizer_fn=None,\n",
    "    build_scheduler_fn=None,\n",
    "    override_cfg: Optional[Dict[str, Any]] = None,\n",
    "    prefer_safetensors: bool = True\n",
    "):\n",
    "    # Load config\n",
    "    with open(os.path.join(load_dir, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "        cfg_json = json.load(f)\n",
    "    if override_cfg:\n",
    "        cfg_json.update(override_cfg)\n",
    "\n",
    "    class _Cfg: ...\n",
    "    cfg_obj = _Cfg()\n",
    "    for k, v in cfg_json.items():\n",
    "        setattr(cfg_obj, k, v)\n",
    "\n",
    "    model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "\n",
    "    ckpt_base = f\"clip_mt5_prefix_{checkpoint_tag}\"\n",
    "    safepath = os.path.join(load_dir, ckpt_base + '.safetensors')\n",
    "    ptpath = os.path.join(load_dir, ckpt_base + '.pt')\n",
    "\n",
    "    optimizer_state = scheduler_state = None\n",
    "    epoch = global_step = None\n",
    "    hyperparams = None\n",
    "    used_safetensors = False\n",
    "\n",
    "    if prefer_safetensors and os.path.isfile(safepath):\n",
    "        try:\n",
    "            from safetensors.torch import load_file\n",
    "            weights = load_file(safepath, device=device)\n",
    "            model.load_state_dict(weights, strict=True)\n",
    "            used_safetensors = True\n",
    "            meta_path = os.path.join(load_dir, 'meta.json')\n",
    "            meta = {}\n",
    "            if os.path.isfile(meta_path):\n",
    "                with open(meta_path, 'r') as f:\n",
    "                    meta = json.load(f)\n",
    "            epoch = meta.get('epoch'); global_step = meta.get('global_step')\n",
    "            hyperparams = meta.get('hyperparams')\n",
    "            if resume_optimizer and meta.get('has_optimizer'):\n",
    "                opt_file = os.path.join(load_dir, ckpt_base + '.optimizer.pt')\n",
    "                if os.path.isfile(opt_file):\n",
    "                    optimizer_state = torch.load(opt_file, map_location='cpu')\n",
    "            if resume_optimizer and meta.get('has_scheduler'):\n",
    "                sch_file = os.path.join(load_dir, ckpt_base + '.scheduler.pt')\n",
    "                if os.path.isfile(sch_file):\n",
    "                    scheduler_state = torch.load(sch_file, map_location='cpu')\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] safetensors load failed ({e}); falling back to .pt\")\n",
    "            used_safetensors = False\n",
    "\n",
    "    if not used_safetensors:\n",
    "        if not os.path.isfile(ptpath):\n",
    "            raise FileNotFoundError(f\"No checkpoint found at {ptpath}\")\n",
    "        bundle = torch.load(ptpath, map_location=device)\n",
    "        model.load_state_dict(bundle['model_state'], strict=True)\n",
    "        epoch = bundle.get('epoch'); global_step = bundle.get('global_step')\n",
    "        hyperparams = bundle.get('hyperparams')\n",
    "        if resume_optimizer:\n",
    "            optimizer_state = bundle.get('optimizer_state')\n",
    "            scheduler_state = bundle.get('scheduler_state')\n",
    "\n",
    "    print(f\"[IMPORT] Model loaded (epoch={epoch}, global_step={global_step})\")\n",
    "    return model, cfg_obj, hyperparams, optimizer_state, scheduler_state, epoch, global_step\n",
    "\n",
    "# ------------------------------\n",
    "# Simple one-shot retrain loader\n",
    "# ------------------------------\n",
    "\n",
    "def load_model_for_retrain(\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    new_lr: Optional[float] = None,\n",
    "    new_clip_lr_scale: Optional[float] = None,\n",
    "    reset_optimizer: bool = True,\n",
    "    freeze_clip: Optional[bool] = None,\n",
    "    override_cfg: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"Load a .pt bundle and prepare model + (fresh) optimizer hyperparams for retraining.\n",
    "\n",
    "    Returns (model, cfg_obj, optimizer_hparams, epoch_loaded, global_step_loaded)\n",
    "    Caller should then: optimizer = build_optimizer_from_hparams(model, optimizer_hparams)\n",
    "    and build a new scheduler with build_scheduler_from_hparams once steps_per_epoch known.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "    bundle = torch.load(checkpoint_path, map_location=device)\n",
    "    if 'model_state' not in bundle:\n",
    "        raise ValueError('Not an export_model bundle (.pt)')\n",
    "    cfg_json = dict(bundle['cfg'])\n",
    "    if override_cfg:\n",
    "        cfg_json.update(override_cfg)\n",
    "\n",
    "    class _Cfg: ...\n",
    "    cfg_obj = _Cfg()\n",
    "    for k, v in cfg_json.items():\n",
    "        setattr(cfg_obj, k, v)\n",
    "    if freeze_clip is not None:\n",
    "        cfg_obj.freeze_clip = freeze_clip\n",
    "\n",
    "    model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "    model.load_state_dict(bundle['model_state'], strict=True)\n",
    "\n",
    "    base_h = bundle.get('hyperparams', {})\n",
    "    # Override learning rates if requested\n",
    "    if new_lr is not None:\n",
    "        base_h['lr'] = new_lr\n",
    "    if new_clip_lr_scale is not None:\n",
    "        base_h['clip_lr_scale'] = new_clip_lr_scale\n",
    "\n",
    "    # If freezing clip newly, we still keep clip_lr_scale but it won't matter\n",
    "    epoch_loaded = bundle.get('epoch')\n",
    "    global_step_loaded = bundle.get('global_step')\n",
    "    print(f\"[RETRAIN] Loaded weights (epoch={epoch_loaded}). Preparing fresh optimizer hyperparams.\")\n",
    "    return model, cfg_obj, base_h, epoch_loaded, global_step_loaded\n",
    "\n",
    "print('[READY] Enhanced export/import + retrain helpers available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "export_model(\"/kaggle/working/exports/run1\", model_mm, cfg, optimizer, scheduler, epoch=epoch, global_step=global_step, tag=\"epoch_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # --- Validation Subset Metrics (pycocoevalcap) Helper ---\n",
    "# # Use to compute BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE on a fixed subset (default 200) of validation images each epoch.\n",
    "# # Call inside training/resume loops after computing (train/val) losses.\n",
    "\n",
    "# def compute_val_subset_metrics(\n",
    "#     model,\n",
    "#     val_dataset,\n",
    "#     images_root: str,\n",
    "#     device,\n",
    "#     amp_dtype=None,\n",
    "#     sample_size: int = 200,\n",
    "#     seed: int = 42,\n",
    "#     batch_size: int = 8,\n",
    "#     verbose: bool = False,\n",
    "# ):\n",
    "#     import random, time, os, torch\n",
    "#     from collections import defaultdict\n",
    "#     if val_dataset is None or len(val_dataset) == 0:\n",
    "#         return {}\n",
    "\n",
    "#     # Build refs map (filename -> list[captions]) based on dataset.samples (filename, caption)\n",
    "#     refs_map = defaultdict(list)\n",
    "#     for (fname, cap) in val_dataset.samples:\n",
    "#         c = cap.strip()\n",
    "#         if c:\n",
    "#             refs_map[fname].append(c)\n",
    "#     unique_files = list(refs_map.keys())\n",
    "#     if not unique_files:\n",
    "#         return {}\n",
    "#     random.Random(seed).shuffle(unique_files)\n",
    "#     subset_files = unique_files[: min(sample_size, len(unique_files))]\n",
    "\n",
    "#     image_paths = [os.path.join(images_root, f) for f in subset_files]\n",
    "\n",
    "#     hyps, refs = [], []\n",
    "#     model.eval()\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     def _gen(paths):\n",
    "#         if amp_dtype is not None:\n",
    "#             with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "#                 return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "#         return model.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "#     for i in range(0, len(image_paths), batch_size):\n",
    "#         batch = image_paths[i:i+batch_size]\n",
    "#         outs = _gen(batch)\n",
    "#         for p, pred in zip(batch, outs):\n",
    "#             fname = os.path.basename(p)\n",
    "#             hyps.append(pred if pred else \"<EMPTY>\")\n",
    "#             refs.append(refs_map.get(fname, [\" \"]))\n",
    "\n",
    "#     gts = {i: refs[i] for i in range(len(refs))}\n",
    "#     res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "#     from pycocoevalcap.bleu.bleu import Bleu\n",
    "#     from pycocoevalcap.meteor.meteor import Meteor\n",
    "#     from pycocoevalcap.rouge.rouge import Rouge\n",
    "#     from pycocoevalcap.cider.cider import Cider\n",
    "#     try:\n",
    "#         from pycocoevalcap.spice.spice import Spice\n",
    "#     except Exception:\n",
    "#         Spice = None\n",
    "\n",
    "#     metrics = {}\n",
    "#     try:\n",
    "#         bleu_scores, _ = Bleu(4).compute_score(gts, res)\n",
    "#         metrics['bleu1'] = float(bleu_scores[0]); metrics['bleu2'] = float(bleu_scores[1])\n",
    "#         metrics['bleu3'] = float(bleu_scores[2]); metrics['bleu4'] = float(bleu_scores[3])\n",
    "#         metrics['bleu'] = float(bleu_scores[3])\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] BLEU failed:', e)\n",
    "#     try:\n",
    "#         meteor_score, _ = Meteor().compute_score(gts, res)\n",
    "#         metrics['meteor'] = float(meteor_score)\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] METEOR failed:', e)\n",
    "#     try:\n",
    "#         rouge_score, _ = Rouge().compute_score(gts, res)\n",
    "#         metrics['rougeL'] = float(rouge_score)\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] ROUGE-L failed:', e)\n",
    "#     try:\n",
    "#         cider_score, _ = Cider().compute_score(gts, res)\n",
    "#         metrics['cider'] = float(cider_score)\n",
    "#     except Exception as e:\n",
    "#         if verbose: print('[VAL_SUBSET] CIDEr failed:', e)\n",
    "#     if Spice:\n",
    "#         try:\n",
    "#             spice_score, _ = Spice().compute_score(gts, res)\n",
    "#             metrics['spice'] = float(spice_score)\n",
    "#         except Exception as e:\n",
    "#             if verbose: print('[VAL_SUBSET] SPICE failed:', e)\n",
    "\n",
    "#     metrics['subset_size'] = len(subset_files)\n",
    "#     metrics['eval_time_s'] = round(time.time() - t0, 2)\n",
    "#     if verbose:\n",
    "#         print('[VAL_SUBSET]', ', '.join(f\"{k}={v:.3f}\" for k,v in metrics.items() if isinstance(v,(int,float))))\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # === Direct .pt Checkpoint Loader (file-based) ===\n",
    "# import torch, os, json\n",
    "# from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# def load_model_from_checkpoint_file(\n",
    "#     checkpoint_path: str,\n",
    "#     device: torch.device,\n",
    "#     resume_optimizer: bool = False,\n",
    "#     build_optimizer_fn=None,\n",
    "#     build_scheduler_fn=None,\n",
    "#     override_cfg: Optional[Dict[str, Any]] = None,\n",
    "# ):\n",
    "#     \"\"\"Load model (and optional optimizer/scheduler) directly from a single .pt file.\n",
    "\n",
    "#     Expects the .pt bundle produced by export_model (contains 'model_state' and 'cfg').\n",
    "\n",
    "#     Returns: (model, cfg_obj, optimizer, scheduler, epoch, global_step)\n",
    "#     \"\"\"\n",
    "#     assert os.path.isfile(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "#     bundle = torch.load(checkpoint_path, map_location=device)\n",
    "#     if 'model_state' not in bundle or 'cfg' not in bundle:\n",
    "#         raise ValueError(\"Checkpoint missing required keys 'model_state' or 'cfg'\")\n",
    "\n",
    "#     cfg_json = dict(bundle['cfg'])\n",
    "#     if override_cfg:\n",
    "#         cfg_json.update(override_cfg)\n",
    "\n",
    "#     class _Cfg: ...\n",
    "#     cfg_obj = _Cfg()\n",
    "#     for k, v in cfg_json.items():\n",
    "#         setattr(cfg_obj, k, v)\n",
    "\n",
    "#     model = CLIPmT5Pipeline(cfg_obj).to(device)\n",
    "#     model.load_state_dict(bundle['model_state'], strict=True)\n",
    "\n",
    "#     epoch = bundle.get('epoch')\n",
    "#     global_step = bundle.get('global_step')\n",
    "\n",
    "#     optimizer = None\n",
    "#     scheduler = None\n",
    "#     if resume_optimizer and 'optimizer_state' in bundle:\n",
    "#         if build_optimizer_fn is None:\n",
    "#             print('[WARN] Optimizer state present but build_optimizer_fn not provided; skipping optimizer restore.')\n",
    "#         else:\n",
    "#             optimizer = build_optimizer_fn(cfg_obj, model)\n",
    "#             try:\n",
    "#                 optimizer.load_state_dict(bundle['optimizer_state'])\n",
    "#                 print('[IMPORT] Optimizer state restored.')\n",
    "#             except Exception as e:\n",
    "#                 print(f'[WARN] Failed to load optimizer state: {e}')\n",
    "#     if resume_optimizer and optimizer and 'scheduler_state' in bundle and build_scheduler_fn:\n",
    "#         scheduler = build_scheduler_fn(cfg_obj, optimizer)\n",
    "#         try:\n",
    "#             scheduler.load_state_dict(bundle['scheduler_state'])\n",
    "#             print('[IMPORT] Scheduler state restored.')\n",
    "#         except Exception as e:\n",
    "#             print(f'[WARN] Failed to load scheduler state: {e}')\n",
    "\n",
    "#     print(f\"[IMPORT] Loaded model from file: {checkpoint_path}\")\n",
    "#     return model, cfg_obj, optimizer, scheduler, epoch, global_step\n",
    "\n",
    "# # One-line usage example (inference only):\n",
    "# # model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt', device=torch.device('cuda'), resume_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model_mm, cfg_loaded, _, _, epoch_loaded, step_loaded = load_model_from_checkpoint_file('/kaggle/input/25_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt', device=torch.device('cuda'), resume_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Testing / Inference cell (evaluate test set + pycocoevalcap metrics only)\n",
    "# import torch, time, os, json, math\n",
    "# from typing import List\n",
    "# from PIL import Image\n",
    "\n",
    "# assert torch.cuda.is_available(), \"CUDA GPU required for inference.\"\n",
    "# model_mm.eval()\n",
    "# device = torch.device('cuda')\n",
    "# model_mm.to(device)\n",
    "\n",
    "# # Determine AMP dtype consistent with training\n",
    "# amp_dtype = None\n",
    "# if getattr(cfg, 'use_amp', True):\n",
    "#     if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "#         amp_dtype = torch.bfloat16\n",
    "#     else:\n",
    "#         amp_dtype = torch.float16\n",
    "\n",
    "# # -------------------- Test Loss --------------------\n",
    "# test_loss = None\n",
    "# if test_loader:\n",
    "#     t0 = time.time(); total=0.0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             imgs, srcs, tgts = batch\n",
    "#             imgs = imgs.to(device, non_blocking=True)\n",
    "#             if amp_dtype is not None:\n",
    "#                 with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "#                     out = model_mm(imgs, srcs, tgts)\n",
    "#             else:\n",
    "#                 out = model_mm(imgs, srcs, tgts)\n",
    "#             total += out.loss.item()\n",
    "#     test_loss = total / max(1,len(test_loader))\n",
    "#     print(f\"Test loss={test_loss:.4f} time={(time.time()-t0):.1f}s\")\n",
    "# else:\n",
    "#     print(\"No test split available.\")\n",
    "\n",
    "# ############################################\n",
    "# # Caption Quality Metrics (BLEU1-4, METEOR, CIDEr, ROUGE-L, SPICE) + W&B logging\n",
    "# # All via pycocoevalcap (no evaluate dependency)\n",
    "# ############################################\n",
    "# metrics = {}\n",
    "# try:\n",
    "#     import nltk\n",
    "#     for pkg in ['punkt', 'wordnet', 'omw-1.4']:\n",
    "#         try:\n",
    "#             nltk.data.find(f'tokenizers/{pkg}')\n",
    "#         except Exception:\n",
    "#             try:\n",
    "#                 nltk.download(pkg, quiet=True)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "\n",
    "#     # Build reference sets (all captions per test image) from original JSON\n",
    "#     with open(json_path) as f:\n",
    "#         data_json = json.load(f)\n",
    "#     img_entries = data_json['images'] if isinstance(data_json, dict) else data_json\n",
    "\n",
    "#     # Gather unique test image filenames present in test_dataset\n",
    "#     test_image_names = sorted({s[0] for s in test_dataset.samples})\n",
    "#     refs_map = {}\n",
    "#     for entry in img_entries:\n",
    "#         if not isinstance(entry, dict):\n",
    "#             continue\n",
    "#         fname = entry.get('filename')\n",
    "#         if fname in test_image_names:\n",
    "#             all_caps = []\n",
    "#             for s in entry.get('sentences', []):\n",
    "#                 if isinstance(s, dict) and 'raw' in s:\n",
    "#                     cap = s['raw'].strip()\n",
    "#                     if cap:\n",
    "#                         all_caps.append(cap)\n",
    "#             if all_caps:\n",
    "#                 refs_map[fname] = all_caps\n",
    "\n",
    "#     # Generate hypotheses (batched for speed)\n",
    "#     hyps = []\n",
    "#     refs = []\n",
    "#     BATCH_GEN = 12  # moderate batch size; adjust as GPU allows\n",
    "#     image_paths = [os.path.join(images_root, fname) for fname in test_image_names]\n",
    "\n",
    "#     def _generate_batch(paths):\n",
    "#         if amp_dtype is not None:\n",
    "#             with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "#                 return model_mm.generate(image_paths=paths, ban_sentinels=True)\n",
    "#         return model_mm.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "#     gen_start = time.time()\n",
    "#     for i in range(0, len(image_paths), BATCH_GEN):\n",
    "#         batch_paths = image_paths[i:i+BATCH_GEN]\n",
    "#         caps = _generate_batch(batch_paths)\n",
    "#         for p, pred in zip(batch_paths, caps):\n",
    "#             fname = os.path.basename(p)\n",
    "#             pred = pred if pred else \"<EMPTY>\"\n",
    "#             hyps.append(pred)\n",
    "#             # list of refs (avoid empties)\n",
    "#             cur_refs = refs_map.get(fname, [\" \"])\n",
    "#             refs.append([r for r in cur_refs if r.strip()] or [\" \"])\n",
    "#     print(f\"[GEN] Generated {len(hyps)} captions in {time.time()-gen_start:.1f}s\")\n",
    "\n",
    "#     # Build COCO-style dicts for pycocoevalcap\n",
    "#     gts = {i: refs[i] for i in range(len(refs))}\n",
    "#     res = {i: [hyps[i]] for i in range(len(hyps))}\n",
    "\n",
    "#     # ---- pycocoevalcap metrics ----\n",
    "#     from pycocoevalcap.bleu.bleu import Bleu\n",
    "#     from pycocoevalcap.meteor.meteor import Meteor\n",
    "#     from pycocoevalcap.rouge.rouge import Rouge\n",
    "#     from pycocoevalcap.cider.cider import Cider\n",
    "#     from pycocoevalcap.spice.spice import Spice\n",
    "\n",
    "#     metrics = {}\n",
    "#     # BLEU (returns list of 4 scores)\n",
    "#     try:\n",
    "#         bleu_scorer = Bleu(4)\n",
    "#         bleu_scores, _ = bleu_scorer.compute_score(gts, res)\n",
    "#         metrics['bleu1'] = float(bleu_scores[0])\n",
    "#         metrics['bleu2'] = float(bleu_scores[1])\n",
    "#         metrics['bleu3'] = float(bleu_scores[2])\n",
    "#         metrics['bleu4'] = float(bleu_scores[3])\n",
    "#         metrics['bleu'] = float(bleu_scores[3])  # treat BLEU-4 as aggregate\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] BLEU failed:', e)\n",
    "\n",
    "#     try:\n",
    "#         meteor_scorer = Meteor()\n",
    "#         meteor_score, _ = meteor_scorer.compute_score(gts, res)\n",
    "#         metrics['meteor'] = float(meteor_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] METEOR failed:', e)\n",
    "\n",
    "#     try:\n",
    "#         rouge_scorer = Rouge()\n",
    "#         rouge_score, _ = rouge_scorer.compute_score(gts, res)\n",
    "#         metrics['rougeL'] = float(rouge_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] ROUGE-L failed:', e)\n",
    "\n",
    "#     try:\n",
    "#         cider_scorer = Cider()\n",
    "#         cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "#         metrics['cider'] = float(cider_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] CIDEr failed:', e)\n",
    "\n",
    "#     try:\n",
    "#         spice_scorer = Spice()\n",
    "#         spice_score, spice_scores = spice_scorer.compute_score(gts, res)\n",
    "#         metrics['spice'] = float(spice_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] SPICE failed (Java required?):', e)\n",
    "\n",
    "#     if test_loss is not None:\n",
    "#         metrics['test_loss'] = test_loss\n",
    "\n",
    "#     print(\"=== Caption Metrics (pycocoevalcap) ===\")\n",
    "#     for k,v in metrics.items():\n",
    "#         print(f\"{k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "#     if use_wandb:\n",
    "#         wandb.log({f\"eval/{k}\": v for k,v in metrics.items()}, commit=True)\n",
    "#         print(\"[wandb] Logged caption metrics.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"[ERROR] Metric computation failed:\", e)\n",
    "\n",
    "# # Show a few sample generated captions from first N test images with diversity controls\n",
    "# default_gen_kwargs = dict(\n",
    "#     repetition_penalty=1.2,\n",
    "#     no_repeat_ngram_size=3,\n",
    "# )\n",
    "# SAMPLE_PRINTS = 5\n",
    "# if test_loader and SAMPLE_PRINTS > 0:\n",
    "#     shown = 0\n",
    "#     printed_imgs = set()\n",
    "#     for img_path, _ in [(os.path.join(images_root, s[0]), s[1]) for s in test_dataset.samples]:\n",
    "#         if img_path in printed_imgs:\n",
    "#             continue\n",
    "#         if amp_dtype is not None:\n",
    "#             with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "#                 caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n",
    "#         else:\n",
    "#             caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n",
    "#         gt_caption = next(s[1] for s in test_dataset.samples if os.path.join(images_root, s[0]) == img_path)\n",
    "#         print('GT:', gt_caption)\n",
    "#         print('CAP:', caps[0])\n",
    "#         print('---------------')\n",
    "#         printed_imgs.add(img_path)\n",
    "#         shown += 1\n",
    "#         if shown >= SAMPLE_PRINTS:\n",
    "#             break\n",
    "\n",
    "# # Optional: stochastic sampling helper for more diverse outputs\n",
    "# def generate_captions(image_paths: List[str], mode='beam', **kwargs):\n",
    "#     if mode == 'sample':\n",
    "#         sample_defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "#         for k, v in sample_defaults.items():\n",
    "#             kwargs.setdefault(k, v)\n",
    "#     else:  # beam\n",
    "#         beam_defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3)\n",
    "#         for k, v in beam_defaults.items():\n",
    "#             kwargs.setdefault(k, v)\n",
    "#     if amp_dtype is not None:\n",
    "#         with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "#             return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "#     return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "\n",
    "# print(\"[Ready] generate_captions(['path/to/img.jpg'], mode='beam')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Single Image Inference with Optional pycocoevalcap Metrics (BLEU1-4, METEOR, ROUGE-L, CIDEr, SPICE)\n",
    "# from typing import Optional, Dict, Any, List\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os, json, torch\n",
    "\n",
    "# # Cache scorers globally to avoid repeated Java/METEOR/Spice init costs\n",
    "# _PYCOCO_SCORERS = {}\n",
    "\n",
    "\n",
    "# def _lazy_load_scorers():\n",
    "#     global _PYCOCO_SCORERS\n",
    "#     from pycocoevalcap.bleu.bleu import Bleu\n",
    "#     from pycocoevalcap.meteor.meteor import Meteor\n",
    "#     from pycocoevalcap.rouge.rouge import Rouge\n",
    "#     from pycocoevalcap.cider.cider import Cider\n",
    "#     try:\n",
    "#         from pycocoevalcap.spice.spice import Spice\n",
    "#     except Exception:\n",
    "#         Spice = None\n",
    "#     if 'bleu' not in _PYCOCO_SCORERS:\n",
    "#         _PYCOCO_SCORERS['bleu'] = Bleu(4)\n",
    "#     if 'meteor' not in _PYCOCO_SCORERS:\n",
    "#         _PYCOCO_SCORERS['meteor'] = Meteor()\n",
    "#     if 'rouge' not in _PYCOCO_SCORERS:\n",
    "#         _PYCOCO_SCORERS['rouge'] = Rouge()\n",
    "#     if 'cider' not in _PYCOCO_SCORERS:\n",
    "#         _PYCOCO_SCORERS['cider'] = Cider()\n",
    "#     if Spice and 'spice' not in _PYCOCO_SCORERS:\n",
    "#         _PYCOCO_SCORERS['spice'] = Spice()\n",
    "#     return _PYCOCO_SCORERS\n",
    "\n",
    "\n",
    "# def _compute_single_caption_metrics(pred: str, refs: List[str]) -> Dict[str, float]:\n",
    "#     refs_clean = [r.strip() for r in refs if r and r.strip()]\n",
    "#     if not refs_clean:\n",
    "#         return {}\n",
    "#     scorers = _lazy_load_scorers()\n",
    "#     gts = {0: refs_clean}\n",
    "#     res = {0: [pred]}\n",
    "#     out = {}\n",
    "#     # BLEU\n",
    "#     try:\n",
    "#         bleu_scores, _ = scorers['bleu'].compute_score(gts, res)\n",
    "#         out['bleu1'] = float(bleu_scores[0])\n",
    "#         out['bleu2'] = float(bleu_scores[1])\n",
    "#         out['bleu3'] = float(bleu_scores[2])\n",
    "#         out['bleu4'] = float(bleu_scores[3])\n",
    "#         out['bleu'] = float(bleu_scores[3])\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] BLEU failed:', e)\n",
    "#     # METEOR\n",
    "#     try:\n",
    "#         meteor_score, _ = scorers['meteor'].compute_score(gts, res)\n",
    "#         out['meteor'] = float(meteor_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] METEOR failed:', e)\n",
    "#     # ROUGE-L\n",
    "#     try:\n",
    "#         rouge_score, _ = scorers['rouge'].compute_score(gts, res)\n",
    "#         out['rougeL'] = float(rouge_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] ROUGE-L failed:', e)\n",
    "#     # CIDEr\n",
    "#     try:\n",
    "#         cider_score, _ = scorers['cider'].compute_score(gts, res)\n",
    "#         out['cider'] = float(cider_score)\n",
    "#     except Exception as e:\n",
    "#         print('[WARN] CIDEr failed:', e)\n",
    "#     # SPICE (may require Java)\n",
    "#     if 'spice' in scorers:\n",
    "#         try:\n",
    "#             spice_score, spice_scores = scorers['spice'].compute_score(gts, res)\n",
    "#             out['spice'] = float(spice_score)\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] SPICE failed:', e)\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def predict(\n",
    "#     image_path: str,\n",
    "#     prompt: str = \"Bu fotoğrafı açıkla: \",\n",
    "#     mode: str = \"beam\",              # 'beam' or 'sample'\n",
    "#     max_refs: Optional[int] = 5,\n",
    "#     show_image: bool = True,\n",
    "#     show_refs: bool = True,\n",
    "#     gen_kwargs: Optional[Dict[str, Any]] = None,\n",
    "#     json_file: Optional[str] = None,\n",
    "#     ban_sentinels: bool = True,\n",
    "#     compute_metrics: bool = True,\n",
    "#     print_metrics: bool = True,\n",
    "# ) -> Dict[str, Any]:\n",
    "#     \"\"\"Generate a caption and optionally compute pycocoevalcap metrics.\n",
    "\n",
    "#     Returns dict with: image_path, prediction, references (list), metrics (dict), mode.\n",
    "#     \"\"\"\n",
    "#     assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n",
    "#     jf = json_file or json_path  # use global json_path defined earlier\n",
    "\n",
    "#     # Collect reference captions\n",
    "#     refs: List[str] = []\n",
    "#     try:\n",
    "#         with open(jf) as f:\n",
    "#             data = json.load(f)\n",
    "#         entries = data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "#         target_name = os.path.basename(image_path)\n",
    "#         for e in entries:\n",
    "#             if not isinstance(e, dict):\n",
    "#                 continue\n",
    "#             if e.get('filename') == target_name:\n",
    "#                 for s in e.get('sentences', []):\n",
    "#                     if isinstance(s, dict) and 'raw' in s:\n",
    "#                         cap = s['raw'].strip()\n",
    "#                         if cap:\n",
    "#                             refs.append(cap)\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] Could not parse references ({e})\")\n",
    "\n",
    "#     if max_refs is not None:\n",
    "#         refs = refs[:max_refs]\n",
    "\n",
    "#     # Defaults for decoding\n",
    "#     gen_kwargs = (gen_kwargs or {}).copy()\n",
    "#     if mode == \"sample\":\n",
    "#         defaults = dict(temperature=0.8, top_p=0.9, do_sample=True,\n",
    "#                         repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "#     else:\n",
    "#         defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3,\n",
    "#                         num_beams=getattr(cfg, \"num_beams_infer\", 4))\n",
    "#     for k, v in defaults.items():\n",
    "#         gen_kwargs.setdefault(k, v)\n",
    "\n",
    "#     local_amp_dtype = globals().get(\"amp_dtype\", None)\n",
    "#     model_mm.eval()\n",
    "#     if local_amp_dtype is not None:\n",
    "#         with torch.amp.autocast('cuda', dtype=local_amp_dtype):\n",
    "#             pred = model_mm.generate(\n",
    "#                 image_paths=[image_path],\n",
    "#                 prompt=prompt,\n",
    "#                 ban_sentinels=ban_sentinels,\n",
    "#                 **gen_kwargs\n",
    "#             )[0]\n",
    "#     else:\n",
    "#         pred = model_mm.generate(\n",
    "#             image_paths=[image_path],\n",
    "#             prompt=prompt,\n",
    "#             ban_sentinels=ban_sentinels,\n",
    "#             **gen_kwargs\n",
    "#         )[0]\n",
    "\n",
    "#     metrics: Dict[str, float] = {}\n",
    "#     if compute_metrics and refs:\n",
    "#         try:\n",
    "#             metrics = _compute_single_caption_metrics(pred if pred else \"<EMPTY>\", refs)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] Metric computation failed: {e}\")\n",
    "\n",
    "#     # Visualization\n",
    "#     if show_image:\n",
    "#         try:\n",
    "#             img = Image.open(image_path).convert(\"RGB\")\n",
    "#             plt.figure(figsize=(5,5))\n",
    "#             plt.imshow(img); plt.axis(\"off\")\n",
    "#             plt.title(\"Image\")\n",
    "#             plt.show()\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] Could not display image ({e})\")\n",
    "\n",
    "#     if show_refs:\n",
    "#         if refs:\n",
    "#             print(\"References:\")\n",
    "#             for i, r in enumerate(refs, 1):\n",
    "#                 print(f\"  {i}. {r}\")\n",
    "#         else:\n",
    "#             print(\"(No references found)\")\n",
    "\n",
    "#     print(\"Prediction:\")\n",
    "#     print(\" \", pred)\n",
    "\n",
    "#     if print_metrics and metrics:\n",
    "#         print(\"\\nMetrics (pycocoevalcap):\")\n",
    "#         for k, v in metrics.items():\n",
    "#             print(f\"  {k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"  {k}: {v}\")\n",
    "\n",
    "#     return {\n",
    "#         \"image_path\": image_path,\n",
    "#         \"prediction\": pred,\n",
    "#         \"references\": refs,\n",
    "#         \"metrics\": metrics,\n",
    "#         \"mode\": mode,\n",
    "#     }\n",
    "\n",
    "# # Example:\n",
    "# # result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode=\"beam\")\n",
    "# # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Example: Retrain from exported checkpoint on same or new dataset ===\n",
    "# checkpoint_path = '/kaggle/input/your_export/clip_mt5_prefix_latest.pt'\n",
    "# from torch.utils.data import DataLoader\n",
    "# device = torch.device('cuda')\n",
    "# model_re, cfg_re, hparams, epoch_loaded, step_loaded = load_model_for_retrain(\n",
    "#     checkpoint_path,\n",
    "#     device=device,\n",
    "#     new_lr=5e-5,              # optionally override LR\n",
    "#     new_clip_lr_scale=0.02,   # optionally override CLIP LR scale\n",
    "#     freeze_clip=None,         # or True to freeze CLIP now\n",
    "# )\n",
    "# optimizer_re = build_optimizer_from_hparams(model_re, hparams)\n",
    "# steps_per_epoch = len(train_loader)  # after you rebuild train_loader for (same or new) dataset\n",
    "# scheduler_re = build_scheduler_from_hparams(optimizer_re, hparams, steps_per_epoch, total_epochs=5)\n",
    "# # Proceed with standard training loop using model_re / optimizer_re / scheduler_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ==== Resume / Continue Training From Exported or Training Checkpoint (.pt) ====\n",
    "# # Improvements:\n",
    "# #  - Ensures 'checkpoints' directory exists before saving.\n",
    "# #  - Handles tokenizer class mismatch notice (mt5 vs t5) by reloading MT5 tokenizer explicitly.\n",
    "# #  - Initializes best_val robustly (evaluates validation if inf).\n",
    "# #  - Safe scheduler positioning.\n",
    "# #  - Adds optional immediate validation before continuing to set baseline.\n",
    "\n",
    "# RESUME_PATH = '/kaggle/input/15_epochs_tasviret/pytorch/default/1/clip_mt5_prefix_epoch_last.pt'  # change as needed\n",
    "# EXTRA_EPOCHS = 5          # number of extra epochs to train\n",
    "# RUN_INITIAL_VAL = True    # run a validation pass right after load to set best_val if missing\n",
    "# SAVE_NAME = 'best_resumed.pt'\n",
    "\n",
    "# import os, math, time, torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# if not os.path.isfile(RESUME_PATH):\n",
    "#     print(f\"[RESUME] Path not found: {RESUME_PATH} -> Skip.\")\n",
    "# else:\n",
    "#     print(f\"[RESUME] Loading: {RESUME_PATH}\")\n",
    "#     bundle = torch.load(RESUME_PATH, map_location='cpu')\n",
    "\n",
    "#     has_export_format = 'model_state' in bundle\n",
    "#     has_train_ckpt_format = 'model' in bundle\n",
    "#     if not (has_export_format or has_train_ckpt_format):\n",
    "#         raise ValueError('Unrecognized checkpoint format.')\n",
    "\n",
    "#     cfg_dict = bundle.get('cfg') or {}\n",
    "#     class _Cfg: ...\n",
    "#     cfg_resume = _Cfg()\n",
    "#     for k, v in base_config.items():\n",
    "#         setattr(cfg_resume, k, cfg_dict.get(k, v))\n",
    "\n",
    "#     # Force consistent tokenizer type (MT5Tokenizer) regardless of original class\n",
    "#     model_resume = CLIPmT5Pipeline(cfg_resume).to(device)\n",
    "\n",
    "#     if has_export_format:\n",
    "#         model_resume.load_state_dict(bundle['model_state'], strict=True)\n",
    "#         loaded_epoch = bundle.get('epoch', -1)\n",
    "#         global_step_loaded = bundle.get('global_step', None)\n",
    "#         best_val_loaded = bundle.get('best_val', float('inf'))\n",
    "#     else:\n",
    "#         model_resume.load_state_dict(bundle['model'], strict=True)\n",
    "#         loaded_epoch = bundle.get('epoch', -1)\n",
    "#         global_step_loaded = bundle.get('global_step', None)\n",
    "#         best_val_loaded = bundle.get('best_val', float('inf'))\n",
    "\n",
    "#     print(f\"[RESUME] Weights loaded (epoch={loaded_epoch}, best_val={best_val_loaded})\")\n",
    "\n",
    "#     main_params, clip_params = [], []\n",
    "#     for n, p in model_resume.named_parameters():\n",
    "#         if not p.requires_grad: continue\n",
    "#         (clip_params if n.startswith('clip.') else main_params).append(p)\n",
    "#     param_groups = []\n",
    "#     if main_params: param_groups.append({'params': main_params, 'lr': cfg_resume.lr})\n",
    "#     if clip_params: param_groups.append({'params': clip_params, 'lr': cfg_resume.lr * getattr(cfg_resume, 'clip_lr_scale', 0.05)})\n",
    "#     optimizer_resume = AdamW(param_groups, weight_decay=cfg_resume.weight_decay)\n",
    "\n",
    "#     steps_per_epoch = len(train_loader)\n",
    "#     total_planned_epochs = loaded_epoch + 1 + EXTRA_EPOCHS\n",
    "#     total_steps = steps_per_epoch * total_planned_epochs\n",
    "\n",
    "#     def lr_lambda(step):\n",
    "#         if cfg_resume.warmup_steps > 0 and step < cfg_resume.warmup_steps:\n",
    "#             return float(step) / float(max(1, cfg_resume.warmup_steps))\n",
    "#         progress = (step - cfg_resume.warmup_steps) / float(max(1, total_steps - cfg_resume.warmup_steps))\n",
    "#         progress = min(max(progress, 0.0), 1.0)\n",
    "#         return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "#     scheduler_resume = LambdaLR(optimizer_resume, lr_lambda=lr_lambda)\n",
    "\n",
    "#     opt_key = 'optimizer_state' if has_export_format else 'optimizer'\n",
    "#     sch_key = 'scheduler_state' if has_export_format else 'scheduler'\n",
    "#     if opt_key in bundle:\n",
    "#         try:\n",
    "#             optimizer_resume.load_state_dict(bundle[opt_key])\n",
    "#             print('[RESUME] Optimizer state restored.')\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] Optimizer state load failed:', e)\n",
    "#     if sch_key in bundle:\n",
    "#         try:\n",
    "#             scheduler_resume.load_state_dict(bundle[sch_key])\n",
    "#             print('[RESUME] Scheduler state restored.')\n",
    "#         except Exception as e:\n",
    "#             print('[WARN] Scheduler state load failed:', e)\n",
    "\n",
    "#     if scheduler_resume.last_epoch < 0 and loaded_epoch >= 0:\n",
    "#         completed_steps = (loaded_epoch + 1) * steps_per_epoch\n",
    "#         scheduler_resume.last_epoch = completed_steps\n",
    "#         print(f\"[RESUME] Scheduler last_epoch set to {scheduler_resume.last_epoch}\")\n",
    "\n",
    "#     amp_dtype_resume = None\n",
    "#     if getattr(cfg_resume, 'use_amp', True):\n",
    "#         if getattr(cfg_resume, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "#             amp_dtype_resume = torch.bfloat16\n",
    "#         else:\n",
    "#             amp_dtype_resume = torch.float16\n",
    "#     scaler_resume = torch.amp.GradScaler('cuda', enabled=amp_dtype_resume is not None)\n",
    "\n",
    "#     start_epoch = loaded_epoch + 1\n",
    "#     end_epoch = loaded_epoch + EXTRA_EPOCHS\n",
    "\n",
    "#     best_val = best_val_loaded\n",
    "\n",
    "#     # Optional immediate validation to set best_val if it's inf or user wants baseline\n",
    "#     if RUN_INITIAL_VAL and (best_val == float('inf') or best_val != best_val):  # inf or NaN\n",
    "#         if val_loader:\n",
    "#             model_resume.eval(); v=0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in val_loader:\n",
    "#                     imgs, srcs, tgts = batch\n",
    "#                     imgs = imgs.to(device, non_blocking=True)\n",
    "#                     if amp_dtype_resume is not None:\n",
    "#                         with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                             out = model_resume(imgs, srcs, tgts)\n",
    "#                     else:\n",
    "#                         out = model_resume(imgs, srcs, tgts)\n",
    "#                     v += out.loss.item()\n",
    "#             best_val = v / max(1, len(val_loader))\n",
    "#             print(f\"[RESUME] Initial validation baseline best_val set to {best_val:.4f}\")\n",
    "#         else:\n",
    "#             best_val = float('inf')\n",
    "\n",
    "#     early_patience = getattr(cfg_resume, 'early_stop_patience', None)\n",
    "#     min_delta = getattr(cfg_resume, 'early_stop_min_delta', 0.0)\n",
    "#     _epochs_no_improve = 0\n",
    "\n",
    "#     global_step_resume = global_step_loaded if global_step_loaded is not None else (start_epoch * steps_per_epoch)\n",
    "\n",
    "#     print(f\"[RESUME] Continue training for {EXTRA_EPOCHS} more epochs: {start_epoch} -> {end_epoch}\")\n",
    "\n",
    "#     for epoch in range(start_epoch, end_epoch + 1):\n",
    "#         model_resume.train()\n",
    "#         sum_loss = 0.0\n",
    "#         t0 = time.time()\n",
    "#         for step, batch in enumerate(train_loader, start=1):\n",
    "#             imgs, srcs, tgts = batch\n",
    "#             imgs = imgs.to(device, non_blocking=True)\n",
    "#             optimizer_resume.zero_grad(set_to_none=True)\n",
    "#             if amp_dtype_resume is not None:\n",
    "#                 with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                     out = model_resume(imgs, srcs, tgts)\n",
    "#                     loss = out.loss\n",
    "#                 scaler_resume.scale(loss).backward()\n",
    "#                 if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n",
    "#                     scaler_resume.unscale_(optimizer_resume)\n",
    "#                     torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n",
    "#                 scaler_resume.step(optimizer_resume)\n",
    "#                 scaler_resume.update()\n",
    "#             else:\n",
    "#                 out = model_resume(imgs, srcs, tgts); loss = out.loss\n",
    "#                 loss.backward()\n",
    "#                 if cfg_resume.grad_clip and cfg_resume.grad_clip > 0:\n",
    "#                     torch.nn.utils.clip_grad_norm_(model_resume.parameters(), cfg_resume.grad_clip)\n",
    "#                 optimizer_resume.step()\n",
    "#             scheduler_resume.step()\n",
    "#             sum_loss += loss.item()\n",
    "#             global_step_resume += 1\n",
    "#         train_loss = sum_loss / max(1, len(train_loader))\n",
    "\n",
    "#         val_loss = None\n",
    "#         if val_loader:\n",
    "#             model_resume.eval(); v = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in val_loader:\n",
    "#                     imgs, srcs, tgts = batch\n",
    "#                     imgs = imgs.to(device, non_blocking=True)\n",
    "#                     if amp_dtype_resume is not None:\n",
    "#                         with torch.amp.autocast('cuda', dtype=amp_dtype_resume):\n",
    "#                             out = model_resume(imgs, srcs, tgts)\n",
    "                        \n",
    "#                     else:\n",
    "#                         out = model_resume(imgs, srcs, tgts)\n",
    "#                     v += out.loss.item()\n",
    "#             val_loss = v / max(1, len(val_loader))\n",
    "\n",
    "#         dt = time.time() - t0\n",
    "#         lr_cur = scheduler_resume.get_last_lr()[0]\n",
    "#         if val_loss is not None:\n",
    "#             print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} val={val_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n",
    "#         else:\n",
    "#             print(f\"[RESUME] Epoch {epoch} train={train_loss:.4f} time={dt:.1f}s lr={lr_cur:.2e}\")\n",
    "\n",
    "#         metric = val_loss if val_loss is not None else train_loss\n",
    "#         improved = metric < (best_val - min_delta)\n",
    "#         if improved:\n",
    "#             best_val = metric\n",
    "#             _epochs_no_improve = 0\n",
    "#             save_obj = {\n",
    "#                 'model': model_resume.state_dict(),\n",
    "#                 'cfg': {k: getattr(cfg_resume, k) for k in base_config.keys()},\n",
    "#                 'epoch': epoch,\n",
    "#                 'optimizer': optimizer_resume.state_dict(),\n",
    "#                 'scheduler': scheduler_resume.state_dict(),\n",
    "#                 'best_val': best_val,\n",
    "#                 'global_step': global_step_resume,\n",
    "#             }\n",
    "#             torch.save(save_obj, os.path.join('checkpoints', SAVE_NAME))\n",
    "#             print(f\"   -> [RESUME] Saved {SAVE_NAME} (metric={best_val:.4f})\")\n",
    "#         else:\n",
    "#             _epochs_no_improve += 1\n",
    "#             if early_patience is not None and _epochs_no_improve >= early_patience:\n",
    "#                 print(f\"[Early Stop - Resume] No improvement for {early_patience} epochs.\")\n",
    "#                 break\n",
    "\n",
    "#     print('[RESUME] Training extension finished. Final best_val=', best_val)\n",
    "#     model_mm = model_resume"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2808179,
     "sourceId": 4845244,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8193626,
     "sourceId": 12947536,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8200149,
     "sourceId": 12956829,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8223778,
     "sourceId": 12992543,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 442058,
     "modelInstanceId": 424569,
     "sourceId": 559988,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 444390,
     "modelInstanceId": 427385,
     "sourceId": 567111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
