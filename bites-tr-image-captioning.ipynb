{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP (ViT-B/32) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:01:20.413911Z",
     "iopub.status.busy": "2025-09-03T13:01:20.413650Z",
     "iopub.status.idle": "2025-09-03T13:01:20.418086Z",
     "shell.execute_reply": "2025-09-03T13:01:20.417441Z",
     "shell.execute_reply.started": "2025-09-03T13:01:20.413891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Rerun at each session start\n",
    "\n",
    "# # Download CLIP\n",
    "# !pip install -q git+https://github.com/openai/CLIP.git\n",
    "# !pip install -q transformers sentencepiece accelerate\n",
    "# !pip install -r kaggle/input/metric-requirements/metric_requirements.txt\n",
    "\n",
    "# # Download mT5-small ViT-B/32\n",
    "# import clip, torch\n",
    "# from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_name = \"google/mt5-small\"\n",
    "\n",
    "# tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "# mt5 = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:01:20.419262Z",
     "iopub.status.busy": "2025-09-03T13:01:20.418942Z",
     "iopub.status.idle": "2025-09-03T13:01:36.627682Z",
     "shell.execute_reply": "2025-09-03T13:01:36.626841Z",
     "shell.execute_reply.started": "2025-09-03T13:01:20.419245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Unified configuration + optional wandb init\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"bites-tr-image-captioning\"\n",
    "RUN_NAME = \"clip_mt5_prefix_run\"\n",
    "\n",
    "ENABLE_WANDB = True\n",
    "\n",
    "WANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/32\",  # backbone\n",
    "    \"prefix_tokens\": 32,          # stronger conditioning\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 10,                  # allow a bit longer now that we fine-tune CLIP\n",
    "    \"dataset_limit\": None,\n",
    "    # --- CLIP fine-tuning controls ---\n",
    "    \"freeze_clip\": False,          # set False to allow full CLIP fine-tuning\n",
    "    \"unfreeze_clip_last_n\": 0,     # if >0 unfreezes only last N vision blocks (overrides freeze_clip); 0 means ignore and use freeze_clip flag\n",
    "    \"clip_lr_scale\": 0.05,         # scaled LR for ALL CLIP params (set lower than main to avoid destroying pretrained space)\n",
    "    \"use_clip_patch_tokens\": True, # richer conditioning (patch tokens path)\n",
    "    # --- T5 freezing ---\n",
    "    \"freeze_t5_encoder\": False,    # unfreeze encoder so it can adapt to visual prefix distribution\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    # --- Optimization ---\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 500,           # linear warmup steps before cosine decay\n",
    "    # --- Inference defaults ---\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    # --- Early stopping hyperparameters ---\n",
    "    \"early_stop_patience\": 4,\n",
    "    \"early_stop_min_delta\": 0.01,\n",
    "    \n",
    "    \"use_amp\": True,              # force AMP on CUDA for speed\n",
    "    # Optional extras:\n",
    "    \"use_bf16\": True,\n",
    "    \"enable_tf32\": True,\n",
    "    \"finite_loss_skip\": True,\n",
    "    \"save_every\": 0\n",
    "}\n",
    "\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "if ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config)\n",
    "        cfg = wandb.config\n",
    "        use_wandb = True\n",
    "        print(\"[wandb] run initialized\")\n",
    "    except Exception as e:\n",
    "        print(\"[wandb] disabled (init failed):\", e)\n",
    "\n",
    "if cfg is None:\n",
    "    class _Cfg: pass\n",
    "    cfg = _Cfg()\n",
    "    for k, v in base_config.items():\n",
    "        setattr(cfg, k, v)\n",
    "    print(\"[INFO] Using local cfg (wandb off)\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# --- Enforce CUDA-only environment ---\n",
    "assert torch.cuda.is_available(), \"CUDA GPU is required but not detected. Please run in a CUDA-enabled environment.\"\n",
    "device = torch.device('cuda')\n",
    "print(f\"[DEVICE] Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Performance toggles\n",
    "if getattr(cfg, 'enable_tf32', False):\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"[DEVICE] TF32 enabled.\")\n",
    "    except Exception as _e:\n",
    "        print(\"[WARN] Could not enable TF32:\", _e)\n",
    "\n",
    "if getattr(cfg, 'use_bf16', False):\n",
    "    bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    print(f\"[DEVICE] bfloat16 support: {bf16_ok}\")\n",
    "\n",
    "print(\"Active config:\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:01:36.629319Z",
     "iopub.status.busy": "2025-09-03T13:01:36.628781Z",
     "iopub.status.idle": "2025-09-03T13:02:06.251853Z",
     "shell.execute_reply": "2025-09-03T13:02:06.250983Z",
     "shell.execute_reply.started": "2025-09-03T13:01:36.629290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# (CUDA enforcement handled in config cell; assume cuda device later)\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = MT5Tokenizer.from_pretrained(cfg.model)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        # CLIP freezing / unfreezing strategy\n",
    "        if cfg.unfreeze_clip_last_n and cfg.unfreeze_clip_last_n > 0:\n",
    "            # Freeze everything first\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "            blocks = list(self.clip.visual.transformer.resblocks)\n",
    "            for block in blocks[-cfg.unfreeze_clip_last_n:]:\n",
    "                for p in block.parameters():\n",
    "                    p.requires_grad = True\n",
    "        else:\n",
    "            # Respect freeze_clip flag (False means fully trainable)\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = not cfg.freeze_clip\n",
    "        # T5 freeze toggles\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine embedding dim dynamically (CLIP projection output)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    def _encode_image_single(self, images: torch.Tensor):\n",
    "        # (B, D) pooled embedding (already passed through ln_post + proj inside encode_image)\n",
    "        pooled = self.clip.encode_image(images)\n",
    "        return pooled\n",
    "\n",
    "    def _encode_image_patch_tokens(self, images: torch.Tensor):\n",
    "        \"\"\"Return patch-averaged embedding projected into CLIP joint space.\n",
    "        We manually replicate encode_image path but average patch tokens (excluding CLS),\n",
    "        then apply ln_post and proj so final dim == visual.output_dim (e.g. 512) to match ProjectionMLP in_dim.\"\"\"\n",
    "        visual = self.clip.visual\n",
    "        x = visual.conv1(images)                      # (B, width, grid, grid)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)     # (B, width, patches)\n",
    "        x = x.permute(0, 2, 1)                        # (B, patches, width)\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)         # prepend CLS\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        x = visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)                        # (sequence, B, width)\n",
    "        for block in visual.transformer.resblocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(1, 0, 2)                        # (B, sequence, width)\n",
    "        patches = x[:, 1:, :]                         # drop CLS\n",
    "        pooled = patches.mean(dim=1)                  # (B, width)\n",
    "        if hasattr(visual, 'ln_post'):\n",
    "            pooled = visual.ln_post(pooled)\n",
    "        if hasattr(visual, 'proj') and visual.proj is not None:\n",
    "            pooled = pooled @ visual.proj             # (B, output_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        images = images.to(device)\n",
    "        clip_emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        images = images.to(next(self.parameters()).device)\n",
    "        emb = self._encode_image_patch_tokens(images) if self.cfg.use_clip_patch_tokens else self._encode_image_single(images)\n",
    "        return self.proj(emb)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu görüntüyü açıkla: \", ban_sentinels=True, **gen_kwargs):\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap\n",
    "\n",
    "json_path = '/kaggle/input/tasviret/flickr8k/tasviret8k_captions.json'\n",
    "images_root = '/kaggle/input/tasviret/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=None)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:02:06.255586Z",
     "iopub.status.busy": "2025-09-03T13:02:06.254694Z",
     "iopub.status.idle": "2025-09-03T14:07:52.392159Z",
     "shell.execute_reply": "2025-09-03T14:07:52.391375Z",
     "shell.execute_reply.started": "2025-09-03T13:02:06.255549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training cell (only training + validation)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "# CUDA-only enforcement (already asserted earlier, but double-check for isolation run)\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required.\"\n",
    "device = torch.device('cuda')\n",
    "model_mm.to(device)\n",
    "\n",
    "# Decide AMP dtype\n",
    "amp_dtype = None\n",
    "if getattr(cfg, 'use_amp', True):\n",
    "    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16\n",
    "        print(\"[AMP] Using bfloat16 mixed precision\")\n",
    "    else:\n",
    "        amp_dtype = torch.float16\n",
    "        print(\"[AMP] Using float16 mixed precision\")\n",
    "else:\n",
    "    print(\"[AMP] Disabled; using full float32\")\n",
    "\n",
    "# Parameter groups: separate CLIP vs non-CLIP for LR scaling\n",
    "main_params = []\n",
    "clip_params = []\n",
    "for name, p in model_mm.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith('clip.'):\n",
    "        clip_params.append(p)\n",
    "    else:\n",
    "        main_params.append(p)\n",
    "param_groups = []\n",
    "if main_params:\n",
    "    param_groups.append({\"params\": main_params, \"lr\": cfg.lr})\n",
    "if clip_params:\n",
    "    scaled_lr = cfg.lr * getattr(cfg, 'clip_lr_scale', 0.05)\n",
    "    param_groups.append({\"params\": clip_params, \"lr\": scaled_lr})\n",
    "    print(f\"[INFO] CLIP fine-tune params: {len(clip_params)} with lr={scaled_lr:.2e}\")\n",
    "\n",
    "optimizer = AdamW(param_groups, weight_decay=cfg.weight_decay)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * cfg.epochs\n",
    "\n",
    "def lr_lambda(step):\n",
    "    # linear warmup then cosine decay\n",
    "    if cfg.warmup_steps > 0 and step < cfg.warmup_steps:\n",
    "        return float(step) / float(max(1, cfg.warmup_steps))\n",
    "    progress = (step - cfg.warmup_steps) / float(max(1, total_steps - cfg.warmup_steps))\n",
    "    progress = min(max(progress, 0.0), 1.0)\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=amp_dtype is not None)\n",
    "best_val = float('inf')\n",
    "CKPT_DIR = 'checkpoints'; os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Early stopping state\n",
    "early_patience = getattr(cfg, 'early_stop_patience', None)\n",
    "min_delta = getattr(cfg, 'early_stop_min_delta', 0.0)\n",
    "_epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "\n",
    "monitor_history = []  # (epoch, metric)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    model_mm.train()\n",
    "    train_sum = 0.0\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        imgs, srcs, tgts = batch\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if amp_dtype is not None:\n",
    "            with torch.amp.autocast('cuda', dtype=amp_dtype):\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                loss = out.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model_mm(imgs, srcs, tgts); loss = out.loss\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model_mm.parameters(), cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_sum += loss.item()\n",
    "        global_step += 1\n",
    "    train_epoch_loss = train_sum / max(1, len(train_loader))\n",
    "\n",
    "    val_epoch_loss = None\n",
    "    if val_loader:\n",
    "        model_mm.eval(); v = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                if amp_dtype is not None:\n",
    "                    with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                        out = model_mm(imgs, srcs, tgts)\n",
    "                else:\n",
    "                    out = model_mm(imgs, srcs, tgts)\n",
    "                v += out.loss.item()\n",
    "        val_epoch_loss = v / max(1, len(val_loader))\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    if val_epoch_loss is not None:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} val_loss={val_epoch_loss:.4f} time={dt:.1f}s lr={current_lr:.2e}\", flush=True)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"val/epoch_loss\": val_epoch_loss, \"lr\": current_lr}, step=epoch)\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} time={dt:.1f}s lr={current_lr:.2e}\", flush=True)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"lr\": current_lr}, step=epoch)\n",
    "\n",
    "    metric = val_epoch_loss if val_epoch_loss is not None else train_epoch_loss\n",
    "    monitor_history.append((epoch, metric))\n",
    "\n",
    "    improved = metric < (best_val - min_delta)\n",
    "    if improved:\n",
    "        best_val = metric\n",
    "        _epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            'model': model_mm.state_dict(),\n",
    "            'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "        }, os.path.join(CKPT_DIR, 'best.pt'))\n",
    "        print(f\"  -> Saved checkpoint (metric {best_val:.4f})\")\n",
    "    else:\n",
    "        _epochs_no_improve += 1\n",
    "\n",
    "    if early_patience is not None and _epochs_no_improve >= early_patience:\n",
    "        print(f\"[Early Stop] No improvement (>{min_delta} delta) for {early_patience} consecutive epochs. Stopping at epoch {epoch+1}.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "print(\"Training finished. Best metric:\", best_val, \"(early stop)\" if stopped_early else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T14:07:52.393668Z",
     "iopub.status.busy": "2025-09-03T14:07:52.393381Z",
     "iopub.status.idle": "2025-09-03T14:18:42.230070Z",
     "shell.execute_reply": "2025-09-03T14:18:42.229397Z",
     "shell.execute_reply.started": "2025-09-03T14:07:52.393638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Testing / Inference cell (evaluate test set + simple generation API)\n",
    "import torch, time, os, json, math\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required for inference.\"\n",
    "model_mm.eval()\n",
    "device = torch.device('cuda')\n",
    "model_mm.to(device)\n",
    "\n",
    "# Determine AMP dtype consistent with training\n",
    "amp_dtype = None\n",
    "if getattr(cfg, 'use_amp', True):\n",
    "    if getattr(cfg, 'use_bf16', False) and torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16\n",
    "    else:\n",
    "        amp_dtype = torch.float16\n",
    "\n",
    "test_loss = None\n",
    "if test_loader:\n",
    "    t0 = time.time(); total=0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            imgs, srcs, tgts = batch\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            if amp_dtype is not None:\n",
    "                with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                    out = model_mm(imgs, srcs, tgts)\n",
    "            else:\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "            total += out.loss.item()\n",
    "    test_loss = total / max(1,len(test_loader))\n",
    "    print(f\"Test loss={test_loss:.4f} time={(time.time()-t0):.1f}s\")\n",
    "else:\n",
    "    print(\"No test split available.\")\n",
    "\n",
    "############################################\n",
    "# Caption Quality Metrics (BLEU1-4, METEOR, CIDEr, ROUGE-L) + W&B logging\n",
    "############################################\n",
    "metrics = {}\n",
    "try:\n",
    "    # Build reference sets (all captions per test image) from original JSON\n",
    "    # Assumes json_path & test_dataset defined in earlier cell\n",
    "    with open(json_path) as f:\n",
    "        data_json = json.load(f)\n",
    "    img_entries = data_json['images'] if isinstance(data_json, dict) else data_json\n",
    "\n",
    "    # Gather unique test image filenames present in test_dataset\n",
    "    test_image_names = sorted({s[0] for s in test_dataset.samples})\n",
    "    refs_map = {}\n",
    "    for entry in img_entries:\n",
    "        if not isinstance(entry, dict): \n",
    "            continue\n",
    "        fname = entry.get('filename')\n",
    "        if fname in test_image_names:\n",
    "            # collect all raw captions\n",
    "            all_caps = []\n",
    "            for s in entry.get('sentences', []):\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    cap = s['raw'].strip()\n",
    "                    if cap:\n",
    "                        all_caps.append(cap)\n",
    "            if all_caps:\n",
    "                refs_map[fname] = all_caps\n",
    "\n",
    "    # Generate hypotheses (batched for speed)\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    BATCH_GEN = 8\n",
    "    image_root = images_root  # already defined earlier\n",
    "    image_paths = [os.path.join(image_root, fname) for fname in test_image_names]\n",
    "\n",
    "    def _generate_batch(paths):\n",
    "        if amp_dtype is not None:\n",
    "            with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                return model_mm.generate(image_paths=paths, ban_sentinels=True)\n",
    "        return model_mm.generate(image_paths=paths, ban_sentinels=True)\n",
    "\n",
    "    for i in range(0, len(image_paths), BATCH_GEN):\n",
    "        batch_paths = image_paths[i:i+BATCH_GEN]\n",
    "        caps = _generate_batch(batch_paths)\n",
    "        for p, pred in zip(batch_paths, caps):\n",
    "            fname = os.path.basename(p)\n",
    "            hyps.append(pred)\n",
    "            refs.append(refs_map.get(fname, [\"\"]))  # list of refs\n",
    "\n",
    "    # Install/load evaluate (quiet) if missing\n",
    "    try:\n",
    "        import evaluate\n",
    "    except ImportError:\n",
    "        import subprocess, sys\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"evaluate\"], check=False)\n",
    "        import evaluate\n",
    "\n",
    "    # Load metrics\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    meteor_metric = evaluate.load(\"meteor\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    cider_metric = None\n",
    "    try:\n",
    "        cider_metric = evaluate.load(\"cider\")\n",
    "    except Exception:\n",
    "        print(\"[WARN] CIDEr metric load failed (pycocoevalcap might not be available).\")\n",
    "\n",
    "    # Compute\n",
    "    bleu_res = bleu_metric.compute(predictions=hyps, references=refs)\n",
    "    meteor_res = meteor_metric.compute(predictions=hyps, references=refs)\n",
    "    rouge_res = rouge_metric.compute(predictions=hyps, references=[\" \".join(r[0].split()) if isinstance(r,list) and len(r)==1 else r[0] for r in refs])  # rouge expects single ref; use first\n",
    "    cider_score = None\n",
    "    if cider_metric:\n",
    "        # cider expects list[str] predictions and list[list[str]] references\n",
    "        cider_res = cider_metric.compute(predictions=hyps, references=refs)\n",
    "        cider_score = cider_res.get(\"score\")\n",
    "\n",
    "    metrics = {\n",
    "        \"bleu1\": bleu_res[\"precisions\"][0],\n",
    "        \"bleu2\": bleu_res[\"precisions\"][1],\n",
    "        \"bleu3\": bleu_res[\"precisions\"][2],\n",
    "        \"bleu4\": bleu_res[\"precisions\"][3],\n",
    "        \"bleu\": bleu_res[\"bleu\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rougeL\": rouge_res.get(\"rougeL\", float('nan')),\n",
    "    }\n",
    "    if cider_score is not None:\n",
    "        metrics[\"cider\"] = cider_score\n",
    "    if test_loss is not None:\n",
    "        metrics[\"test_loss\"] = test_loss\n",
    "\n",
    "    print(\"=== Caption Metrics ===\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v,(int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "    if use_wandb:\n",
    "        # Namespace under eval/\n",
    "        wandb.log({f\"eval/{k}\": v for k,v in metrics.items()}, commit=True)\n",
    "        print(\"[wandb] Logged caption metrics.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Metric computation failed:\", e)\n",
    "\n",
    "# Show a few sample generated captions from first N test images with diversity controls\n",
    "default_gen_kwargs = dict(\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "SAMPLE_PRINTS = 5\n",
    "if test_loader and SAMPLE_PRINTS > 0:\n",
    "    shown = 0\n",
    "    printed_imgs = set()\n",
    "    for img_path, _ in [(os.path.join('/kaggle/input/tasviret/flickr8k/Images', s[0]), s[1]) for s in test_dataset.samples]:\n",
    "        if img_path in printed_imgs:\n",
    "            continue\n",
    "        if amp_dtype is not None:\n",
    "            with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n",
    "        else:\n",
    "            caps = model_mm.generate(image_paths=[img_path], **default_gen_kwargs)\n",
    "        gt_caption = next(s[1] for s in test_dataset.samples if os.path.join(images_root, s[0]) == img_path)\n",
    "        print('GT:', gt_caption)\n",
    "        print('CAP:', caps[0])\n",
    "        print('---------------')\n",
    "        printed_imgs.add(img_path)\n",
    "        shown += 1\n",
    "        if shown >= SAMPLE_PRINTS:\n",
    "            break\n",
    "\n",
    "# Optional: stochastic sampling helper for more diverse outputs\n",
    "def generate_captions(image_paths: List[str], mode='beam', **kwargs):\n",
    "    if mode == 'sample':\n",
    "        sample_defaults = dict(temperature=0.8, top_p=0.9, do_sample=True, repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "        for k, v in sample_defaults.items():\n",
    "            kwargs.setdefault(k, v)\n",
    "    else:  # beam\n",
    "        beam_defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3)\n",
    "        for k, v in beam_defaults.items():\n",
    "            kwargs.setdefault(k, v)\n",
    "    if amp_dtype is not None:\n",
    "        with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "            return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "    return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "\n",
    "print(\"[Ready] generate_captions(['path/to/img.jpg'], mode='beam'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T14:19:34.746991Z",
     "iopub.status.busy": "2025-09-03T14:19:34.746370Z",
     "iopub.status.idle": "2025-09-03T14:19:34.759064Z",
     "shell.execute_reply": "2025-09-03T14:19:34.758248Z",
     "shell.execute_reply.started": "2025-09-03T14:19:34.746965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Single Image Inference\n",
    "# --- Predict helper (single image) ---\n",
    "from typing import Optional, Dict, Any, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(\n",
    "    image_path: str,\n",
    "    prompt: str = \"Bu fotoğrafı açıkla: \",\n",
    "    mode: str = \"beam\",              # 'beam' or 'sample'\n",
    "    max_refs: Optional[int] = 5,\n",
    "    show_image: bool = True,\n",
    "    show_refs: bool = True,\n",
    "    gen_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    json_file: Optional[str] = None,\n",
    "    ban_sentinels: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a caption for one image and (optionally) display it with reference captions.\n",
    "\n",
    "    Returns dict with: image_path, prediction, references (list), mode.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n",
    "    jf = json_file or json_path  # use global json_path defined earlier\n",
    "\n",
    "    # Collect reference captions (all captions for this filename)\n",
    "    refs: List[str] = []\n",
    "    try:\n",
    "        with open(jf) as f:\n",
    "            data = json.load(f)\n",
    "        entries = data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "        target_name = os.path.basename(image_path)\n",
    "        for e in entries:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            if e.get('filename') == target_name:\n",
    "                for s in e.get('sentences', []):\n",
    "                    if isinstance(s, dict) and 'raw' in s:\n",
    "                        cap = s['raw'].strip()\n",
    "                        if cap:\n",
    "                            refs.append(cap)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse references ({e})\")\n",
    "\n",
    "    if max_refs is not None:\n",
    "        refs = refs[:max_refs]\n",
    "\n",
    "    # Defaults for decoding\n",
    "    gen_kwargs = (gen_kwargs or {}).copy()\n",
    "    if mode == \"sample\":\n",
    "        defaults = dict(temperature=0.8, top_p=0.9, do_sample=True,\n",
    "                        repetition_penalty=1.15, no_repeat_ngram_size=3)\n",
    "    else:\n",
    "        defaults = dict(repetition_penalty=1.2, no_repeat_ngram_size=3,\n",
    "                        num_beams=getattr(cfg, \"num_beams_infer\", 4))\n",
    "    for k, v in defaults.items():\n",
    "        gen_kwargs.setdefault(k, v)\n",
    "\n",
    "    # Use global mixed precision dtype if available\n",
    "    local_amp_dtype = globals().get(\"amp_dtype\", None)\n",
    "    model_mm.eval()\n",
    "    if local_amp_dtype is not None:\n",
    "        with torch.cuda.amp.autocast(dtype=local_amp_dtype):\n",
    "            pred = model_mm.generate(\n",
    "                image_paths=[image_path],\n",
    "                prompt=prompt,\n",
    "                ban_sentinels=ban_sentinels,\n",
    "                **gen_kwargs\n",
    "            )[0]\n",
    "    else:\n",
    "        pred = model_mm.generate(\n",
    "            image_paths=[image_path],\n",
    "            prompt=prompt,\n",
    "            ban_sentinels=ban_sentinels,\n",
    "            **gen_kwargs\n",
    "        )[0]\n",
    "\n",
    "    # Visualization\n",
    "    if show_image:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img); plt.axis(\"off\")\n",
    "            plt.title(\"Image\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not display image ({e})\")\n",
    "\n",
    "    if show_refs:\n",
    "        if refs:\n",
    "            print(\"References:\")\n",
    "            for i, r in enumerate(refs, 1):\n",
    "                print(f\"  {i}. {r}\")\n",
    "        else:\n",
    "            print(\"(No references found)\")\n",
    "    print(\"Prediction:\")\n",
    "    print(\" \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T14:19:42.040085Z",
     "iopub.status.busy": "2025-09-03T14:19:42.039468Z",
     "iopub.status.idle": "2025-09-03T14:19:46.924950Z",
     "shell.execute_reply": "2025-09-03T14:19:46.924385Z",
     "shell.execute_reply.started": "2025-09-03T14:19:42.040063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = predict(\"/kaggle/input/tasviret/flickr8k/Images/1032460886_4a598ed535.jpg\", mode='beam')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8193626,
     "sourceId": 12947536,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8195803,
     "sourceId": 12950629,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
