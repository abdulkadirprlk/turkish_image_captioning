{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea62c044",
   "metadata": {},
   "source": [
    "## CLIP (ViT-L/14) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulkadirparlak\u001b[0m (\u001b[33mabdulkadirparlak-hacettepe-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/abdulkadir/Documents/bites/wandb/run-20250901_175006-j2znqso0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/j2znqso0' target=\"_blank\">clip_mt5_prefix_run</a></strong> to <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/j2znqso0' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/j2znqso0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wandb] run initialized\n",
      "Active config:\n",
      "  model: google/mt5-small\n",
      "  clip_encoder: ViT-B/32\n",
      "  prefix_tokens: 8\n",
      "  batch_size: 4\n",
      "  lr: 0.0001\n",
      "  epochs: 20\n",
      "  dataset_limit: 1000\n",
      "  freeze_clip: True\n",
      "  freeze_t5_encoder: True\n",
      "  freeze_t5_decoder: False\n",
      "  seed: 42\n",
      "  weight_decay: 0\n",
      "  grad_clip: 1\n",
      "  warmup_steps: 0\n",
      "  num_beams_infer: 4\n",
      "  max_new_tokens_infer: 32\n",
      "  src_max_len: 64\n",
      "  tgt_max_len: 64\n"
     ]
    }
   ],
   "source": [
    "# Unified configuration + optional wandb init\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"bites-tr-image-captioning\"\n",
    "RUN_NAME = \"clip_mt5_prefix_run\"\n",
    "\n",
    "ENABLE_WANDB = True\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/32\",  # switched from ViT-L/14 to ViT-B/32 for speed\n",
    "    \"prefix_tokens\": 8,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 30,\n",
    "    \"dataset_limit\": 1000,\n",
    "    \"freeze_clip\": True,\n",
    "    \"freeze_t5_encoder\": True,\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "}\n",
    "\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "if ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config)\n",
    "        cfg = wandb.config\n",
    "        use_wandb = True\n",
    "        print(\"[wandb] run initialized\")\n",
    "    except Exception as e:\n",
    "        print(\"[wandb] disabled (init failed):\", e)\n",
    "\n",
    "if cfg is None:\n",
    "    class _Cfg: pass\n",
    "    cfg = _Cfg()\n",
    "    for k, v in base_config.items():\n",
    "        setattr(cfg, k, v)\n",
    "    print(\"[INFO] Using local cfg (wandb off)\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "print(\"Active config:\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26224d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1000  Val: 50  Test: 30\n",
      "Trainable params: 157960896\n"
     ]
    }
   ],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = MT5Tokenizer.from_pretrained(cfg.model)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        for p in self.clip.parameters(): p.requires_grad = not cfg.freeze_clip\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters(): p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters(): p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine CLIP embedding dim dynamically (ViT-B/32 = 512, ViT-L/14 = 768)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    # Training forward (teacher forcing)\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            clip_emb = self.clip.encode_image(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "\n",
    "    # --- Inference helpers ---\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            emb = self.clip.encode_image(images)\n",
    "        return self.proj(emb)  # (B, K, d)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\" \", ban_sentinels=True):\n",
    "        \"\"\"High-level caption generation.\n",
    "        Args:\n",
    "            image_paths: list of image file paths (if images tensor not provided).\n",
    "            images: preprocessed tensor (B,C,H,W) matching CLIP preprocess.\n",
    "            num_beams: beam search width.\n",
    "            max_new_tokens: decoding length.\n",
    "            prompt: optional textual prompt appended after prefix.\n",
    "            ban_sentinels: whether to block <extra_id_*> placeholders.\n",
    "        Returns: list[str] captions.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]  # lightweight fetch\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)  # (B,K,d)\n",
    "        # Prompt tokens\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap\n",
    "\n",
    "json_path = 'data/flickr8k/tasviret8k_captions.json'\n",
    "images_root = 'data/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=50)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=30)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell (only training + validation)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model_mm.to(device)\n",
    "params = [p for p in model_mm.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * cfg.epochs\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if cfg.warmup_steps > 0 and step < cfg.warmup_steps:\n",
    "        return float(step) / float(max(1, cfg.warmup_steps))\n",
    "    progress = (step - cfg.warmup_steps) / float(max(1, total_steps - cfg.warmup_steps))\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = torch.amp.GradScaler('cuda') if use_cuda else None\n",
    "best_val = float('inf')\n",
    "CKPT_DIR = 'checkpoints'; os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    model_mm.train()\n",
    "    train_sum = 0.0\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        imgs, srcs, tgts = batch\n",
    "        imgs = imgs.to(device)\n",
    "        if use_cuda:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                loss = out.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            out = model_mm(imgs, srcs, tgts); loss = out.loss\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "        train_sum += loss.item()\n",
    "        global_step += 1\n",
    "    train_epoch_loss = train_sum / max(1, len(train_loader))\n",
    "\n",
    "    val_epoch_loss = None\n",
    "    if val_loader:\n",
    "        model_mm.eval(); v = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device)\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                v += out.loss.item()\n",
    "        val_epoch_loss = v / max(1, len(val_loader))\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    if val_epoch_loss is not None:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} val_loss={val_epoch_loss:.4f} time={dt:.1f}s lr={scheduler.get_last_lr()[0]:.2e}\", flush=True)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"val/epoch_loss\": val_epoch_loss, \"lr\": scheduler.get_last_lr()[0]}, step=epoch)\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} time={dt:.1f}s lr={scheduler.get_last_lr()[0]:.2e}\", flush=True)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"lr\": scheduler.get_last_lr()[0]}, step=epoch)\n",
    "\n",
    "    metric = val_epoch_loss if val_epoch_loss is not None else train_epoch_loss\n",
    "    if metric < best_val:\n",
    "        best_val = metric\n",
    "        torch.save({\n",
    "            'model': model_mm.state_dict(),\n",
    "            'cfg': base_config,\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "        }, os.path.join(CKPT_DIR, 'best.pt'))\n",
    "        print(f\"  -> Saved checkpoint (metric {best_val:.4f})\")\n",
    "print(\"Training finished. Best metric:\", best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2831fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing / Inference cell (evaluate test set + simple generation API)\n",
    "import torch, time, os\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "model_mm.eval()\n",
    "device = next(model_mm.parameters()).device\n",
    "\n",
    "test_loss = None\n",
    "if test_loader:\n",
    "    t0 = time.time(); total=0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            imgs, srcs, tgts = batch\n",
    "            imgs = imgs.to(device)\n",
    "            out = model_mm(imgs, srcs, tgts)\n",
    "            total += out.loss.item()\n",
    "    test_loss = total / max(1,len(test_loader))\n",
    "    print(f\"Test loss={test_loss:.4f} time={(time.time()-t0):.1f}s\")\n",
    "else:\n",
    "    print(\"No test split available.\")\n",
    "\n",
    "# Show a few sample generated captions from first N test images\n",
    "SAMPLE_PRINTS = 5\n",
    "if test_loader and SAMPLE_PRINTS > 0:\n",
    "    shown = 0\n",
    "    printed_imgs = set()\n",
    "    for img_path, _ in [(os.path.join('data/flickr8k/Images', s[0]), s[1]) for s in test_dataset.samples]:\n",
    "        if img_path in printed_imgs:  # avoid duplicates if multiple captions per image\n",
    "            continue\n",
    "        caps = model_mm.generate(image_paths=[img_path])\n",
    "        # Find the ground truth caption for this image (first caption in test_dataset.samples for this image)\n",
    "        gt_caption = next(s[1] for s in test_dataset.samples if os.path.join(images_root, s[0]) == img_path)\n",
    "        print('GT:', gt_caption)\n",
    "        print('CAP:', caps[0])\n",
    "        print('---------------')\n",
    "        printed_imgs.add(img_path)\n",
    "        shown += 1\n",
    "        if shown >= SAMPLE_PRINTS:\n",
    "            break\n",
    "\n",
    "def generate_captions(image_paths: List[str], **kwargs):\n",
    "    return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "\n",
    "print(\"[Ready] generate_captions(['path/to/img.jpg'])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bites",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
