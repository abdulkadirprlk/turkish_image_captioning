{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea62c044",
   "metadata": {},
   "source": [
    "## CLIP (ViT-L/14) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98b47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulkadirparlak\u001b[0m (\u001b[33mabdulkadirparlak-hacettepe-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/abdulkadir/Documents/bites/wandb/run-20250902_103851-if953o39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/if953o39' target=\"_blank\">clip_mt5_prefix_run</a></strong> to <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/if953o39' target=\"_blank\">https://wandb.ai/abdulkadirparlak-hacettepe-university/bites-tr-image-captioning/runs/if953o39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wandb] run initialized\n",
      "Active config:\n",
      "  model: google/mt5-small\n",
      "  clip_encoder: ViT-B/32\n",
      "  prefix_tokens: 16\n",
      "  batch_size: 4\n",
      "  lr: 0.0001\n",
      "  epochs: 20\n",
      "  dataset_limit: 3000\n",
      "  freeze_clip: True\n",
      "  freeze_t5_encoder: True\n",
      "  freeze_t5_decoder: False\n",
      "  seed: 42\n",
      "  weight_decay: 0\n",
      "  grad_clip: 1\n",
      "  warmup_steps: 0\n",
      "  num_beams_infer: 4\n",
      "  max_new_tokens_infer: 32\n",
      "  src_max_len: 64\n",
      "  tgt_max_len: 64\n",
      "  early_stop_patience: 4\n",
      "  early_stop_min_delta: 0.01\n",
      "Active config:\n",
      "  model: google/mt5-small\n",
      "  clip_encoder: ViT-B/32\n",
      "  prefix_tokens: 16\n",
      "  batch_size: 4\n",
      "  lr: 0.0001\n",
      "  epochs: 20\n",
      "  dataset_limit: 3000\n",
      "  freeze_clip: True\n",
      "  freeze_t5_encoder: True\n",
      "  freeze_t5_decoder: False\n",
      "  seed: 42\n",
      "  weight_decay: 0\n",
      "  grad_clip: 1\n",
      "  warmup_steps: 0\n",
      "  num_beams_infer: 4\n",
      "  max_new_tokens_infer: 32\n",
      "  src_max_len: 64\n",
      "  tgt_max_len: 64\n",
      "  early_stop_patience: 4\n",
      "  early_stop_min_delta: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Unified configuration + optional wandb init\n",
    "import os\n",
    "\n",
    "PROJECT_NAME = \"bites-tr-image-captioning\"\n",
    "RUN_NAME = \"clip_mt5_prefix_run\"\n",
    "\n",
    "ENABLE_WANDB = True\n",
    "\n",
    "base_config = {\n",
    "    \"model\": \"google/mt5-small\",\n",
    "    \"clip_encoder\": \"ViT-B/32\",  # switched from ViT-L/14 to ViT-B/32 for speed\n",
    "    \"prefix_tokens\": 16,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"dataset_limit\": 3000,\n",
    "    \"freeze_clip\": True,\n",
    "    \"freeze_t5_encoder\": True,\n",
    "    \"freeze_t5_decoder\": False,\n",
    "    \"seed\": 42,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"num_beams_infer\": 4,\n",
    "    \"max_new_tokens_infer\": 32,\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    # --- Early stopping hyperparameters ---\n",
    "    \"early_stop_patience\": 4,      # stop if no meaningful improvement for these epochs\n",
    "    \"early_stop_min_delta\": 0.01,  # minimal absolute improvement in monitored metric to reset patience\n",
    "}\n",
    "\n",
    "use_wandb = False\n",
    "cfg = None\n",
    "if ENABLE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, config=base_config)\n",
    "        cfg = wandb.config\n",
    "        use_wandb = True\n",
    "        print(\"[wandb] run initialized\")\n",
    "    except Exception as e:\n",
    "        print(\"[wandb] disabled (init failed):\", e)\n",
    "\n",
    "if cfg is None:\n",
    "    class _Cfg: pass\n",
    "    cfg = _Cfg()\n",
    "    for k, v in base_config.items():\n",
    "        setattr(cfg, k, v)\n",
    "    print(\"[INFO] Using local cfg (wandb off)\")\n",
    "\n",
    "import random, numpy as np, torch\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "print(\"Active config:\")\n",
    "for k, v in base_config.items():\n",
    "    print(f\"  {k}: {getattr(cfg, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26224d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3000  Val: 50  Test: 30\n",
      "Trainable params: 162159296\n"
     ]
    }
   ],
   "source": [
    "# Model, dataset and pipeline definitions (data + model init only)\n",
    "import torch, os, json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, prefix_tokens=8, hidden=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, out_dim * prefix_tokens)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.fc2(x)\n",
    "        x = x.view(x.size(0), self.prefix_tokens, -1)\n",
    "        x = self.ln(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = MT5Tokenizer.from_pretrained(cfg.model)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(cfg.model)\n",
    "        self.clip, _ = clip.load(cfg.clip_encoder, device='cpu')\n",
    "        for p in self.clip.parameters(): p.requires_grad = not cfg.freeze_clip\n",
    "        if cfg.freeze_t5_encoder:\n",
    "            for p in self.model.encoder.parameters(): p.requires_grad = False\n",
    "        if cfg.freeze_t5_decoder:\n",
    "            for p in self.model.decoder.parameters(): p.requires_grad = False\n",
    "        self.prefix_tokens = cfg.prefix_tokens\n",
    "        # Determine CLIP embedding dim dynamically (ViT-B/32 = 512, ViT-L/14 = 768)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            embed_dim = self.clip.visual.output_dim if hasattr(self.clip.visual, 'output_dim') else self.clip.encode_image(dummy).shape[-1]\n",
    "        self.proj = ProjectionMLP(in_dim=embed_dim, out_dim=self.model.config.d_model, prefix_tokens=cfg.prefix_tokens)\n",
    "        self._cached_sentinel_ids = None  # lazy cache\n",
    "\n",
    "    # Training forward (teacher forcing)\n",
    "    def forward(self, images, src_texts, tgt_texts):\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            clip_emb = self.clip.encode_image(images)\n",
    "        prefix_emb = self.proj(clip_emb)\n",
    "        tok_src = self.tokenizer(list(src_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        tok_tgt = self.tokenizer(list(tgt_texts), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.tgt_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok_src.input_ids)\n",
    "        full_emb = torch.cat([prefix_emb, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(prefix_emb.size(0), self.prefix_tokens, dtype=tok_src.attention_mask.dtype, device=device),\n",
    "            tok_src.attention_mask\n",
    "        ], dim=1)\n",
    "        return self.model(inputs_embeds=full_emb, attention_mask=full_attn, labels=tok_tgt.input_ids)\n",
    "\n",
    "    # --- Inference helpers ---\n",
    "    def _prepare_prefix(self, images: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            emb = self.clip.encode_image(images)\n",
    "        return self.proj(emb)  # (B, K, d)\n",
    "\n",
    "    def _get_sentinel_bad_words(self, n=50):\n",
    "        if self._cached_sentinel_ids is None:\n",
    "            ids = [self.tokenizer(f'<extra_id_{i}>').input_ids[0] for i in range(n)]\n",
    "            self._cached_sentinel_ids = [[i] for i in ids]\n",
    "        return self._cached_sentinel_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, image_paths=None, images=None, num_beams=None, max_new_tokens=None, prompt=\"Bu fotoğrafı açıkla: \", ban_sentinels=True):\n",
    "        \"\"\"High-level caption generation.\n",
    "        Args:\n",
    "            image_paths: list of image file paths (if images tensor not provided).\n",
    "            images: preprocessed tensor (B,C,H,W) matching CLIP preprocess.\n",
    "            num_beams: beam search width.\n",
    "            max_new_tokens: decoding length.\n",
    "            prompt: optional textual prompt appended after prefix.\n",
    "            ban_sentinels: whether to block <extra_id_*> placeholders.\n",
    "        Returns: list[str] captions.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        num_beams = num_beams or self.cfg.num_beams_infer\n",
    "        max_new_tokens = max_new_tokens or self.cfg.max_new_tokens_infer\n",
    "        if images is None:\n",
    "            assert image_paths is not None, \"Provide image_paths or images tensor\"\n",
    "            preprocess = clip.load(self.cfg.clip_encoder, device='cpu')[1]  # lightweight fetch\n",
    "            pil_images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "            images = torch.stack([preprocess(im) for im in pil_images])\n",
    "        images = images.to(device)\n",
    "        prefix_tokens = self._prepare_prefix(images)  # (B,K,d)\n",
    "        # Prompt tokens\n",
    "        tok = self.tokenizer([prompt]*images.size(0), return_tensors='pt', padding=True, truncation=True, max_length=self.cfg.src_max_len).to(device)\n",
    "        text_emb = self.model.encoder.embed_tokens(tok.input_ids)\n",
    "        full_emb = torch.cat([prefix_tokens, text_emb], dim=1)\n",
    "        full_attn = torch.cat([\n",
    "            torch.ones(images.size(0), self.prefix_tokens, device=device, dtype=tok.attention_mask.dtype),\n",
    "            tok.attention_mask\n",
    "        ], dim=1)\n",
    "        bad_words_ids = self._get_sentinel_bad_words() if ban_sentinels else None\n",
    "        gen_ids = self.model.generate(\n",
    "            inputs_embeds=full_emb,\n",
    "            attention_mask=full_attn,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bad_words_ids=bad_words_ids\n",
    "        )\n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True).strip() for g in gen_ids]\n",
    "        return [c if c else \"<EMPTY>\" for c in captions]\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, split=None, limit=None, clip_preprocess=None):\n",
    "        self.images_root = images_root\n",
    "        raw = json.load(open(json_path))\n",
    "        rows = raw['images'] if isinstance(raw, dict) and 'images' in raw else raw\n",
    "        self.samples = []\n",
    "        for row in rows:\n",
    "            if not isinstance(row, dict): continue\n",
    "            if split and row.get('split') != split: continue\n",
    "            img = row.get('filename') or row.get('image') or row.get('img')\n",
    "            sentences = row.get('sentences')\n",
    "            if not img or not sentences: continue\n",
    "            for s in sentences:\n",
    "                if isinstance(s, dict) and 'raw' in s:\n",
    "                    self.samples.append((img, s['raw']))\n",
    "        if limit: self.samples = self.samples[:limit]\n",
    "        self.transform = clip_preprocess or clip.load(cfg.clip_encoder, device='cpu')[1]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.samples[idx]\n",
    "        path = os.path.join(self.images_root, img_name)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        return self.transform(image), \" \", cap\n",
    "\n",
    "json_path = 'data/flickr8k/tasviret8k_captions.json'\n",
    "images_root = 'data/flickr8k/Images'\n",
    "train_dataset = Flickr8kCaptions(json_path, images_root, split='train', limit=cfg.dataset_limit)\n",
    "val_dataset   = Flickr8kCaptions(json_path, images_root, split='val',   limit=50)\n",
    "test_dataset  = Flickr8kCaptions(json_path, images_root, split='test',  limit=30)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=cfg.batch_size, shuffle=False) if len(val_dataset)>0 else None\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=cfg.batch_size, shuffle=False) if len(test_dataset)>0 else None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(cfg)\n",
    "print(f\"Train samples: {len(train_dataset)}  Val: {len(val_dataset)}  Test: {len(test_dataset)}\")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model_mm.parameters() if p.requires_grad))\n",
    "\n",
    "train_imgs = {img for img,_ in train_dataset.samples}\n",
    "val_imgs   = {img for img,_ in val_dataset.samples}\n",
    "test_imgs  = {img for img,_ in test_dataset.samples}\n",
    "print('overlap train∩val', train_imgs & val_imgs)\n",
    "print('overlap train∩test', train_imgs & test_imgs)\n",
    "print('overlap val∩test', val_imgs & test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fb8bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 train_loss=10.2667 val_loss=2.7040 time=294.3s lr=9.94e-05\n",
      "  -> Saved checkpoint (metric 2.7040)\n",
      "  -> Saved checkpoint (metric 2.7040)\n",
      "Epoch 2/20 train_loss=2.9179 val_loss=2.3779 time=296.5s lr=9.76e-05\n",
      "Epoch 2/20 train_loss=2.9179 val_loss=2.3779 time=296.5s lr=9.76e-05\n",
      "  -> Saved checkpoint (metric 2.3779)\n",
      "  -> Saved checkpoint (metric 2.3779)\n",
      "Epoch 3/20 train_loss=2.6122 val_loss=2.2738 time=305.2s lr=9.46e-05\n",
      "Epoch 3/20 train_loss=2.6122 val_loss=2.2738 time=305.2s lr=9.46e-05\n",
      "  -> Saved checkpoint (metric 2.2738)\n",
      "  -> Saved checkpoint (metric 2.2738)\n",
      "Epoch 4/20 train_loss=2.4367 val_loss=2.1790 time=285.7s lr=9.05e-05\n",
      "Epoch 4/20 train_loss=2.4367 val_loss=2.1790 time=285.7s lr=9.05e-05\n",
      "  -> Saved checkpoint (metric 2.1790)\n",
      "  -> Saved checkpoint (metric 2.1790)\n",
      "Epoch 5/20 train_loss=2.3708 val_loss=2.1652 time=285.9s lr=8.54e-05\n",
      "Epoch 5/20 train_loss=2.3708 val_loss=2.1652 time=285.9s lr=8.54e-05\n",
      "  -> Saved checkpoint (metric 2.1652)\n",
      "  -> Saved checkpoint (metric 2.1652)\n",
      "Epoch 6/20 train_loss=2.2606 val_loss=2.1286 time=318.3s lr=7.94e-05\n",
      "Epoch 6/20 train_loss=2.2606 val_loss=2.1286 time=318.3s lr=7.94e-05\n",
      "  -> Saved checkpoint (metric 2.1286)\n",
      "  -> Saved checkpoint (metric 2.1286)\n",
      "Epoch 7/20 train_loss=2.1953 val_loss=2.1165 time=317.4s lr=7.27e-05\n",
      "Epoch 7/20 train_loss=2.1953 val_loss=2.1165 time=317.4s lr=7.27e-05\n",
      "  -> Saved checkpoint (metric 2.1165)\n",
      "  -> Saved checkpoint (metric 2.1165)\n",
      "Epoch 8/20 train_loss=2.1370 val_loss=2.0931 time=299.5s lr=6.55e-05\n",
      "Epoch 8/20 train_loss=2.1370 val_loss=2.0931 time=299.5s lr=6.55e-05\n",
      "  -> Saved checkpoint (metric 2.0931)\n",
      "  -> Saved checkpoint (metric 2.0931)\n",
      "Epoch 9/20 train_loss=2.0949 val_loss=2.0793 time=312.4s lr=5.78e-05\n",
      "Epoch 9/20 train_loss=2.0949 val_loss=2.0793 time=312.4s lr=5.78e-05\n",
      "  -> Saved checkpoint (metric 2.0793)\n",
      "  -> Saved checkpoint (metric 2.0793)\n",
      "Epoch 10/20 train_loss=2.0589 val_loss=2.0827 time=328.6s lr=5.00e-05\n",
      "Epoch 10/20 train_loss=2.0589 val_loss=2.0827 time=328.6s lr=5.00e-05\n",
      "Epoch 11/20 train_loss=2.0148 val_loss=2.0643 time=310.1s lr=4.22e-05\n",
      "Epoch 11/20 train_loss=2.0148 val_loss=2.0643 time=310.1s lr=4.22e-05\n",
      "  -> Saved checkpoint (metric 2.0643)\n",
      "  -> Saved checkpoint (metric 2.0643)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m     optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m     scheduler.step()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     train_sum += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     global_step += \u001b[32m1\u001b[39m\n\u001b[32m     62\u001b[39m train_epoch_loss = train_sum / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training cell (only training + validation)\n",
    "import math, time, torch, warnings, os\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "warnings.filterwarnings(\"ignore\", message=\".*legacy behaviour.*MT5Tokenizer.*\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model_mm.to(device)\n",
    "params = [p for p in model_mm.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * cfg.epochs\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if cfg.warmup_steps > 0 and step < cfg.warmup_steps:\n",
    "        return float(step) / float(max(1, cfg.warmup_steps))\n",
    "    progress = (step - cfg.warmup_steps) / float(max(1, total_steps - cfg.warmup_steps))\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = torch.amp.GradScaler('cuda') if use_cuda else None\n",
    "best_val = float('inf')\n",
    "CKPT_DIR = 'checkpoints'; os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Early stopping state\n",
    "early_patience = getattr(cfg, 'early_stop_patience', None)\n",
    "min_delta = getattr(cfg, 'early_stop_min_delta', 0.0)\n",
    "_epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "\n",
    "monitor_history = []  # (epoch, metric)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    model_mm.train()\n",
    "    train_sum = 0.0\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        imgs, srcs, tgts = batch\n",
    "        imgs = imgs.to(device)\n",
    "        if use_cuda:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                loss = out.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            out = model_mm(imgs, srcs, tgts); loss = out.loss\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "        train_sum += loss.item()\n",
    "        global_step += 1\n",
    "    train_epoch_loss = train_sum / max(1, len(train_loader))\n",
    "\n",
    "    val_epoch_loss = None\n",
    "    if val_loader:\n",
    "        model_mm.eval(); v = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, srcs, tgts = batch\n",
    "                imgs = imgs.to(device)\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                v += out.loss.item()\n",
    "        val_epoch_loss = v / max(1, len(val_loader))\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    if val_epoch_loss is not None:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} val_loss={val_epoch_loss:.4f} time={dt:.1f}s lr={current_lr:.2e}\", flush=True)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"val/epoch_loss\": val_epoch_loss, \"lr\": current_lr}, step=epoch)\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} train_loss={train_epoch_loss:.4f} time={dt:.1f}s lr={current_lr:.2e}\", flush=True)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train/epoch_loss\": train_epoch_loss, \"lr\": current_lr}, step=epoch)\n",
    "\n",
    "    metric = val_epoch_loss if val_epoch_loss is not None else train_epoch_loss\n",
    "    monitor_history.append((epoch, metric))\n",
    "\n",
    "    improved = metric < (best_val - min_delta)\n",
    "    if improved:\n",
    "        best_val = metric\n",
    "        _epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            'model': model_mm.state_dict(),\n",
    "            'cfg': {k: getattr(cfg, k) for k in base_config.keys()},\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "        }, os.path.join(CKPT_DIR, 'best.pt'))\n",
    "        print(f\"  -> Saved checkpoint (metric {best_val:.4f})\")\n",
    "    else:\n",
    "        _epochs_no_improve += 1\n",
    "\n",
    "    if early_patience is not None and _epochs_no_improve >= early_patience:\n",
    "        print(f\"[Early Stop] No improvement (>{min_delta} delta) for {early_patience} consecutive epochs. Stopping at epoch {epoch+1}.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "print(\"Training finished. Best metric:\", best_val, \"(early stop)\" if stopped_early else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2831fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss=2.0120 time=1.3s\n",
      "GT: İki kahverengi köpek kar üstünde kavga ediyor.\n",
      "CAP: Çimlerde koşan bir köpek.\n",
      "---------------\n",
      "GT: İki kahverengi köpek kar üstünde kavga ediyor.\n",
      "CAP: Çimlerde koşan bir köpek.\n",
      "---------------\n",
      "GT: Havuzda sahibine doğru yüzen küçük bir köpek.\n",
      "CAP: Çimlerde koşan bir köpek.\n",
      "---------------\n",
      "GT: Havuzda sahibine doğru yüzen küçük bir köpek.\n",
      "CAP: Çimlerde koşan bir köpek.\n",
      "---------------\n",
      "GT: Bir sokak festivalinde bir kadın ve bir erkek sambacı dans ediyor, kadın tüy şapka giymiş, arkada bir adam fotoğraf çekiyor.\n",
      "CAP: Bir erkek çocuğu yeşil tişört giyiyor.\n",
      "---------------\n",
      "GT: Bir sokak festivalinde bir kadın ve bir erkek sambacı dans ediyor, kadın tüy şapka giymiş, arkada bir adam fotoğraf çekiyor.\n",
      "CAP: Bir erkek çocuğu yeşil tişört giyiyor.\n",
      "---------------\n",
      "GT: İki kadınla bir adam tahta masa sandalyede bir şemsiyenin altında oturmuş içeceklerini içiyor.\n",
      "CAP: Bir erkek çocuğu yeşillikler içinde yürüyor.\n",
      "---------------\n",
      "GT: İki kadınla bir adam tahta masa sandalyede bir şemsiyenin altında oturmuş içeceklerini içiyor.\n",
      "CAP: Bir erkek çocuğu yeşillikler içinde yürüyor.\n",
      "---------------\n",
      "GT: Maç sırasında odaklanmış belli bir noktaya bakmakta olan bir Amerikan futbol oyuncusu.\n",
      "CAP: Kırmızı tişörtlü bir kız çocuğu yeşil tişört giyiyor.\n",
      "---------------\n",
      "[Ready] generate_captions(['path/to/img.jpg'])\n",
      "GT: Maç sırasında odaklanmış belli bir noktaya bakmakta olan bir Amerikan futbol oyuncusu.\n",
      "CAP: Kırmızı tişörtlü bir kız çocuğu yeşil tişört giyiyor.\n",
      "---------------\n",
      "[Ready] generate_captions(['path/to/img.jpg'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "# Testing / Inference cell (evaluate test set + simple generation API)\n",
    "import torch, time, os\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "model_mm.eval()\n",
    "device = next(model_mm.parameters()).device\n",
    "\n",
    "test_loss = None\n",
    "if test_loader:\n",
    "    t0 = time.time(); total=0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            imgs, srcs, tgts = batch\n",
    "            imgs = imgs.to(device)\n",
    "            out = model_mm(imgs, srcs, tgts)\n",
    "            total += out.loss.item()\n",
    "    test_loss = total / max(1,len(test_loader))\n",
    "    print(f\"Test loss={test_loss:.4f} time={(time.time()-t0):.1f}s\")\n",
    "else:\n",
    "    print(\"No test split available.\")\n",
    "\n",
    "# Show a few sample generated captions from first N test images\n",
    "SAMPLE_PRINTS = 5\n",
    "if test_loader and SAMPLE_PRINTS > 0:\n",
    "    shown = 0\n",
    "    printed_imgs = set()\n",
    "    for img_path, _ in [(os.path.join('data/flickr8k/Images', s[0]), s[1]) for s in test_dataset.samples]:\n",
    "        if img_path in printed_imgs:  # avoid duplicates if multiple captions per image\n",
    "            continue\n",
    "        caps = model_mm.generate(image_paths=[img_path])\n",
    "        # Find the ground truth caption for this image (first caption in test_dataset.samples for this image)\n",
    "        gt_caption = next(s[1] for s in test_dataset.samples if os.path.join(images_root, s[0]) == img_path)\n",
    "        print('GT:', gt_caption)\n",
    "        print('CAP:', caps[0])\n",
    "        print('---------------')\n",
    "        printed_imgs.add(img_path)\n",
    "        shown += 1\n",
    "        if shown >= SAMPLE_PRINTS:\n",
    "            break\n",
    "\n",
    "def generate_captions(image_paths: List[str], **kwargs):\n",
    "    return model_mm.generate(image_paths=image_paths, **kwargs)\n",
    "\n",
    "print(\"[Ready] generate_captions(['path/to/img.jpg'])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bites",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
