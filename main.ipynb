{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea62c044",
   "metadata": {},
   "source": [
    "## CLIP (ViT-L/14) + Projection MLP + mT5-Small Decoder Pipeline\n",
    "\n",
    "Bu bölüm: \n",
    "- CLIP ViT-L/14 image encoder (tamamen freeze)\n",
    "- Görsel embedding -> K adet prefix token üreten MLP (öğrenilir)\n",
    "- mT5-small (sadece decoder veya istersen tamamı) caption/çeviri üretimi\n",
    "- Projection MLP ve mT5 decoder parametreleri eğitilecek.\n",
    "\n",
    "Strateji (prefix approach):\n",
    "1. Image -> CLIP encode_image -> (B,512)\n",
    "2. MLP: 512 -> (K * d_model) reshape -> (B,K,512) -> LayerNorm\n",
    "3. mT5 encoder'a inputs_embeds olarak bu prefix (opsiyonel ek tekst prompt tokenleri ile concat)\n",
    "4. Decoder hedef yazıyı üretir (teacher forcing, cross-entropy)\n",
    "\n",
    "Seçilebilir dondurma opsiyonları:\n",
    "- freeze_clip = True (zorunlu senaryon)\n",
    "- freeze_t5_encoder = True bırakıp sadece decoder + projection eğitilebilir\n",
    "\n",
    "Aşağıdaki kod Flickr8k JSON (tasviret8k_captions.json) içinden (örnek) tek caption seçip dataset oluşturma iskeleti içerir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26224d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: 3000\n",
      "Sample item: ('498404951_527adba7b8.jpg', 'Beyaz renkli bir köpek otların arasında bir suyun içinde.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params:\n",
      "162421440 parameters\n"
     ]
    }
   ],
   "source": [
    "import json, random, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim=512, d_model=512, prefix_tokens=16, hidden=1024, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.d_model = d_model\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, prefix_tokens * d_model)\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        out = self.net(x).view(b, self.prefix_tokens, self.d_model)\n",
    "        out = self.ln(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class CLIPmT5Pipeline(nn.Module):\n",
    "    def __init__(self, mt5_name=\"google/mt5-small\", clip_name=\"ViT-L/14\", prefix_tokens=16,\n",
    "                 freeze_clip=True, freeze_t5_encoder=True, freeze_t5_decoder=False, device=None):\n",
    "        super().__init__()\n",
    "        self.device_ref = device or (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(mt5_name)\n",
    "        self.t5 = MT5ForConditionalGeneration.from_pretrained(mt5_name)\n",
    "        if freeze_t5_encoder:\n",
    "            for p in self.t5.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        if freeze_t5_decoder:\n",
    "            for p in self.t5.decoder.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.t5.lm_head.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.clip_model, self.clip_preprocess = clip.load(clip_name, device=\"cpu\")\n",
    "        if freeze_clip:\n",
    "            for p in self.clip_model.parameters():\n",
    "                p.requires_grad = False\n",
    "        clip_dim = self.clip_model.visual.output_dim if hasattr(self.clip_model.visual, 'output_dim') else 512\n",
    "        d_model = self.t5.config.d_model\n",
    "        assert d_model == 512, \"mt5-small d_model beklenen 512 değil!\"\n",
    "        self.proj = ProjectionMLP(in_dim=clip_dim, d_model=d_model, prefix_tokens=prefix_tokens)\n",
    "        self.to(self.device_ref)\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.device_ref\n",
    "    def encode_image(self, pil_images):\n",
    "        imgs = torch.stack([self.clip_preprocess(im) for im in pil_images]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            feats = self.clip_model.encode_image(imgs)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        return feats\n",
    "    def forward(self, images, source_texts, target_texts=None, max_new_tokens=64, num_beams=1):\n",
    "        feats = self.encode_image(images)\n",
    "        prefix = self.proj(feats)\n",
    "        enc_tokens = self.tokenizer(list(source_texts), padding=True, return_tensors=\"pt\")\n",
    "        input_ids = enc_tokens.input_ids.to(self.device)\n",
    "        attn_mask = enc_tokens.attention_mask.to(self.device)\n",
    "        text_embeds = self.t5.encoder.embed_tokens(input_ids)\n",
    "        inputs_embeds = torch.cat([prefix, text_embeds], dim=1)\n",
    "        prefix_mask = torch.ones(prefix.size()[:2], dtype=attn_mask.dtype, device=self.device)\n",
    "        encoder_attention_mask = torch.cat([prefix_mask, attn_mask], dim=1)\n",
    "        encoder_outputs = self.t5.encoder(inputs_embeds=inputs_embeds, attention_mask=encoder_attention_mask, return_dict=True)\n",
    "        if target_texts is not None:\n",
    "            dec_tokens = self.tokenizer(list(target_texts), padding=True, return_tensors=\"pt\")\n",
    "            labels = dec_tokens.input_ids.to(self.device)\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            out = self.t5(encoder_outputs=encoder_outputs, attention_mask=encoder_attention_mask, labels=labels, return_dict=True)\n",
    "            return out\n",
    "        gen = self.t5.generate(encoder_outputs=encoder_outputs, attention_mask=encoder_attention_mask,\n",
    "                               max_new_tokens=max_new_tokens, num_beams=num_beams)\n",
    "        decoded = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        return decoded\n",
    "\n",
    "class Flickr8kCaptions(Dataset):\n",
    "    def __init__(self, json_path, images_root, instruction_prefix=\"describe image in Turkish:\", limit=None, seed=42):\n",
    "        data = json.loads(Path(json_path).read_text())\n",
    "        items = []\n",
    "        # Expected structure: {\"images\": [{\"filename\":..., \"sentences\": [{\"raw\":..., \"tokens\":[...]}, ...]}, ...]}\n",
    "        if isinstance(data, dict) and isinstance(data.get(\"images\"), list):\n",
    "            for im_entry in data[\"images\"]:\n",
    "                fn = im_entry.get(\"filename\")\n",
    "                sents = im_entry.get(\"sentences\", [])\n",
    "                for s in sents:\n",
    "                    caption = s.get(\"raw\") or \" \".join(s.get(\"tokens\", []))\n",
    "                    if fn and caption:\n",
    "                        items.append((fn, caption))\n",
    "        else:\n",
    "            # Fallback to previous (filename -> list) style if needed\n",
    "            for k, v in (data.items() if isinstance(data, dict) else []):\n",
    "                if isinstance(v, list):\n",
    "                    for c in v:\n",
    "                        items.append((k, c))\n",
    "        random.Random(seed).shuffle(items)\n",
    "        if limit:\n",
    "            items = items[:limit]\n",
    "        self.items = items\n",
    "        self.base = Path(images_root)\n",
    "        self.prefix = instruction_prefix\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.items[idx]\n",
    "        img_path = self.base / Path(img_name).name\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        return image, self.prefix, caption\n",
    "\n",
    "def collate(batch):\n",
    "    images, sources, targets = zip(*batch)\n",
    "    return list(images), list(sources), list(targets)\n",
    "\n",
    "json_path = \"data/flickr8k/tasviret8k_captions.json\"\n",
    "images_root = \"data/flickr8k/Images\"\n",
    "try:\n",
    "    dataset = Flickr8kCaptions(json_path, images_root, limit=3000)\n",
    "    print(\"Loaded samples:\", len(dataset))\n",
    "    if len(dataset):\n",
    "        print(\"Sample item:\", dataset.items[0])\n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate)\n",
    "except Exception as e:\n",
    "    print(\"Dataset init error (check JSON format):\", e)\n",
    "    loader = None\n",
    "\n",
    "model_mm = CLIPmT5Pipeline(prefix_tokens=16, freeze_clip=True, freeze_t5_encoder=True, freeze_t5_decoder=False)\n",
    "print(\"Trainable params:\")\n",
    "trainable = sum(p.numel() for p in model_mm.parameters() if p.requires_grad)\n",
    "print(trainable, \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec9cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 5 Loss 21.6134\n",
      "Epoch 1 Step 10 Loss 19.7147\n",
      "Epoch 1 Step 15 Loss 24.0285\n",
      "Epoch 1 Step 20 Loss 15.8134\n",
      "Epoch 1 Step 25 Loss 21.7727\n",
      "Epoch 1 Step 30 Loss 17.6961\n",
      "Epoch 1 Step 35 Loss 19.3310\n",
      "Epoch 1 Step 40 Loss 20.1627\n",
      "Epoch 1 Step 45 Loss 18.5610\n",
      "Epoch 1 Step 50 Loss 19.1264\n",
      "Epoch 1 Step 55 Loss 23.5003\n",
      "Epoch 1 Step 60 Loss 21.5944\n",
      "Epoch 1 Step 65 Loss 17.9103\n",
      "Epoch 1 Step 70 Loss 16.1339\n",
      "Epoch 1 Step 75 Loss 22.5555\n",
      "Epoch 1 Step 80 Loss 13.6943\n",
      "Epoch 1 Step 85 Loss 21.5188\n",
      "Epoch 1 Step 90 Loss 16.4548\n",
      "Epoch 1 Step 95 Loss 13.1756\n",
      "Epoch 1 Step 100 Loss 16.3613\n",
      "Epoch 1 Step 105 Loss 13.4107\n",
      "Epoch 1 Step 110 Loss 14.7152\n",
      "Epoch 1 Step 115 Loss 13.5813\n",
      "Epoch 1 Step 120 Loss 16.3904\n",
      "Epoch 1 Step 125 Loss 15.1450\n",
      "Epoch 1 Step 130 Loss 14.4007\n",
      "Epoch 1 Step 135 Loss 14.2127\n",
      "Epoch 1 Step 140 Loss 15.1400\n",
      "Epoch 1 Step 145 Loss 16.2860\n",
      "Epoch 1 Step 150 Loss 12.9744\n",
      "Epoch 1 Step 155 Loss 14.1738\n",
      "Epoch 1 Step 160 Loss 17.0998\n",
      "Epoch 1 Step 165 Loss 12.6047\n",
      "Epoch 1 Step 170 Loss 16.0285\n",
      "Epoch 1 Step 175 Loss 17.8194\n",
      "Epoch 1 Step 180 Loss 13.8955\n",
      "Epoch 1 Step 185 Loss 14.5204\n",
      "Epoch 1 Step 190 Loss 14.1962\n",
      "Epoch 1 Step 195 Loss 13.9251\n",
      "Epoch 1 Step 200 Loss 17.5391\n",
      "Epoch 1 Step 205 Loss 13.4350\n",
      "Epoch 1 Step 210 Loss 13.5915\n",
      "Epoch 1 Step 215 Loss 16.5491\n",
      "Epoch 1 Step 220 Loss 13.8922\n",
      "Epoch 1 Step 225 Loss 13.0651\n",
      "Epoch 1 Step 230 Loss 14.8807\n",
      "Epoch 1 Step 235 Loss 15.9722\n",
      "Epoch 1 Step 240 Loss 13.2162\n",
      "Epoch 1 Step 245 Loss 13.2348\n",
      "Epoch 1 Step 250 Loss 12.2865\n",
      "Epoch 1 Step 255 Loss 11.8724\n",
      "Epoch 1 Step 260 Loss 12.0692\n",
      "Epoch 1 Step 265 Loss 13.9367\n",
      "Epoch 1 Step 270 Loss 15.5447\n",
      "Epoch 1 Step 275 Loss 14.0155\n",
      "Epoch 1 Step 280 Loss 14.7842\n",
      "Epoch 1 Step 285 Loss 14.7884\n",
      "Epoch 1 Step 290 Loss 12.9881\n",
      "Epoch 1 Step 295 Loss 12.7969\n",
      "Epoch 1 Step 300 Loss 13.9252\n",
      "Epoch 1 Step 305 Loss 15.9305\n",
      "Epoch 1 Step 310 Loss 13.1292\n",
      "Epoch 1 Step 315 Loss 14.9493\n",
      "Epoch 1 Step 320 Loss 13.7133\n",
      "Epoch 1 Step 325 Loss 13.4527\n",
      "Epoch 1 Step 330 Loss 13.6471\n",
      "Epoch 1 Step 335 Loss 12.5808\n",
      "Epoch 1 Step 340 Loss 11.3536\n",
      "Epoch 1 Step 345 Loss 12.2361\n",
      "Epoch 1 Step 350 Loss 14.3402\n",
      "Epoch 1 Step 355 Loss 12.7030\n",
      "Epoch 1 Step 360 Loss 12.5860\n",
      "Epoch 1 Step 365 Loss 11.8775\n",
      "Epoch 1 Step 370 Loss 13.2080\n",
      "Epoch 1 Step 375 Loss 13.3293\n",
      "Epoch 1 Step 380 Loss 13.6981\n",
      "Epoch 1 Step 385 Loss 14.0540\n",
      "Epoch 1 Step 390 Loss 14.7803\n",
      "Epoch 1 Step 395 Loss 11.0736\n",
      "Epoch 1 Step 400 Loss 11.8194\n",
      "Epoch 1 Step 405 Loss 13.9719\n",
      "Epoch 1 Step 410 Loss 13.6839\n",
      "Epoch 1 Step 415 Loss 11.2553\n",
      "Epoch 1 Step 420 Loss 12.9720\n",
      "Epoch 1 Step 425 Loss 12.4135\n",
      "Epoch 1 Step 430 Loss 11.7628\n",
      "Epoch 1 Step 435 Loss 12.6244\n",
      "Epoch 1 Step 440 Loss 15.8848\n",
      "Epoch 1 Step 445 Loss 13.1920\n",
      "Epoch 1 Step 450 Loss 13.9326\n",
      "Epoch 1 Step 455 Loss 14.6869\n",
      "Epoch 1 Step 460 Loss 13.8731\n",
      "Epoch 1 Step 465 Loss 13.8121\n",
      "Epoch 1 Step 470 Loss 12.4743\n",
      "Epoch 1 Step 475 Loss 12.7868\n",
      "Epoch 1 Step 480 Loss 11.7912\n",
      "Epoch 1 Step 485 Loss 12.4427\n",
      "Epoch 1 Step 490 Loss 11.6292\n",
      "Epoch 1 Step 495 Loss 11.5490\n",
      "Epoch 1 Step 500 Loss 11.4289\n",
      "Epoch 1 Step 505 Loss 11.7308\n",
      "Epoch 1 Step 510 Loss 13.7759\n",
      "Epoch 1 Step 515 Loss 10.8806\n",
      "Epoch 1 Step 520 Loss 11.1425\n",
      "Epoch 1 Step 525 Loss 11.0898\n",
      "Epoch 1 Step 530 Loss 11.3607\n",
      "Epoch 1 Step 535 Loss 12.8027\n",
      "Epoch 1 Step 540 Loss 11.9269\n",
      "Epoch 1 Step 545 Loss 10.9191\n",
      "Epoch 1 Step 550 Loss 12.6514\n",
      "Epoch 1 Step 555 Loss 12.1127\n",
      "Epoch 1 Step 560 Loss 14.3301\n",
      "Epoch 1 Step 565 Loss 13.8396\n",
      "Epoch 1 Step 570 Loss 15.6954\n",
      "Epoch 1 Step 575 Loss 12.5368\n",
      "Epoch 1 Step 580 Loss 18.4362\n",
      "Epoch 1 Step 585 Loss 15.5136\n",
      "Epoch 1 Step 590 Loss 14.3028\n",
      "Epoch 1 Step 595 Loss 13.3736\n",
      "Epoch 1 Step 600 Loss 15.7733\n",
      "Epoch 1 Step 605 Loss 12.9042\n",
      "Epoch 1 Step 610 Loss 15.3476\n",
      "Epoch 1 Step 615 Loss 14.5069\n",
      "Epoch 1 Step 620 Loss 16.9528\n",
      "Epoch 1 Step 625 Loss 14.9222\n",
      "Epoch 1 Step 630 Loss 13.3966\n",
      "Epoch 1 Step 635 Loss 13.3994\n",
      "Epoch 1 Step 640 Loss 15.4688\n",
      "Epoch 1 Step 645 Loss 16.7549\n",
      "Epoch 1 Step 650 Loss 18.7803\n",
      "Epoch 1 Step 655 Loss 20.1777\n",
      "Epoch 1 Step 660 Loss 23.9363\n",
      "Epoch 1 Step 665 Loss 19.5522\n",
      "Epoch 1 Step 670 Loss 24.9026\n",
      "Epoch 1 Step 675 Loss 23.1970\n",
      "Epoch 1 Step 680 Loss 23.9292\n",
      "Epoch 1 Step 685 Loss 18.6496\n",
      "Epoch 1 Step 690 Loss 21.3351\n",
      "Epoch 1 Step 695 Loss 24.2356\n",
      "Epoch 1 Step 700 Loss 21.8240\n",
      "Epoch 1 Step 705 Loss 19.7436\n",
      "Epoch 1 Step 710 Loss 22.3858\n",
      "Epoch 1 Step 715 Loss 14.9534\n",
      "Epoch 1 Step 720 Loss 23.3169\n",
      "Epoch 1 Step 725 Loss 23.9903\n",
      "Epoch 1 Step 730 Loss 21.1494\n",
      "Epoch 1 Step 735 Loss 22.6480\n",
      "Epoch 1 Step 740 Loss 14.7907\n",
      "Epoch 1 Step 745 Loss 15.7480\n",
      "Epoch 1 Step 750 Loss 19.0412\n",
      "Epoch 1 Mean Loss: 15.3431\n",
      "Epoch 2 Step 5 Loss 20.5270\n",
      "Epoch 2 Step 10 Loss 16.6390\n",
      "Epoch 2 Step 15 Loss 15.9801\n",
      "Epoch 2 Step 20 Loss 17.2734\n",
      "Epoch 2 Step 25 Loss 16.9163\n",
      "Epoch 2 Step 30 Loss 14.6895\n",
      "Epoch 2 Step 35 Loss 15.4775\n",
      "Epoch 2 Step 40 Loss 15.7578\n",
      "Epoch 2 Step 45 Loss 18.3571\n",
      "Epoch 2 Step 50 Loss 16.2106\n",
      "Epoch 2 Step 55 Loss 13.7498\n",
      "Epoch 2 Step 60 Loss 17.4923\n",
      "Epoch 2 Step 65 Loss 10.9449\n",
      "Epoch 2 Step 70 Loss 10.1062\n",
      "Epoch 2 Step 75 Loss 12.5598\n",
      "Epoch 2 Step 80 Loss 14.3534\n",
      "Epoch 2 Step 85 Loss 15.1284\n",
      "Epoch 2 Step 90 Loss 10.7574\n",
      "Epoch 2 Step 95 Loss 14.9287\n",
      "Epoch 2 Step 100 Loss 14.2379\n",
      "Epoch 2 Step 105 Loss 14.5259\n",
      "Epoch 2 Step 110 Loss 19.8567\n",
      "Epoch 2 Step 115 Loss 17.9692\n",
      "Epoch 2 Step 120 Loss 18.6232\n",
      "Epoch 2 Step 125 Loss 22.7084\n",
      "Epoch 2 Step 130 Loss 19.6455\n",
      "Epoch 2 Step 135 Loss 15.8764\n",
      "Epoch 2 Step 140 Loss 13.8874\n",
      "Epoch 2 Step 145 Loss 16.2533\n",
      "Epoch 2 Step 150 Loss 13.8430\n",
      "Epoch 2 Step 155 Loss 15.3404\n",
      "Epoch 2 Step 160 Loss 18.5313\n",
      "Epoch 2 Step 165 Loss 15.9586\n",
      "Epoch 2 Step 170 Loss 18.5427\n",
      "Epoch 2 Step 175 Loss 17.5240\n",
      "Epoch 2 Step 180 Loss 23.7894\n",
      "Epoch 2 Step 185 Loss 21.6962\n",
      "Epoch 2 Step 190 Loss 23.0980\n",
      "Epoch 2 Step 195 Loss 26.6332\n",
      "Epoch 2 Step 200 Loss 18.1427\n",
      "Epoch 2 Step 205 Loss 28.6233\n",
      "Epoch 2 Step 210 Loss 25.3047\n",
      "Epoch 2 Step 215 Loss 22.1848\n",
      "Epoch 2 Step 220 Loss 40.5567\n",
      "Epoch 2 Step 225 Loss 45.1384\n",
      "Epoch 2 Step 230 Loss 45.1446\n",
      "Epoch 2 Step 235 Loss 26.7054\n",
      "Epoch 2 Step 240 Loss 40.0368\n",
      "Epoch 2 Step 245 Loss 38.5371\n",
      "Epoch 2 Step 250 Loss 42.2107\n",
      "Epoch 2 Step 255 Loss 34.5975\n",
      "Epoch 2 Step 260 Loss 38.3286\n",
      "Epoch 2 Step 265 Loss 36.9934\n",
      "Epoch 2 Step 270 Loss 41.4143\n",
      "Epoch 2 Step 275 Loss 39.3754\n",
      "Epoch 2 Step 280 Loss 37.7348\n",
      "Epoch 2 Step 285 Loss 32.8036\n",
      "Epoch 2 Step 290 Loss 30.2809\n",
      "Epoch 2 Step 295 Loss 27.8598\n",
      "Epoch 2 Step 300 Loss 30.7374\n",
      "Epoch 2 Step 305 Loss 33.2897\n",
      "Epoch 2 Step 310 Loss 31.7819\n",
      "Epoch 2 Step 315 Loss 28.1615\n",
      "Epoch 2 Step 320 Loss 30.3378\n",
      "Epoch 2 Step 325 Loss 28.3366\n",
      "Epoch 2 Step 330 Loss 29.7764\n",
      "Epoch 2 Step 335 Loss 27.3288\n",
      "Epoch 2 Step 340 Loss 23.2416\n",
      "Epoch 2 Step 345 Loss 22.8096\n",
      "Epoch 2 Step 350 Loss 32.4656\n",
      "Epoch 2 Step 355 Loss 29.1745\n",
      "Epoch 2 Step 360 Loss 29.6313\n",
      "Epoch 2 Step 365 Loss 27.5510\n",
      "Epoch 2 Step 370 Loss 29.7951\n",
      "Epoch 2 Step 375 Loss 25.5211\n",
      "Epoch 2 Step 380 Loss 30.9201\n",
      "Epoch 2 Step 385 Loss 31.4323\n",
      "Epoch 2 Step 390 Loss 29.8405\n",
      "Epoch 2 Step 395 Loss 26.3737\n",
      "Epoch 2 Step 400 Loss 22.0583\n",
      "Epoch 2 Step 405 Loss 25.5696\n",
      "Epoch 2 Step 410 Loss 21.7128\n",
      "Epoch 2 Step 415 Loss 27.6285\n",
      "Epoch 2 Step 420 Loss 24.3551\n",
      "Epoch 2 Step 425 Loss 31.2676\n",
      "Epoch 2 Step 430 Loss 25.8285\n",
      "Epoch 2 Step 435 Loss 21.0477\n",
      "Epoch 2 Step 440 Loss 26.6590\n",
      "Epoch 2 Step 445 Loss 32.9967\n",
      "Epoch 2 Step 450 Loss 27.0026\n",
      "Epoch 2 Step 455 Loss 30.4286\n",
      "Epoch 2 Step 460 Loss 25.3478\n",
      "Epoch 2 Step 465 Loss 26.1631\n",
      "Epoch 2 Step 470 Loss 26.3482\n",
      "Epoch 2 Step 475 Loss 25.0356\n",
      "Epoch 2 Step 480 Loss 29.0329\n",
      "Epoch 2 Step 485 Loss 23.3054\n",
      "Epoch 2 Step 490 Loss 27.1032\n",
      "Epoch 2 Step 495 Loss 18.4424\n",
      "Epoch 2 Step 500 Loss 27.0863\n",
      "Epoch 2 Step 505 Loss 27.7913\n",
      "Epoch 2 Step 510 Loss 27.9800\n",
      "Epoch 2 Step 515 Loss 28.9139\n",
      "Epoch 2 Step 520 Loss 24.7534\n",
      "Epoch 2 Step 525 Loss 22.9364\n",
      "Epoch 2 Step 530 Loss 27.3935\n",
      "Epoch 2 Step 535 Loss 22.3111\n",
      "Epoch 2 Step 540 Loss 27.8895\n",
      "Epoch 2 Step 545 Loss 29.2323\n",
      "Epoch 2 Step 550 Loss 25.2066\n",
      "Epoch 2 Step 555 Loss 25.6042\n",
      "Epoch 2 Step 560 Loss 22.3063\n",
      "Epoch 2 Step 565 Loss 25.9507\n",
      "Epoch 2 Step 570 Loss 26.3985\n",
      "Epoch 2 Step 575 Loss 28.7876\n",
      "Epoch 2 Step 580 Loss 28.4561\n",
      "Epoch 2 Step 585 Loss 25.6785\n",
      "Epoch 2 Step 590 Loss 29.7645\n",
      "Epoch 2 Step 595 Loss 26.3859\n",
      "Epoch 2 Step 600 Loss 28.8334\n",
      "Epoch 2 Step 605 Loss 24.2178\n",
      "Epoch 2 Step 610 Loss 24.0751\n",
      "Epoch 2 Step 615 Loss 27.0373\n",
      "Epoch 2 Step 620 Loss 22.3287\n",
      "Epoch 2 Step 625 Loss 22.6673\n",
      "Epoch 2 Step 630 Loss 28.9627\n",
      "Epoch 2 Step 635 Loss 27.9006\n",
      "Epoch 2 Step 640 Loss 27.0781\n",
      "Epoch 2 Step 645 Loss 28.9196\n",
      "Epoch 2 Step 650 Loss 24.5670\n",
      "Epoch 2 Step 655 Loss 29.0443\n",
      "Epoch 2 Step 660 Loss 24.8000\n",
      "Epoch 2 Step 665 Loss 31.1821\n",
      "Epoch 2 Step 670 Loss 29.2864\n",
      "Epoch 2 Step 675 Loss 34.5289\n",
      "Epoch 2 Step 680 Loss 31.3117\n",
      "Epoch 2 Step 685 Loss 30.9805\n",
      "Epoch 2 Step 690 Loss 27.6085\n",
      "Epoch 2 Step 695 Loss 33.8421\n",
      "Epoch 2 Step 700 Loss 33.7185\n",
      "Epoch 2 Step 705 Loss 32.1812\n",
      "Epoch 2 Step 710 Loss 32.5338\n",
      "Epoch 2 Step 715 Loss 33.0972\n",
      "Epoch 2 Step 720 Loss 33.7366\n",
      "Epoch 2 Step 725 Loss 36.8862\n",
      "Epoch 2 Step 730 Loss 30.9713\n",
      "Epoch 2 Step 735 Loss 30.6364\n",
      "Epoch 2 Step 740 Loss 32.2238\n",
      "Epoch 2 Step 745 Loss 37.0762\n",
      "Epoch 2 Step 750 Loss 31.8250\n",
      "Epoch 2 Mean Loss: 26.2558\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from contextlib import nullcontext\n",
    "from torch import amp\n",
    "\n",
    "max_epochs = 2\n",
    "lr = 2e-4\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_amp = use_cuda  # Only enable AMP on CUDA\n",
    "scaler = amp.GradScaler(device_type='cuda', enabled=use_amp) if use_amp else None\n",
    "\n",
    "optim_params = [p for p in model_mm.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(optim_params, lr=lr)\n",
    "\n",
    "if loader is not None:\n",
    "    model_mm.train()\n",
    "    for epoch in range(max_epochs):\n",
    "        total_loss = 0.0\n",
    "        for step, (imgs, srcs, tgts) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            ctx = amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext()\n",
    "            with ctx:\n",
    "                out = model_mm(imgs, srcs, tgts)\n",
    "                loss = out.loss\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if (step + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1} Step {step+1} Loss {loss.item():.4f}\")\n",
    "        print(f\"Epoch {epoch+1} Mean Loss: {total_loss/len(loader):.4f}\")\n",
    "else:\n",
    "    print(\"Loader not initialized; check dataset parsing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fb8bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: describe image in Turkish:\n",
      "PRED: brali rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada rada\n",
      "GOLD: Bir grup insan deniz aracının içinden selam veriyor.\n",
      "----\n",
      "SRC: describe image in Turkish:\n",
      "PRED: nkomstātā\n",
      "GOLD: Bir takım israilli insan kendi milletlerini eleştirenlere karşı bir eylemdeler.\n",
      "----\n",
      "SRC: describe image in Turkish:\n",
      "PRED: 㲟 bir... bir.kuyu. köpkuyu.kuyu.kuyu.kuyu bir. köpkuyu. köpkuyu. köpkuyu. köpkuyu. köp\n",
      "GOLD: Ağzında tuttuğu renkli top ile çimlerin üzerinde koşan kahverengi küçük bir köpek.\n",
      "----\n",
      "SRC: describe image in Turkish:\n",
      "PRED: resize\n",
      "GOLD: Göl kenarına kurulmuş bir tribünde bir adam oturmuş, gitar çalıyor.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Inference / quick generation test\n",
    "if loader is not None:\n",
    "    model_mm.eval()\n",
    "    sample_imgs, sample_srcs, sample_tgts = next(iter(loader))\n",
    "    with torch.no_grad():\n",
    "        preds = model_mm(sample_imgs, sample_srcs, target_texts=None, max_new_tokens=32, num_beams=4)\n",
    "    for i in range(len(preds)):\n",
    "        print(f\"SRC: {sample_srcs[i][:50]}\")\n",
    "        print(f\"PRED: {preds[i]}\")\n",
    "        print(f\"GOLD: {sample_tgts[i]}\")\n",
    "        print(\"----\")\n",
    "else:\n",
    "    print(\"No loader for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bites",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
